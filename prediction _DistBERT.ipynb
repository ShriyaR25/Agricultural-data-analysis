{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your preprocessed dataset\n",
    "data1 = pd.read_csv('D:/kcc_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "f_df = data1[(data1['Crop'].str.isnumeric() == False) & (data1['Crop'] != 'Others') & \n",
    "             (data1['QueryType'].str.isnumeric() == False) & (data1['QueryType'] != 'Others')]\n",
    "df= pd.DataFrame(f_df)\n",
    "df = df[df['Year'] >= 2013]\n",
    "# Create a new feature 'Crop_QueryType'\n",
    "df[\"place\"] = df['StateName'] + '_' + df['DistrictName']\n",
    "data = df[['Month', 'Year', 'place', 'Crop', 'QueryType']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROP WISE QUERYTYPE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode QueryType as categorical variable\n",
    "data['QueryType_code'] = data['QueryType'].astype('category').cat.codes\n",
    "\n",
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop']} {row['QueryType']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs using DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(data['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Extract labels for training (QueryType codes)\n",
    "labels = torch.tensor(data['QueryType_code'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_inputs, test_inputs, train_labels, test_labels, train_idx, test_idx = train_test_split(\n",
    "    inputs['input_ids'], labels, data.index, test_size=0.2, random_state=42)\n",
    "train_masks, test_masks = train_test_split(inputs['attention_mask'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, torch.tensor(train_masks), train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, torch.tensor(test_masks), test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=data['QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 6000/915532 | Loss: 0.002130125416442752\n",
      "Epoch 1/1 | Step 7000/915532 | Loss: 0.00012731247988995165\n",
      "Epoch 1/1 | Step 8000/915532 | Loss: 0.0002920789993368089\n",
      "Epoch 1/1 | Step 9000/915532 | Loss: 2.5189950974890962e-05\n",
      "Epoch 1/1 | Step 10000/915532 | Loss: 0.0002047727321041748\n",
      "Epoch 1/1 | Step 11000/915532 | Loss: 6.216885230969638e-05\n",
      "Epoch 1/1 | Step 12000/915532 | Loss: 8.47478149808012e-05\n",
      "Epoch 1/1 | Step 13000/915532 | Loss: 1.0929831660178024e-05\n",
      "Epoch 1/1 | Step 14000/915532 | Loss: 2.804975702019874e-05\n",
      "Epoch 1/1 | Step 15000/915532 | Loss: 2.073386167467106e-05\n",
      "Epoch 1/1 | Step 16000/915532 | Loss: 1.1168199307576288e-05\n",
      "Epoch 1/1 | Step 17000/915532 | Loss: 2.00625308934832e-05\n",
      "Epoch 1/1 | Step 18000/915532 | Loss: 0.0001100663430406712\n",
      "Epoch 1/1 | Step 19000/915532 | Loss: 2.5927965907612815e-06\n",
      "Epoch 1/1 | Step 20000/915532 | Loss: 1.5124625178941642e-06\n",
      "Epoch 1/1 | Step 21000/915532 | Loss: 0.00011443998664617538\n",
      "Epoch 1/1 | Step 22000/915532 | Loss: 2.153636160073802e-05\n",
      "Epoch 1/1 | Step 23000/915532 | Loss: 6.57282944303006e-05\n",
      "Epoch 1/1 | Step 24000/915532 | Loss: 4.500084742176114e-06\n",
      "Epoch 1/1 | Step 25000/915532 | Loss: 1.2024239367747214e-05\n",
      "Epoch 1/1 | Step 26000/915532 | Loss: 1.7136228507297346e-06\n",
      "Epoch 1/1 | Step 27000/915532 | Loss: 5.51342679955269e-07\n",
      "Epoch 1/1 | Step 28000/915532 | Loss: 0.00022169636213220656\n",
      "Epoch 1/1 | Step 29000/915532 | Loss: 5.014154339733068e-06\n",
      "Epoch 1/1 | Step 30000/915532 | Loss: 9.238710276804341e-07\n",
      "Epoch 1/1 | Step 31000/915532 | Loss: 3.799794683345681e-07\n",
      "Epoch 1/1 | Step 32000/915532 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 33000/915532 | Loss: 5.066391395303071e-07\n",
      "Epoch 1/1 | Step 34000/915532 | Loss: 4.023308690648264e-07\n",
      "Epoch 1/1 | Step 35000/915532 | Loss: 4.5448490482158377e-07\n",
      "Epoch 1/1 | Step 36000/915532 | Loss: 2.458690175899392e-07\n",
      "Epoch 1/1 | Step 37000/915532 | Loss: 9.834708407652215e-07\n",
      "Epoch 1/1 | Step 38000/915532 | Loss: 9.685753354915505e-08\n",
      "Epoch 1/1 | Step 39000/915532 | Loss: 6.399727226380492e-06\n",
      "Epoch 1/1 | Step 40000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 41000/915532 | Loss: 3.4938530006911606e-05\n",
      "Epoch 1/1 | Step 42000/915532 | Loss: 1.2367939916657633e-06\n",
      "Epoch 1/1 | Step 43000/915532 | Loss: 1.221884076585411e-06\n",
      "Epoch 1/1 | Step 44000/915532 | Loss: 2.682207878024201e-07\n",
      "Epoch 1/1 | Step 45000/915532 | Loss: 6.034955504219397e-07\n",
      "Epoch 1/1 | Step 46000/915532 | Loss: 7.152538046284462e-07\n",
      "Epoch 1/1 | Step 47000/915532 | Loss: 7.525065939262277e-07\n",
      "Epoch 1/1 | Step 48000/915532 | Loss: 2.682207025372918e-07\n",
      "Epoch 1/1 | Step 49000/915532 | Loss: 1.0654290463207872e-06\n",
      "Epoch 1/1 | Step 50000/915532 | Loss: 1.2665982751514093e-07\n",
      "Epoch 1/1 | Step 51000/915532 | Loss: 1.1175867342672063e-07\n",
      "Epoch 1/1 | Step 52000/915532 | Loss: 1.490114556190747e-07\n",
      "Epoch 1/1 | Step 53000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 54000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 55000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 56000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 57000/915532 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 58000/915532 | Loss: 1.6391270207805064e-07\n",
      "Epoch 1/1 | Step 59000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 60000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 61000/915532 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 62000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 63000/915532 | Loss: 1.4156087502215087e-07\n",
      "Epoch 1/1 | Step 64000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 65000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 66000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 67000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 68000/915532 | Loss: 3.397403361304896e-06\n",
      "Epoch 1/1 | Step 69000/915532 | Loss: 6.407466344171553e-07\n",
      "Epoch 1/1 | Step 70000/915532 | Loss: 1.959472456292133e-06\n",
      "Epoch 1/1 | Step 71000/915532 | Loss: 4.5448396690517257e-07\n",
      "Epoch 1/1 | Step 72000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 73000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 74000/915532 | Loss: 8.195635103902532e-08\n",
      "Epoch 1/1 | Step 75000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 76000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 77000/915532 | Loss: 1.1920923270736239e-07\n",
      "Epoch 1/1 | Step 78000/915532 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 79000/915532 | Loss: 8.940693874137651e-08\n",
      "Epoch 1/1 | Step 80000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 81000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 82000/915532 | Loss: 1.4156093186556973e-07\n",
      "Epoch 1/1 | Step 83000/915532 | Loss: 1.490115550950577e-07\n",
      "Epoch 1/1 | Step 84000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 85000/915532 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 86000/915532 | Loss: 7.897567684267415e-07\n",
      "Epoch 1/1 | Step 87000/915532 | Loss: 2.7567122629079677e-07\n",
      "Epoch 1/1 | Step 88000/915532 | Loss: 8.485733269480988e-05\n",
      "Epoch 1/1 | Step 89000/915532 | Loss: 8.589940080128144e-06\n",
      "Epoch 1/1 | Step 90000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 91000/915532 | Loss: 8.940694584680386e-08\n",
      "Epoch 1/1 | Step 92000/915532 | Loss: 4.172312060291006e-07\n",
      "Epoch 1/1 | Step 93000/915532 | Loss: 1.1920926112907182e-07\n",
      "Epoch 1/1 | Step 94000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 95000/915532 | Loss: 7.078014618855377e-07\n",
      "Epoch 1/1 | Step 96000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 97000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 98000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 99000/915532 | Loss: 1.4901149825163884e-07\n",
      "Epoch 1/1 | Step 100000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 101000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 102000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 103000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 104000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 105000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 106000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 107000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 108000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 109000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 110000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 111000/915532 | Loss: 7.450577044210149e-08\n",
      "Epoch 1/1 | Step 112000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 113000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 114000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 115000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 116000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 117000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 118000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 119000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 120000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 121000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 122000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 123000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 124000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 125000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 126000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 127000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 128000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 129000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 130000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 131000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 132000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 133000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 134000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 135000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 136000/915532 | Loss: 5.2154042862184724e-08\n",
      "Epoch 1/1 | Step 137000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 138000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 139000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 140000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 141000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 142000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 143000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 144000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 145000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 146000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 147000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 148000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 149000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 150000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 151000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 152000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 153000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 154000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 155000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 156000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 157000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 158000/915532 | Loss: 7.450577754752885e-08\n",
      "Epoch 1/1 | Step 159000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 160000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 161000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 162000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 163000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 164000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 165000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 166000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 167000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 168000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 169000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 170000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 171000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 172000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 173000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 174000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 175000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 176000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 177000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 178000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 179000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 180000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 181000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 182000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 183000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 184000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 185000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 186000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 187000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 188000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 189000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 190000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 191000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 192000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 193000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 194000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 195000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 196000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 197000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 198000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 199000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 200000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 201000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 202000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 203000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 204000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 205000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 206000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 207000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 208000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 209000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 210000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 211000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 212000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 213000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 214000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 215000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 216000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 217000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 218000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 219000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 220000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 221000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 222000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 223000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 224000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 225000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 226000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 227000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 228000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 229000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 230000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 231000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 232000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 233000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 234000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 235000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 236000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 237000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 238000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 239000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 240000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 241000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 242000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 243000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 244000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 245000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 246000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 247000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 248000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 249000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 250000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 251000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 252000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 253000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 254000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 255000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 256000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 257000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 258000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 259000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 260000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 261000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 262000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 263000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 264000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 265000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 266000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 267000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 268000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 269000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 270000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 271000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 272000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 273000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 274000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 275000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 276000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 277000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 278000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 279000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 280000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 281000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 282000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 283000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 284000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 285000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 286000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 287000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 288000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 289000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 290000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 291000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 292000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 293000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 294000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 295000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 296000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 297000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 298000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 299000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 300000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 301000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 302000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 303000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 304000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 305000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 306000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 307000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 308000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 309000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 310000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 311000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 312000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 313000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 314000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 315000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 316000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 317000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 318000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 319000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 320000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 321000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 322000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 323000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 324000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 325000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 326000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 327000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 328000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 329000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 330000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 331000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 332000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 333000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 334000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 335000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 336000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 337000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 338000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 339000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 340000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 341000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 342000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 343000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 344000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 345000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 346000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 347000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 348000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 349000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 350000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 351000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 352000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 353000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 354000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 355000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 356000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 357000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 358000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 359000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 360000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 361000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 362000/915532 | Loss: 1.2665974225001264e-07\n",
      "Epoch 1/1 | Step 363000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 364000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 365000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 366000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 367000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 368000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 369000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 370000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 371000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 372000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 373000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 374000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 375000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 376000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 377000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 378000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 379000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 380000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 381000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 382000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 383000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 384000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 385000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 386000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 387000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 388000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 389000/915532 | Loss: 5.9604616353681195e-08\n",
      "Epoch 1/1 | Step 390000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 391000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 392000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 393000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 394000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 395000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 396000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 397000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 398000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 399000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 400000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 401000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 402000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 403000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 404000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 405000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 406000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 407000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 408000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 409000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 410000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 411000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 412000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 413000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 414000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 415000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 416000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 417000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 418000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 419000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 420000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 421000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 422000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 423000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 424000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 425000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 426000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 427000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 428000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 429000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 430000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 431000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 432000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 433000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 434000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 435000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 436000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 437000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 438000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 439000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 440000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 441000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 442000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 443000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 444000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 445000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 446000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 447000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 448000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 449000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 450000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 451000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 452000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 453000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 454000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 455000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 456000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 457000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 458000/915532 | Loss: 2.8162560283817584e-06\n",
      "Epoch 1/1 | Step 459000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 460000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 461000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 462000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 463000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 464000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 465000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 466000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 467000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 468000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 469000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 470000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 471000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 472000/915532 | Loss: 7.450576333667414e-08\n",
      "Epoch 1/1 | Step 473000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 474000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 475000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 476000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 477000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 478000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 479000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 480000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 481000/915532 | Loss: 2.7567094207370246e-07\n",
      "Epoch 1/1 | Step 482000/915532 | Loss: 4.172311207639723e-07\n",
      "Epoch 1/1 | Step 483000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 484000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 485000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 486000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 487000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 488000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 489000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 490000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 491000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 492000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 493000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 494000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 495000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 496000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 497000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 498000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 499000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 500000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 501000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 502000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 503000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 504000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 505000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 506000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 507000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 508000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 509000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 510000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 511000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 512000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 513000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 514000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 515000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 516000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 517000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 518000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 519000/915532 | Loss: 5.140879579812463e-07\n",
      "Epoch 1/1 | Step 520000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 521000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 522000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 523000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 524000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 525000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 526000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 527000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 528000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 529000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 530000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 531000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 532000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 533000/915532 | Loss: 5.215405352032576e-08\n",
      "Epoch 1/1 | Step 534000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 535000/915532 | Loss: 2.592748160168412e-06\n",
      "Epoch 1/1 | Step 536000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 537000/915532 | Loss: 1.780665456863062e-06\n",
      "Epoch 1/1 | Step 538000/915532 | Loss: 1.0430805019723266e-07\n",
      "Epoch 1/1 | Step 539000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 540000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 541000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 542000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 543000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 544000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 545000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 546000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 547000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 548000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 549000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 550000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 551000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 552000/915532 | Loss: 1.318740373790206e-06\n",
      "Epoch 1/1 | Step 553000/915532 | Loss: 8.552681720175315e-06\n",
      "Epoch 1/1 | Step 554000/915532 | Loss: 1.0772620953503065e-05\n",
      "Epoch 1/1 | Step 555000/915532 | Loss: 6.7498685893951915e-06\n",
      "Epoch 1/1 | Step 556000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 557000/915532 | Loss: 7.450577754752885e-08\n",
      "Epoch 1/1 | Step 558000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 559000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 560000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 561000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 562000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 563000/915532 | Loss: 1.1920917586394353e-07\n",
      "Epoch 1/1 | Step 564000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 565000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 566000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 567000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 568000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 569000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 570000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 571000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 572000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 573000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 574000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 575000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 576000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 577000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 578000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 579000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 580000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 581000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 582000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 583000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 584000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 585000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 586000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 587000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 588000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 589000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 590000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 591000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 592000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 593000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 594000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 595000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 596000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 597000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 598000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 599000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 600000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 601000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 602000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 603000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 604000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 605000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 606000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 607000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 608000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 609000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 610000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 611000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 612000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 613000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 614000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 615000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 616000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 617000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 618000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 619000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 620000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 621000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 622000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 623000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 624000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 625000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 626000/915532 | Loss: 9.685746960030883e-08\n",
      "Epoch 1/1 | Step 627000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 628000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 629000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 630000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 631000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 632000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 633000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 634000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 635000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 636000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 637000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 638000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 639000/915532 | Loss: 7.823063583600742e-07\n",
      "Epoch 1/1 | Step 640000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 641000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 642000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 643000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 644000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 645000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 646000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 647000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 648000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 649000/915532 | Loss: 5.9604616353681195e-08\n",
      "Epoch 1/1 | Step 650000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 651000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 652000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 653000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 654000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 655000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 656000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 657000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 658000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 659000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 660000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 661000/915532 | Loss: 2.9802251333421736e-07\n",
      "Epoch 1/1 | Step 662000/915532 | Loss: 8.195632972274325e-08\n",
      "Epoch 1/1 | Step 663000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 664000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 665000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 666000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 667000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 668000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 669000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 670000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 671000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 672000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 673000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 674000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 675000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 676000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 677000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 678000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 679000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 680000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 681000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 682000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 683000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 684000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 685000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 686000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 687000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 688000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 689000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 690000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 691000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 692000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 693000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 694000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 695000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 696000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 697000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 698000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 699000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 700000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 701000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 702000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 703000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 704000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 705000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 706000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 707000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 708000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 709000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 710000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 711000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 712000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 713000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 714000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 715000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 716000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 717000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 718000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 719000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 720000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 721000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 722000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 723000/915532 | Loss: 5.81143069666723e-07\n",
      "Epoch 1/1 | Step 724000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 725000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 726000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 727000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 728000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 729000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 730000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 731000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 732000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 733000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 734000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 735000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 736000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 737000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 738000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 739000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 740000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 741000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 742000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 743000/915532 | Loss: 2.3841812435421161e-07\n",
      "Epoch 1/1 | Step 744000/915532 | Loss: 1.2665974225001264e-07\n",
      "Epoch 1/1 | Step 745000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 746000/915532 | Loss: 7.97207064806571e-07\n",
      "Epoch 1/1 | Step 747000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 748000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 749000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 750000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 751000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 752000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 753000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 754000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 755000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 756000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 757000/915532 | Loss: 1.7881377800677e-07\n",
      "Epoch 1/1 | Step 758000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 759000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 760000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 761000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 762000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 763000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 764000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 765000/915532 | Loss: 1.5646199358343438e-07\n",
      "Epoch 1/1 | Step 766000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 767000/915532 | Loss: 2.533192287046404e-07\n",
      "Epoch 1/1 | Step 768000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 769000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 770000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 771000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 772000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 773000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 774000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 775000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 776000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 777000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 778000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 779000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 780000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 781000/915532 | Loss: 8.121079986267432e-07\n",
      "Epoch 1/1 | Step 782000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 783000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 784000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 785000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 786000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 787000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 788000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 789000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 790000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 791000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 792000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 793000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 794000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 795000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 796000/915532 | Loss: 5.2154042862184724e-08\n",
      "Epoch 1/1 | Step 797000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 798000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 799000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 800000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 801000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 802000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 803000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 804000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 805000/915532 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 806000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 807000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 808000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 809000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 810000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 811000/915532 | Loss: 1.2665974225001264e-07\n",
      "Epoch 1/1 | Step 812000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 813000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 814000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 815000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 816000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 817000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 818000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 819000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 820000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 821000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 822000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 823000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 824000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 825000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 826000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 827000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 828000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 829000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 830000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 831000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 832000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 833000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 834000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 835000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 836000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 837000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 838000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 839000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 840000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 841000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 842000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 843000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 844000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 845000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 846000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 847000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 848000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 849000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 850000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 851000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 852000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 853000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 854000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 855000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 856000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 857000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 858000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 859000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 860000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 861000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 862000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 863000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 864000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 865000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 866000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 867000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 868000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 869000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 870000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 871000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 872000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 873000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 874000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 875000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 876000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 877000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 878000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 879000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 880000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 881000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 882000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 883000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 884000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 885000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 886000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 887000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 888000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 889000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 890000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 891000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 892000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 893000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 894000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 895000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 896000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 897000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 898000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 899000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 900000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 901000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 902000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 903000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 904000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 905000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 906000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 907000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 908000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 909000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 910000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 911000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 912000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 913000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 914000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 915000/915532 | Loss: 0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m  \u001b[38;5;66;03m# End time for the epoch\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m epoch_end_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate average loss and time taken for the epoch\u001b[39;00m\n\u001b[0;32m     32\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader)} | Loss: {loss.item()}\")\n",
    "\n",
    "     # End time for the epoch\n",
    "    # epoch_end_time = time.time()\n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    " Time Taken: {epoch_time:.2f} seconds\")\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\")#\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.05677984567414616\n"
     ]
    }
   ],
   "source": [
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m b_input_ids, b_input_mask, b_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      8\u001b[0m b_input_ids, b_input_mask \u001b[38;5;241m=\u001b[39m b_input_ids\u001b[38;5;241m.\u001b[39mto(device), b_input_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     12\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:798\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    795\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    796\u001b[0m         )\n\u001b[1;32m--> 798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    544\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    545\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m         output_attentions,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:495\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    492\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    496\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:429\u001b[0m, in \u001b[0;36mFFN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:432\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 432\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m    434\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(b_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Month', 'place', 'Crop_QueryType', 'Crop_QueryType_code', 'text1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Crop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Crop'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m predictions_flat\u001b[38;5;241m=\u001b[39mpredictions\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predicted_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m: data\u001b[38;5;241m.\u001b[39mloc[test_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlace\u001b[39m\u001b[38;5;124m'\u001b[39m: data\u001b[38;5;241m.\u001b[39mloc[test_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplace\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQueryType_code\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_flat\n\u001b[0;32m     11\u001b[0m })\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4287\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_level:\n\u001b[1;32m-> 4287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   4288\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Crop'"
     ]
    }
   ],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "#predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "predictions_flat=predictions\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data = pd.DataFrame({\n",
    "    'Month': data.loc[test_idx, 'Month'].values, \n",
    "    'Place': data.loc[test_idx, 'place'].values,\n",
    "    'Crop' : data.loc[test_idx, 'Crop'].values,\n",
    "    'QueryType_code': predictions_flat\n",
    "})\n",
    "print(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'QueryType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'QueryType'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQueryType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQueryType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m predicted_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQueryType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mpredicted_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQueryType_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQueryType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories[x])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'QueryType'"
     ]
    }
   ],
   "source": [
    "data['QueryType'] = data['QueryType'].astype('category')\n",
    "predicted_data['QueryType']=predicted_data['QueryType_code'].apply(lambda x: data['QueryType'].cat.categories[x])\n",
    "print(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Month                       Place                  Crop  \\\n",
      "0           1     A AND N ISLANDS_NICOBAR       Ash Gourd Petha   \n",
      "1           1     A AND N ISLANDS_NICOBAR              Chillies   \n",
      "2           1     A AND N ISLANDS_NICOBAR        POULTRY  FARM    \n",
      "3           1     A AND N ISLANDS_NICOBAR            Paddy Dhan   \n",
      "4           1     A AND N ISLANDS_NICOBAR  Sugarcane Noble Cane   \n",
      "...       ...                         ...                   ...   \n",
      "337110     12  WEST BENGAL_WEST MEDINIPUR                  Teak   \n",
      "337111     12  WEST BENGAL_WEST MEDINIPUR                Tomato   \n",
      "337112     12  WEST BENGAL_WEST MEDINIPUR              Tuberose   \n",
      "337113     12  WEST BENGAL_WEST MEDINIPUR            Watermelon   \n",
      "337114     12  WEST BENGAL_WEST MEDINIPUR                 Wheat   \n",
      "\n",
      "                                                QueryType  \n",
      "0       Index(['Weather'], dtype='object', name=(1, 'A...  \n",
      "1       Index(['Nutrient Management'], dtype='object',...  \n",
      "2       Index(['Livestock Products Processing and Pack...  \n",
      "3       Index(['Nutrient Management', 'Seeds'], dtype=...  \n",
      "4       Index(['Nutrient Management'], dtype='object',...  \n",
      "...                                                   ...  \n",
      "337110  Index(['Cultural Practices'], dtype='object', ...  \n",
      "337111  Index(['Plant Protection', 'Fertilizer Use and...  \n",
      "337112  Index(['Cultural Practices', 'Plant Protection...  \n",
      "337113  Index(['Cultural Practices'], dtype='object', ...  \n",
      "337114  Index(['Plant Protection', 'Cultural Practices...  \n",
      "\n",
      "[337115 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions = predicted_data.groupby(['Month', 'Place', 'Crop'])['QueryType'].apply(lambda x: x.value_counts().index[:10]).reset_index()\n",
    "print(monthly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_predictions= monthly_predictions.explode('QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "top_n = 10  # Change this value to get top N Crop_QueryTypes\n",
    "top_querytypes = (\n",
    "    predicted_data.groupby(['Month', 'Place','Crop'])['QueryType']\n",
    "    .apply(lambda x: x.value_counts().head(top_n).index.tolist())\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "top_querytypes.columns = ['Month', 'Place','Crop', 'Top_QueryTypes']\n",
    "\n",
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "top_querytypes.to_csv('D:/Objective 1 code/top_cropwise_querytypes_list.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_predictions= monthly_predictions.explode('QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions = monthly_predictions.groupby(['Month', 'Place','Crop'])['QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_predictions[['Month', 'Place','Crop','QueryType']].to_csv('D:/Data/Top_predicted_cropwise_querytypes_in_India_111.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "f1_score() got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Print the metrics\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: f1_score() got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def precision(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Positives (FP)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    \n",
    "    # Calculate Precision\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(true_labels, predictions):\n",
    "    # Count the number of correct predictions\n",
    "    correct = np.sum(np.array(true_labels) == np.array(predictions))\n",
    "    # Calculate accuracy\n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels, predictions)\n",
    "mae = mae(true_labels, predictions)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "recall = recall(true_labels, predictions)\n",
    "\n",
    "accuracy= accuracy(true_labels, predictions)\n",
    "precision= precision(true_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=data['QueryType'].cat.categories, \n",
    "#             yticklabels=data['QueryType'].cat.categories)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "      Metric     Value\n",
      "0   Accuracy  1.000000\n",
      "1  Precision  1.000000\n",
      "2     Recall  1.000000\n",
      "3   F1-Score  1.000000\n",
      "4       RMSE  0.019335\n",
      "5        MAE  0.000010\n"
     ]
    }
   ],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rmse, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rmse, mae]\n",
    "})\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/distilbert1_model_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training regarding the Crop_QueryType analysis#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_37300\\2764136759.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_37300\\2764136759.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "df[\"Crop_QueryType\"] = df[\"Crop\"] + \"_\" +df[\"QueryType\"]\n",
    "data=df[['Month','place','Crop_QueryType']]\n",
    "\n",
    "data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes\n",
    "\n",
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs using DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "inputs1 = tokenizer(data['text1'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Extract labels for training (QueryType codes)\n",
    "labels1 = torch.tensor(data['Crop_QueryType_code'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1, train_idx1, test_idx1 = train_test_split(\n",
    "    inputs1['input_ids'], labels1, data.index, test_size=0.2, random_state=42)\n",
    "train_masks1, test_masks1 = train_test_split(inputs1['attention_mask'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_37300\\419209612.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_37300\\419209612.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
    "train_sampler1 = RandomSampler(train_data1)\n",
    "train_dataloader1 = DataLoader(train_data1, sampler=train_sampler1, batch_size=batch_size)\n",
    "\n",
    "test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n",
    "test_sampler1 = SequentialSampler(test_data1)\n",
    "test_dataloader1 = DataLoader(test_data1, sampler=test_sampler1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=8129, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=data['Crop_QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 0/915532 | Loss: 1.9406474828720093\n",
      "Epoch 1/1 | Step 1000/915532 | Loss: 0.3966003358364105\n",
      "Epoch 1/1 | Step 2000/915532 | Loss: 0.03674320876598358\n",
      "Epoch 1/1 | Step 3000/915532 | Loss: 0.5051132440567017\n",
      "Epoch 1/1 | Step 4000/915532 | Loss: 1.341935157775879\n",
      "Epoch 1/1 | Step 5000/915532 | Loss: 1.1538422107696533\n",
      "Epoch 1/1 | Step 6000/915532 | Loss: 0.0064475336112082005\n",
      "Epoch 1/1 | Step 7000/915532 | Loss: 0.6357470750808716\n",
      "Epoch 1/1 | Step 8000/915532 | Loss: 0.0025762412697076797\n",
      "Epoch 1/1 | Step 9000/915532 | Loss: 0.31868141889572144\n",
      "Epoch 1/1 | Step 10000/915532 | Loss: 0.02386941760778427\n",
      "Epoch 1/1 | Step 11000/915532 | Loss: 1.0250197649002075\n",
      "Epoch 1/1 | Step 12000/915532 | Loss: 0.0015231339493766427\n",
      "Epoch 1/1 | Step 13000/915532 | Loss: 0.0030500770080834627\n",
      "Epoch 1/1 | Step 14000/915532 | Loss: 1.0202518701553345\n",
      "Epoch 1/1 | Step 15000/915532 | Loss: 0.28092917799949646\n",
      "Epoch 1/1 | Step 16000/915532 | Loss: 0.0038572936318814754\n",
      "Epoch 1/1 | Step 17000/915532 | Loss: 0.009290733374655247\n",
      "Epoch 1/1 | Step 18000/915532 | Loss: 1.0081969499588013\n",
      "Epoch 1/1 | Step 19000/915532 | Loss: 0.003062087344005704\n",
      "Epoch 1/1 | Step 20000/915532 | Loss: 0.5237137079238892\n",
      "Epoch 1/1 | Step 21000/915532 | Loss: 0.5680418014526367\n",
      "Epoch 1/1 | Step 22000/915532 | Loss: 0.11978787928819656\n",
      "Epoch 1/1 | Step 23000/915532 | Loss: 0.0006168298423290253\n",
      "Epoch 1/1 | Step 24000/915532 | Loss: 0.5218558311462402\n",
      "Epoch 1/1 | Step 25000/915532 | Loss: 0.2948002517223358\n",
      "Epoch 1/1 | Step 26000/915532 | Loss: 0.00027008747565560043\n",
      "Epoch 1/1 | Step 27000/915532 | Loss: 0.0018890353385359049\n",
      "Epoch 1/1 | Step 28000/915532 | Loss: 0.00025407911743968725\n",
      "Epoch 1/1 | Step 29000/915532 | Loss: 0.0009667656267993152\n",
      "Epoch 1/1 | Step 30000/915532 | Loss: 0.000320187013130635\n",
      "Epoch 1/1 | Step 31000/915532 | Loss: 0.00032505072886124253\n",
      "Epoch 1/1 | Step 32000/915532 | Loss: 0.4429382085800171\n",
      "Epoch 1/1 | Step 33000/915532 | Loss: 0.00372195802628994\n",
      "Epoch 1/1 | Step 34000/915532 | Loss: 0.00013865898654330522\n",
      "Epoch 1/1 | Step 35000/915532 | Loss: 4.22571538365446e-05\n",
      "Epoch 1/1 | Step 36000/915532 | Loss: 0.13967560231685638\n",
      "Epoch 1/1 | Step 37000/915532 | Loss: 0.537870466709137\n",
      "Epoch 1/1 | Step 38000/915532 | Loss: 0.2875823676586151\n",
      "Epoch 1/1 | Step 39000/915532 | Loss: 0.5366261005401611\n",
      "Epoch 1/1 | Step 40000/915532 | Loss: 0.00037472951225936413\n",
      "Epoch 1/1 | Step 41000/915532 | Loss: 0.004036384634673595\n",
      "Epoch 1/1 | Step 42000/915532 | Loss: 0.0001604207936907187\n",
      "Epoch 1/1 | Step 43000/915532 | Loss: 2.959254925372079e-05\n",
      "Epoch 1/1 | Step 44000/915532 | Loss: 6.320314423646778e-05\n",
      "Epoch 1/1 | Step 45000/915532 | Loss: 0.0002427070721751079\n",
      "Epoch 1/1 | Step 46000/915532 | Loss: 0.00016702801804058254\n",
      "Epoch 1/1 | Step 47000/915532 | Loss: 0.29313263297080994\n",
      "Epoch 1/1 | Step 48000/915532 | Loss: 2.688044696697034e-05\n",
      "Epoch 1/1 | Step 49000/915532 | Loss: 0.2671636641025543\n",
      "Epoch 1/1 | Step 50000/915532 | Loss: 0.0009134752326644957\n",
      "Epoch 1/1 | Step 51000/915532 | Loss: 0.0010087131522595882\n",
      "Epoch 1/1 | Step 52000/915532 | Loss: 9.012221562443301e-05\n",
      "Epoch 1/1 | Step 53000/915532 | Loss: 9.639979543862864e-05\n",
      "Epoch 1/1 | Step 54000/915532 | Loss: 5.19916029588785e-05\n",
      "Epoch 1/1 | Step 55000/915532 | Loss: 0.0012435128446668386\n",
      "Epoch 1/1 | Step 56000/915532 | Loss: 0.0002308741386514157\n",
      "Epoch 1/1 | Step 57000/915532 | Loss: 0.00026105291908606887\n",
      "Epoch 1/1 | Step 58000/915532 | Loss: 0.3887512981891632\n",
      "Epoch 1/1 | Step 59000/915532 | Loss: 1.2151499504398089e-05\n",
      "Epoch 1/1 | Step 60000/915532 | Loss: 3.927640136680566e-05\n",
      "Epoch 1/1 | Step 61000/915532 | Loss: 0.0003382677386980504\n",
      "Epoch 1/1 | Step 62000/915532 | Loss: 7.392655243165791e-05\n",
      "Epoch 1/1 | Step 63000/915532 | Loss: 3.4672877518460155e-05\n",
      "Epoch 1/1 | Step 64000/915532 | Loss: 2.737903196248226e-05\n",
      "Epoch 1/1 | Step 65000/915532 | Loss: 0.005769857205450535\n",
      "Epoch 1/1 | Step 66000/915532 | Loss: 1.5727655409136787e-05\n",
      "Epoch 1/1 | Step 67000/915532 | Loss: 1.5310182789107785e-05\n",
      "Epoch 1/1 | Step 68000/915532 | Loss: 7.747430936433375e-05\n",
      "Epoch 1/1 | Step 69000/915532 | Loss: 3.339200702612288e-05\n",
      "Epoch 1/1 | Step 70000/915532 | Loss: 2.4861175916157663e-05\n",
      "Epoch 1/1 | Step 71000/915532 | Loss: 0.366605281829834\n",
      "Epoch 1/1 | Step 72000/915532 | Loss: 8.730999252293259e-05\n",
      "Epoch 1/1 | Step 73000/915532 | Loss: 2.0658844732679427e-05\n",
      "Epoch 1/1 | Step 74000/915532 | Loss: 0.9078900218009949\n",
      "Epoch 1/1 | Step 75000/915532 | Loss: 9.65579238254577e-06\n",
      "Epoch 1/1 | Step 76000/915532 | Loss: 3.470852243481204e-05\n",
      "Epoch 1/1 | Step 77000/915532 | Loss: 2.9346620067371987e-05\n",
      "Epoch 1/1 | Step 78000/915532 | Loss: 0.00012446958862710744\n",
      "Epoch 1/1 | Step 79000/915532 | Loss: 9.402527211932465e-06\n",
      "Epoch 1/1 | Step 80000/915532 | Loss: 0.19720390439033508\n",
      "Epoch 1/1 | Step 81000/915532 | Loss: 0.00021497989655472338\n",
      "Epoch 1/1 | Step 82000/915532 | Loss: 3.684759576572105e-05\n",
      "Epoch 1/1 | Step 83000/915532 | Loss: 2.5837029170361347e-05\n",
      "Epoch 1/1 | Step 84000/915532 | Loss: 3.4194283216493204e-05\n",
      "Epoch 1/1 | Step 85000/915532 | Loss: 6.624712113989517e-05\n",
      "Epoch 1/1 | Step 86000/915532 | Loss: 0.05745445191860199\n",
      "Epoch 1/1 | Step 87000/915532 | Loss: 0.00034849619260057807\n",
      "Epoch 1/1 | Step 88000/915532 | Loss: 6.678437057416886e-05\n",
      "Epoch 1/1 | Step 89000/915532 | Loss: 0.03356532007455826\n",
      "Epoch 1/1 | Step 90000/915532 | Loss: 0.5443532466888428\n",
      "Epoch 1/1 | Step 91000/915532 | Loss: 0.20131728053092957\n",
      "Epoch 1/1 | Step 92000/915532 | Loss: 1.1115772394987289e-05\n",
      "Epoch 1/1 | Step 93000/915532 | Loss: 0.01444683875888586\n",
      "Epoch 1/1 | Step 94000/915532 | Loss: 4.390937101561576e-05\n",
      "Epoch 1/1 | Step 95000/915532 | Loss: 0.0002469570899847895\n",
      "Epoch 1/1 | Step 96000/915532 | Loss: 9.185270027955994e-05\n",
      "Epoch 1/1 | Step 97000/915532 | Loss: 2.2499540136777796e-05\n",
      "Epoch 1/1 | Step 98000/915532 | Loss: 3.2931200166785857e-06\n",
      "Epoch 1/1 | Step 99000/915532 | Loss: 0.00016425413195975125\n",
      "Epoch 1/1 | Step 100000/915532 | Loss: 0.0003765762667171657\n",
      "Epoch 1/1 | Step 101000/915532 | Loss: 0.0006036207196302712\n",
      "Epoch 1/1 | Step 102000/915532 | Loss: 1.1528724431991577\n",
      "Epoch 1/1 | Step 103000/915532 | Loss: 0.00016532930021639913\n",
      "Epoch 1/1 | Step 104000/915532 | Loss: 2.7558355213841423e-05\n",
      "Epoch 1/1 | Step 105000/915532 | Loss: 2.6343235731474124e-05\n",
      "Epoch 1/1 | Step 106000/915532 | Loss: 0.00357280345633626\n",
      "Epoch 1/1 | Step 107000/915532 | Loss: 0.0006803158321417868\n",
      "Epoch 1/1 | Step 108000/915532 | Loss: 8.013602928258479e-05\n",
      "Epoch 1/1 | Step 109000/915532 | Loss: 0.0008256215951405466\n",
      "Epoch 1/1 | Step 110000/915532 | Loss: 0.0010414611315354705\n",
      "Epoch 1/1 | Step 111000/915532 | Loss: 0.00011411821469664574\n",
      "Epoch 1/1 | Step 112000/915532 | Loss: 0.5493175387382507\n",
      "Epoch 1/1 | Step 113000/915532 | Loss: 0.05640494450926781\n",
      "Epoch 1/1 | Step 114000/915532 | Loss: 0.0003537947195582092\n",
      "Epoch 1/1 | Step 115000/915532 | Loss: 0.0005997999105602503\n",
      "Epoch 1/1 | Step 116000/915532 | Loss: 0.25207841396331787\n",
      "Epoch 1/1 | Step 117000/915532 | Loss: 0.0004957923083566129\n",
      "Epoch 1/1 | Step 118000/915532 | Loss: 0.4952264428138733\n",
      "Epoch 1/1 | Step 119000/915532 | Loss: 1.5027067092887592e-05\n",
      "Epoch 1/1 | Step 120000/915532 | Loss: 0.00014862653915770352\n",
      "Epoch 1/1 | Step 121000/915532 | Loss: 7.152375928853871e-06\n",
      "Epoch 1/1 | Step 122000/915532 | Loss: 6.093805859563872e-05\n",
      "Epoch 1/1 | Step 123000/915532 | Loss: 4.395791620481759e-06\n",
      "Epoch 1/1 | Step 124000/915532 | Loss: 0.5416657328605652\n",
      "Epoch 1/1 | Step 125000/915532 | Loss: 1.2088093757629395\n",
      "Epoch 1/1 | Step 126000/915532 | Loss: 0.0022385420743376017\n",
      "Epoch 1/1 | Step 127000/915532 | Loss: 0.00016813661204650998\n",
      "Epoch 1/1 | Step 128000/915532 | Loss: 8.893571066437289e-05\n",
      "Epoch 1/1 | Step 129000/915532 | Loss: 0.0008104281150735915\n",
      "Epoch 1/1 | Step 130000/915532 | Loss: 0.5412467122077942\n",
      "Epoch 1/1 | Step 131000/915532 | Loss: 8.404559775954112e-05\n",
      "Epoch 1/1 | Step 132000/915532 | Loss: 3.8918897189432755e-05\n",
      "Epoch 1/1 | Step 133000/915532 | Loss: 1.6860172763699666e-05\n",
      "Epoch 1/1 | Step 134000/915532 | Loss: 0.004309884272515774\n",
      "Epoch 1/1 | Step 135000/915532 | Loss: 3.441044464125298e-05\n",
      "Epoch 1/1 | Step 136000/915532 | Loss: 3.956196451326832e-06\n",
      "Epoch 1/1 | Step 137000/915532 | Loss: 0.0001696428080322221\n",
      "Epoch 1/1 | Step 138000/915532 | Loss: 7.085397555783857e-06\n",
      "Epoch 1/1 | Step 139000/915532 | Loss: 0.028815876692533493\n",
      "Epoch 1/1 | Step 140000/915532 | Loss: 1.630877159186639e-05\n",
      "Epoch 1/1 | Step 141000/915532 | Loss: 0.0001521934464108199\n",
      "Epoch 1/1 | Step 142000/915532 | Loss: 0.00019149309082422405\n",
      "Epoch 1/1 | Step 143000/915532 | Loss: 0.5826907157897949\n",
      "Epoch 1/1 | Step 144000/915532 | Loss: 0.14270485937595367\n",
      "Epoch 1/1 | Step 145000/915532 | Loss: 0.0007490779971703887\n",
      "Epoch 1/1 | Step 146000/915532 | Loss: 1.0132760053238599e-06\n",
      "Epoch 1/1 | Step 147000/915532 | Loss: 0.0006486380589194596\n",
      "Epoch 1/1 | Step 148000/915532 | Loss: 1.423056914973131e-06\n",
      "Epoch 1/1 | Step 149000/915532 | Loss: 9.38731955102412e-06\n",
      "Epoch 1/1 | Step 150000/915532 | Loss: 0.009934554807841778\n",
      "Epoch 1/1 | Step 151000/915532 | Loss: 1.0071513652801514\n",
      "Epoch 1/1 | Step 152000/915532 | Loss: 3.18599195452407e-05\n",
      "Epoch 1/1 | Step 153000/915532 | Loss: 0.0001251339417649433\n",
      "Epoch 1/1 | Step 154000/915532 | Loss: 2.2014444766682573e-05\n",
      "Epoch 1/1 | Step 155000/915532 | Loss: 3.366071541677229e-05\n",
      "Epoch 1/1 | Step 156000/915532 | Loss: 4.388952220324427e-05\n",
      "Epoch 1/1 | Step 157000/915532 | Loss: 2.70346827164758e-05\n",
      "Epoch 1/1 | Step 158000/915532 | Loss: 5.975151452730643e-06\n",
      "Epoch 1/1 | Step 159000/915532 | Loss: 1.8140935935662128e-05\n",
      "Epoch 1/1 | Step 160000/915532 | Loss: 3.41192317137029e-05\n",
      "Epoch 1/1 | Step 161000/915532 | Loss: 4.1315237467642874e-05\n",
      "Epoch 1/1 | Step 162000/915532 | Loss: 3.7848728879907867e-06\n",
      "Epoch 1/1 | Step 163000/915532 | Loss: 6.332806151476689e-06\n",
      "Epoch 1/1 | Step 164000/915532 | Loss: 1.6018649375837413e-06\n",
      "Epoch 1/1 | Step 165000/915532 | Loss: 9.559410682413727e-05\n",
      "Epoch 1/1 | Step 166000/915532 | Loss: 0.5858502388000488\n",
      "Epoch 1/1 | Step 167000/915532 | Loss: 2.491230952728074e-05\n",
      "Epoch 1/1 | Step 168000/915532 | Loss: 1.497561584073992e-06\n",
      "Epoch 1/1 | Step 169000/915532 | Loss: 0.000620481965597719\n",
      "Epoch 1/1 | Step 170000/915532 | Loss: 7.621086842846125e-05\n",
      "Epoch 1/1 | Step 171000/915532 | Loss: 0.00010832391126314178\n",
      "Epoch 1/1 | Step 172000/915532 | Loss: 1.6093218846435775e-06\n",
      "Epoch 1/1 | Step 173000/915532 | Loss: 2.44797865889268e-05\n",
      "Epoch 1/1 | Step 174000/915532 | Loss: 2.4756936909398064e-05\n",
      "Epoch 1/1 | Step 175000/915532 | Loss: 9.692928870208561e-06\n",
      "Epoch 1/1 | Step 176000/915532 | Loss: 9.640547432354651e-06\n",
      "Epoch 1/1 | Step 177000/915532 | Loss: 4.097369310329668e-05\n",
      "Epoch 1/1 | Step 178000/915532 | Loss: 1.1592705959628802e-05\n",
      "Epoch 1/1 | Step 179000/915532 | Loss: 0.7197160124778748\n",
      "Epoch 1/1 | Step 180000/915532 | Loss: 0.0026576665695756674\n",
      "Epoch 1/1 | Step 181000/915532 | Loss: 0.10140715539455414\n",
      "Epoch 1/1 | Step 182000/915532 | Loss: 6.228450729395263e-06\n",
      "Epoch 1/1 | Step 183000/915532 | Loss: 0.00021403952268883586\n",
      "Epoch 1/1 | Step 184000/915532 | Loss: 2.1449106498039328e-05\n",
      "Epoch 1/1 | Step 185000/915532 | Loss: 9.335431059298571e-06\n",
      "Epoch 1/1 | Step 186000/915532 | Loss: 0.43188726902008057\n",
      "Epoch 1/1 | Step 187000/915532 | Loss: 4.745952537632547e-06\n",
      "Epoch 1/1 | Step 188000/915532 | Loss: 0.0007126819109544158\n",
      "Epoch 1/1 | Step 189000/915532 | Loss: 3.7629393773386255e-05\n",
      "Epoch 1/1 | Step 190000/915532 | Loss: 4.701243142335443e-06\n",
      "Epoch 1/1 | Step 191000/915532 | Loss: 2.8717866371152923e-05\n",
      "Epoch 1/1 | Step 192000/915532 | Loss: 8.89578768692445e-06\n",
      "Epoch 1/1 | Step 193000/915532 | Loss: 7.948832353577018e-05\n",
      "Epoch 1/1 | Step 194000/915532 | Loss: 9.238697771252191e-07\n",
      "Epoch 1/1 | Step 195000/915532 | Loss: 0.5774919986724854\n",
      "Epoch 1/1 | Step 196000/915532 | Loss: 0.006245287135243416\n",
      "Epoch 1/1 | Step 197000/915532 | Loss: 4.499997885432094e-06\n",
      "Epoch 1/1 | Step 198000/915532 | Loss: 2.9578422982012853e-06\n",
      "Epoch 1/1 | Step 199000/915532 | Loss: 1.0869594916584902e-05\n",
      "Epoch 1/1 | Step 200000/915532 | Loss: 2.5805124096223153e-05\n",
      "Epoch 1/1 | Step 201000/915532 | Loss: 3.307957013021223e-05\n",
      "Epoch 1/1 | Step 202000/915532 | Loss: 1.3909611880080774e-05\n",
      "Epoch 1/1 | Step 203000/915532 | Loss: 0.0013763897586613894\n",
      "Epoch 1/1 | Step 204000/915532 | Loss: 0.0015332545153796673\n",
      "Epoch 1/1 | Step 205000/915532 | Loss: 6.429710083466489e-06\n",
      "Epoch 1/1 | Step 206000/915532 | Loss: 5.297225470712874e-06\n",
      "Epoch 1/1 | Step 207000/915532 | Loss: 2.0861418761342065e-06\n",
      "Epoch 1/1 | Step 208000/915532 | Loss: 1.0058267889689887e-06\n",
      "Epoch 1/1 | Step 209000/915532 | Loss: 0.00011510965850902721\n",
      "Epoch 1/1 | Step 210000/915532 | Loss: 1.5034749594633467e-05\n",
      "Epoch 1/1 | Step 211000/915532 | Loss: 9.715051419334486e-06\n",
      "Epoch 1/1 | Step 212000/915532 | Loss: 4.745892056234879e-06\n",
      "Epoch 1/1 | Step 213000/915532 | Loss: 0.0003022760502062738\n",
      "Epoch 1/1 | Step 214000/915532 | Loss: 4.090328729944304e-06\n",
      "Epoch 1/1 | Step 215000/915532 | Loss: 0.0010415546130388975\n",
      "Epoch 1/1 | Step 216000/915532 | Loss: 1.4773580915061757e-05\n",
      "Epoch 1/1 | Step 217000/915532 | Loss: 0.533051609992981\n",
      "Epoch 1/1 | Step 218000/915532 | Loss: 7.241880666697398e-06\n",
      "Epoch 1/1 | Step 219000/915532 | Loss: 3.250289591960609e-05\n",
      "Epoch 1/1 | Step 220000/915532 | Loss: 0.524918794631958\n",
      "Epoch 1/1 | Step 221000/915532 | Loss: 5.5878258535813075e-06\n",
      "Epoch 1/1 | Step 222000/915532 | Loss: 8.401551895076409e-05\n",
      "Epoch 1/1 | Step 223000/915532 | Loss: 0.23234614729881287\n",
      "Epoch 1/1 | Step 224000/915532 | Loss: 0.0002701526100281626\n",
      "Epoch 1/1 | Step 225000/915532 | Loss: 0.0004683096194639802\n",
      "Epoch 1/1 | Step 226000/915532 | Loss: 0.16105616092681885\n",
      "Epoch 1/1 | Step 227000/915532 | Loss: 2.8183070753584616e-05\n",
      "Epoch 1/1 | Step 228000/915532 | Loss: 0.00013336270058061928\n",
      "Epoch 1/1 | Step 229000/915532 | Loss: 0.2829990088939667\n",
      "Epoch 1/1 | Step 230000/915532 | Loss: 2.78649827123445e-06\n",
      "Epoch 1/1 | Step 231000/915532 | Loss: 1.4528559404425323e-06\n",
      "Epoch 1/1 | Step 232000/915532 | Loss: 1.6689240283085383e-06\n",
      "Epoch 1/1 | Step 233000/915532 | Loss: 1.0728799679782242e-06\n",
      "Epoch 1/1 | Step 234000/915532 | Loss: 0.48615607619285583\n",
      "Epoch 1/1 | Step 235000/915532 | Loss: 0.8248855471611023\n",
      "Epoch 1/1 | Step 236000/915532 | Loss: 0.5043068528175354\n",
      "Epoch 1/1 | Step 237000/915532 | Loss: 3.5911332361138193e-06\n",
      "Epoch 1/1 | Step 238000/915532 | Loss: 3.3825376704044174e-06\n",
      "Epoch 1/1 | Step 239000/915532 | Loss: 0.0035485613625496626\n",
      "Epoch 1/1 | Step 240000/915532 | Loss: 3.673095989142894e-06\n",
      "Epoch 1/1 | Step 241000/915532 | Loss: 6.0104641306679696e-05\n",
      "Epoch 1/1 | Step 242000/915532 | Loss: 3.76106399926357e-05\n",
      "Epoch 1/1 | Step 243000/915532 | Loss: 0.00013252245844341815\n",
      "Epoch 1/1 | Step 244000/915532 | Loss: 3.129240440102876e-07\n",
      "Epoch 1/1 | Step 245000/915532 | Loss: 8.679607162775937e-06\n",
      "Epoch 1/1 | Step 246000/915532 | Loss: 3.409451528568752e-05\n",
      "Epoch 1/1 | Step 247000/915532 | Loss: 0.01915897987782955\n",
      "Epoch 1/1 | Step 248000/915532 | Loss: 0.0021020793356001377\n",
      "Epoch 1/1 | Step 249000/915532 | Loss: 2.5182694116665516e-06\n",
      "Epoch 1/1 | Step 250000/915532 | Loss: 5.513416567737295e-07\n",
      "Epoch 1/1 | Step 251000/915532 | Loss: 3.882126111420803e-05\n",
      "Epoch 1/1 | Step 252000/915532 | Loss: 0.00025974595337174833\n",
      "Epoch 1/1 | Step 253000/915532 | Loss: 2.756711410256685e-07\n",
      "Epoch 1/1 | Step 254000/915532 | Loss: 1.3268194379634224e-05\n",
      "Epoch 1/1 | Step 255000/915532 | Loss: 6.630875759583432e-06\n",
      "Epoch 1/1 | Step 256000/915532 | Loss: 1.3336418760445667e-06\n",
      "Epoch 1/1 | Step 257000/915532 | Loss: 2.5108136014750926e-06\n",
      "Epoch 1/1 | Step 258000/915532 | Loss: 5.093171785119921e-05\n",
      "Epoch 1/1 | Step 259000/915532 | Loss: 4.321327082834614e-07\n",
      "Epoch 1/1 | Step 260000/915532 | Loss: 1.4795272363699041e-05\n",
      "Epoch 1/1 | Step 261000/915532 | Loss: 3.2037473829404917e-07\n",
      "Epoch 1/1 | Step 262000/915532 | Loss: 6.649243005085737e-05\n",
      "Epoch 1/1 | Step 263000/915532 | Loss: 1.586964003763569e-06\n",
      "Epoch 1/1 | Step 264000/915532 | Loss: 0.00995461456477642\n",
      "Epoch 1/1 | Step 265000/915532 | Loss: 5.795292236143723e-05\n",
      "Epoch 1/1 | Step 266000/915532 | Loss: 0.0005872884066775441\n",
      "Epoch 1/1 | Step 267000/915532 | Loss: 1.425958180334419e-05\n",
      "Epoch 1/1 | Step 268000/915532 | Loss: 5.483519998961128e-06\n",
      "Epoch 1/1 | Step 269000/915532 | Loss: 5.9080948631162755e-06\n",
      "Epoch 1/1 | Step 270000/915532 | Loss: 4.097717464901507e-06\n",
      "Epoch 1/1 | Step 271000/915532 | Loss: 2.458690460116486e-07\n",
      "Epoch 1/1 | Step 272000/915532 | Loss: 1.965244155144319e-05\n",
      "Epoch 1/1 | Step 273000/915532 | Loss: 3.6805049603572115e-06\n",
      "Epoch 1/1 | Step 274000/915532 | Loss: 3.538999862939818e-06\n",
      "Epoch 1/1 | Step 275000/915532 | Loss: 1.8952920072479174e-05\n",
      "Epoch 1/1 | Step 276000/915532 | Loss: 9.223350389220286e-06\n",
      "Epoch 1/1 | Step 277000/915532 | Loss: 0.4291733503341675\n",
      "Epoch 1/1 | Step 278000/915532 | Loss: 7.674090056752902e-07\n",
      "Epoch 1/1 | Step 279000/915532 | Loss: 6.90630404278636e-05\n",
      "Epoch 1/1 | Step 280000/915532 | Loss: 0.004499737173318863\n",
      "Epoch 1/1 | Step 281000/915532 | Loss: 3.4719209907052573e-06\n",
      "Epoch 1/1 | Step 282000/915532 | Loss: 1.1770874152716715e-05\n",
      "Epoch 1/1 | Step 283000/915532 | Loss: 0.001846805796958506\n",
      "Epoch 1/1 | Step 284000/915532 | Loss: 0.2883841097354889\n",
      "Epoch 1/1 | Step 285000/915532 | Loss: 6.660485269094352e-06\n",
      "Epoch 1/1 | Step 286000/915532 | Loss: 7.219249710033182e-06\n",
      "Epoch 1/1 | Step 287000/915532 | Loss: 3.037771602976136e-05\n",
      "Epoch 1/1 | Step 288000/915532 | Loss: 0.027779271826148033\n",
      "Epoch 1/1 | Step 289000/915532 | Loss: 0.0005469699390232563\n",
      "Epoch 1/1 | Step 290000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 291000/915532 | Loss: 0.20462185144424438\n",
      "Epoch 1/1 | Step 292000/915532 | Loss: 1.1681988326017745e-05\n",
      "Epoch 1/1 | Step 293000/915532 | Loss: 1.6964000678854063e-05\n",
      "Epoch 1/1 | Step 294000/915532 | Loss: 0.0023629949428141117\n",
      "Epoch 1/1 | Step 295000/915532 | Loss: 1.0631188160914462e-05\n",
      "Epoch 1/1 | Step 296000/915532 | Loss: 0.535798192024231\n",
      "Epoch 1/1 | Step 297000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 298000/915532 | Loss: 1.0803289569594199e-06\n",
      "Epoch 1/1 | Step 299000/915532 | Loss: 2.945345113403164e-05\n",
      "Epoch 1/1 | Step 300000/915532 | Loss: 1.1138356057927012e-05\n",
      "Epoch 1/1 | Step 301000/915532 | Loss: 0.0014177478151395917\n",
      "Epoch 1/1 | Step 302000/915532 | Loss: 2.6565254302113317e-05\n",
      "Epoch 1/1 | Step 303000/915532 | Loss: 9.625445272831712e-06\n",
      "Epoch 1/1 | Step 304000/915532 | Loss: 6.772401320631616e-06\n",
      "Epoch 1/1 | Step 305000/915532 | Loss: 2.3145183149608783e-05\n",
      "Epoch 1/1 | Step 306000/915532 | Loss: 6.086999292165274e-06\n",
      "Epoch 1/1 | Step 307000/915532 | Loss: 5.66242874810996e-07\n",
      "Epoch 1/1 | Step 308000/915532 | Loss: 3.948801463593554e-07\n",
      "Epoch 1/1 | Step 309000/915532 | Loss: 4.880015694652684e-06\n",
      "Epoch 1/1 | Step 310000/915532 | Loss: 7.919487870822195e-06\n",
      "Epoch 1/1 | Step 311000/915532 | Loss: 0.00019346810586284846\n",
      "Epoch 1/1 | Step 312000/915532 | Loss: 1.4901149825163884e-07\n",
      "Epoch 1/1 | Step 313000/915532 | Loss: 1.0207263585471082e-06\n",
      "Epoch 1/1 | Step 314000/915532 | Loss: 1.820804573071655e-05\n",
      "Epoch 1/1 | Step 315000/915532 | Loss: 0.0008624863694421947\n",
      "Epoch 1/1 | Step 316000/915532 | Loss: 4.619344622369681e-07\n",
      "Epoch 1/1 | Step 317000/915532 | Loss: 3.717746267284383e-06\n",
      "Epoch 1/1 | Step 318000/915532 | Loss: 1.144645094871521\n",
      "Epoch 1/1 | Step 319000/915532 | Loss: 1.5719446309958585e-05\n",
      "Epoch 1/1 | Step 320000/915532 | Loss: 0.10021354258060455\n",
      "Epoch 1/1 | Step 321000/915532 | Loss: 0.3672851026058197\n",
      "Epoch 1/1 | Step 322000/915532 | Loss: 5.081147719465662e-06\n",
      "Epoch 1/1 | Step 323000/915532 | Loss: 8.821167284622788e-05\n",
      "Epoch 1/1 | Step 324000/915532 | Loss: 0.00016577776113990694\n",
      "Epoch 1/1 | Step 325000/915532 | Loss: 3.129217702735332e-06\n",
      "Epoch 1/1 | Step 326000/915532 | Loss: 0.00023769236577209085\n",
      "Epoch 1/1 | Step 327000/915532 | Loss: 9.8347391030984e-07\n",
      "Epoch 1/1 | Step 328000/915532 | Loss: 0.0005814599571749568\n",
      "Epoch 1/1 | Step 329000/915532 | Loss: 2.598590981506277e-05\n",
      "Epoch 1/1 | Step 330000/915532 | Loss: 0.00015834544319659472\n",
      "Epoch 1/1 | Step 331000/915532 | Loss: 5.048047751188278e-05\n",
      "Epoch 1/1 | Step 332000/915532 | Loss: 0.3671817481517792\n",
      "Epoch 1/1 | Step 333000/915532 | Loss: 0.0014443695545196533\n",
      "Epoch 1/1 | Step 334000/915532 | Loss: 2.249720273539424e-05\n",
      "Epoch 1/1 | Step 335000/915532 | Loss: 4.291486220608931e-06\n",
      "Epoch 1/1 | Step 336000/915532 | Loss: 3.1739159567223396e-06\n",
      "Epoch 1/1 | Step 337000/915532 | Loss: 1.5123921912163496e-05\n",
      "Epoch 1/1 | Step 338000/915532 | Loss: 0.2486831694841385\n",
      "Epoch 1/1 | Step 339000/915532 | Loss: 4.693747541750781e-06\n",
      "Epoch 1/1 | Step 340000/915532 | Loss: 0.04276236146688461\n",
      "Epoch 1/1 | Step 341000/915532 | Loss: 0.00013020537153352052\n",
      "Epoch 1/1 | Step 342000/915532 | Loss: 2.8311994810792385e-06\n",
      "Epoch 1/1 | Step 343000/915532 | Loss: 3.747550863408833e-06\n",
      "Epoch 1/1 | Step 344000/915532 | Loss: 8.19563723553074e-08\n",
      "Epoch 1/1 | Step 345000/915532 | Loss: 8.20268178358674e-06\n",
      "Epoch 1/1 | Step 346000/915532 | Loss: 0.0008984663872979581\n",
      "Epoch 1/1 | Step 347000/915532 | Loss: 5.006714673072565e-06\n",
      "Epoch 1/1 | Step 348000/915532 | Loss: 0.0001592617918504402\n",
      "Epoch 1/1 | Step 349000/915532 | Loss: 0.00020874306210316718\n",
      "Epoch 1/1 | Step 350000/915532 | Loss: 3.725286319422594e-07\n",
      "Epoch 1/1 | Step 351000/915532 | Loss: 1.668911068009038e-06\n",
      "Epoch 1/1 | Step 352000/915532 | Loss: 0.00031475795549340546\n",
      "Epoch 1/1 | Step 353000/915532 | Loss: 1.228512337547727e-05\n",
      "Epoch 1/1 | Step 354000/915532 | Loss: 0.0003131003468297422\n",
      "Epoch 1/1 | Step 355000/915532 | Loss: 4.90986440127017e-06\n",
      "Epoch 1/1 | Step 356000/915532 | Loss: 2.801359187287744e-06\n",
      "Epoch 1/1 | Step 357000/915532 | Loss: 5.736936827815953e-07\n",
      "Epoch 1/1 | Step 358000/915532 | Loss: 0.29464250802993774\n",
      "Epoch 1/1 | Step 359000/915532 | Loss: 1.6391270207805064e-07\n",
      "Epoch 1/1 | Step 360000/915532 | Loss: 3.650777387065318e-07\n",
      "Epoch 1/1 | Step 361000/915532 | Loss: 0.00018659752095118165\n",
      "Epoch 1/1 | Step 362000/915532 | Loss: 1.3634505648951745e-06\n",
      "Epoch 1/1 | Step 363000/915532 | Loss: 6.854514822407509e-07\n",
      "Epoch 1/1 | Step 364000/915532 | Loss: 2.503372797946213e-06\n",
      "Epoch 1/1 | Step 365000/915532 | Loss: 5.937875357631128e-06\n",
      "Epoch 1/1 | Step 366000/915532 | Loss: 2.4214100449171383e-06\n",
      "Epoch 1/1 | Step 367000/915532 | Loss: 7.00353268712206e-07\n",
      "Epoch 1/1 | Step 368000/915532 | Loss: 4.246827245424356e-07\n",
      "Epoch 1/1 | Step 369000/915532 | Loss: 2.607699798318208e-07\n",
      "Epoch 1/1 | Step 370000/915532 | Loss: 6.805446173530072e-05\n",
      "Epoch 1/1 | Step 371000/915532 | Loss: 7.894513692008331e-05\n",
      "Epoch 1/1 | Step 372000/915532 | Loss: 0.7152612805366516\n",
      "Epoch 1/1 | Step 373000/915532 | Loss: 0.0002170009393012151\n",
      "Epoch 1/1 | Step 374000/915532 | Loss: 0.00421377457678318\n",
      "Epoch 1/1 | Step 375000/915532 | Loss: 6.556498988175008e-07\n",
      "Epoch 1/1 | Step 376000/915532 | Loss: 3.437535997363739e-05\n",
      "Epoch 1/1 | Step 377000/915532 | Loss: 0.6427861452102661\n",
      "Epoch 1/1 | Step 378000/915532 | Loss: 2.235172757991677e-07\n",
      "Epoch 1/1 | Step 379000/915532 | Loss: 6.124157152953558e-06\n",
      "Epoch 1/1 | Step 380000/915532 | Loss: 0.0003005331673193723\n",
      "Epoch 1/1 | Step 381000/915532 | Loss: 8.940669999901729e-07\n",
      "Epoch 1/1 | Step 382000/915532 | Loss: 0.00013803355977870524\n",
      "Epoch 1/1 | Step 383000/915532 | Loss: 8.10582423582673e-06\n",
      "Epoch 1/1 | Step 384000/915532 | Loss: 6.40747998659208e-07\n",
      "Epoch 1/1 | Step 385000/915532 | Loss: 1.5003904081822839e-05\n",
      "Epoch 1/1 | Step 386000/915532 | Loss: 1.527362428532797e-06\n",
      "Epoch 1/1 | Step 387000/915532 | Loss: 1.4901152667334827e-07\n",
      "Epoch 1/1 | Step 388000/915532 | Loss: 7.253904914250597e-05\n",
      "Epoch 1/1 | Step 389000/915532 | Loss: 8.411330782109872e-06\n",
      "Epoch 1/1 | Step 390000/915532 | Loss: 7.003516202530591e-07\n",
      "Epoch 1/1 | Step 391000/915532 | Loss: 0.46476125717163086\n",
      "Epoch 1/1 | Step 392000/915532 | Loss: 1.1085689948231447e-05\n",
      "Epoch 1/1 | Step 393000/915532 | Loss: 4.136842107982375e-05\n",
      "Epoch 1/1 | Step 394000/915532 | Loss: 1.7136326846411976e-07\n",
      "Epoch 1/1 | Step 395000/915532 | Loss: 5.587918394667213e-07\n",
      "Epoch 1/1 | Step 396000/915532 | Loss: 1.7583281533006812e-06\n",
      "Epoch 1/1 | Step 397000/915532 | Loss: 0.0005198547733016312\n",
      "Epoch 1/1 | Step 398000/915532 | Loss: 2.74943740805611e-05\n",
      "Epoch 1/1 | Step 399000/915532 | Loss: 3.803111030720174e-05\n",
      "Epoch 1/1 | Step 400000/915532 | Loss: 0.4744859039783478\n",
      "Epoch 1/1 | Step 401000/915532 | Loss: 5.0513372116256505e-06\n",
      "Epoch 1/1 | Step 402000/915532 | Loss: 1.1018467375834007e-05\n",
      "Epoch 1/1 | Step 403000/915532 | Loss: 1.1920926112907182e-07\n",
      "Epoch 1/1 | Step 404000/915532 | Loss: 5.215391638557776e-07\n",
      "Epoch 1/1 | Step 405000/915532 | Loss: 1.3485450836014934e-06\n",
      "Epoch 1/1 | Step 406000/915532 | Loss: 2.6002082904597046e-06\n",
      "Epoch 1/1 | Step 407000/915532 | Loss: 2.3692400645813905e-06\n",
      "Epoch 1/1 | Step 408000/915532 | Loss: 5.356791461963439e-06\n",
      "Epoch 1/1 | Step 409000/915532 | Loss: 7.823085184099909e-07\n",
      "Epoch 1/1 | Step 410000/915532 | Loss: 1.0691005627450068e-05\n",
      "Epoch 1/1 | Step 411000/915532 | Loss: 0.000501527392771095\n",
      "Epoch 1/1 | Step 412000/915532 | Loss: 2.4810030936350813e-06\n",
      "Epoch 1/1 | Step 413000/915532 | Loss: 0.20582491159439087\n",
      "Epoch 1/1 | Step 414000/915532 | Loss: 0.3653310239315033\n",
      "Epoch 1/1 | Step 415000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 416000/915532 | Loss: 7.338437626458472e-06\n",
      "Epoch 1/1 | Step 417000/915532 | Loss: 1.4901151246249356e-07\n",
      "Epoch 1/1 | Step 418000/915532 | Loss: 5.698618770111352e-05\n",
      "Epoch 1/1 | Step 419000/915532 | Loss: 2.920560064012534e-06\n",
      "Epoch 1/1 | Step 420000/915532 | Loss: 3.948798337205517e-07\n",
      "Epoch 1/1 | Step 421000/915532 | Loss: 9.685753354915505e-08\n",
      "Epoch 1/1 | Step 422000/915532 | Loss: 3.3079734294005902e-06\n",
      "Epoch 1/1 | Step 423000/915532 | Loss: 2.458666358506889e-06\n",
      "Epoch 1/1 | Step 424000/915532 | Loss: 0.0009015137911774218\n",
      "Epoch 1/1 | Step 425000/915532 | Loss: 7.845097570680082e-06\n",
      "Epoch 1/1 | Step 426000/915532 | Loss: 0.0026492385659366846\n",
      "Epoch 1/1 | Step 427000/915532 | Loss: 0.0006262043607421219\n",
      "Epoch 1/1 | Step 428000/915532 | Loss: 3.3579341106815264e-05\n",
      "Epoch 1/1 | Step 429000/915532 | Loss: 0.00012100696039851755\n",
      "Epoch 1/1 | Step 430000/915532 | Loss: 1.056406745192362e-05\n",
      "Epoch 1/1 | Step 431000/915532 | Loss: 0.02508220635354519\n",
      "Epoch 1/1 | Step 432000/915532 | Loss: 3.3154303764604265e-06\n",
      "Epoch 1/1 | Step 433000/915532 | Loss: 8.128055924316868e-06\n",
      "Epoch 1/1 | Step 434000/915532 | Loss: 4.321322251144011e-07\n",
      "Epoch 1/1 | Step 435000/915532 | Loss: 3.0786592105869204e-05\n",
      "Epoch 1/1 | Step 436000/915532 | Loss: 2.2490203264169395e-05\n",
      "Epoch 1/1 | Step 437000/915532 | Loss: 2.0181234503979795e-05\n",
      "Epoch 1/1 | Step 438000/915532 | Loss: 1.266597990934315e-07\n",
      "Epoch 1/1 | Step 439000/915532 | Loss: 0.6060028076171875\n",
      "Epoch 1/1 | Step 440000/915532 | Loss: 2.6077017878378683e-07\n",
      "Epoch 1/1 | Step 441000/915532 | Loss: 8.940695295223122e-08\n",
      "Epoch 1/1 | Step 442000/915532 | Loss: 4.0978136439662194e-07\n",
      "Epoch 1/1 | Step 443000/915532 | Loss: 0.5897771716117859\n",
      "Epoch 1/1 | Step 444000/915532 | Loss: 4.842867156185093e-07\n",
      "Epoch 1/1 | Step 445000/915532 | Loss: 0.000389156979508698\n",
      "Epoch 1/1 | Step 446000/915532 | Loss: 7.193739293143153e-05\n",
      "Epoch 1/1 | Step 447000/915532 | Loss: 3.083980845985934e-05\n",
      "Epoch 1/1 | Step 448000/915532 | Loss: 2.0116556243010564e-07\n",
      "Epoch 1/1 | Step 449000/915532 | Loss: 2.816283540596487e-06\n",
      "Epoch 1/1 | Step 450000/915532 | Loss: 5.244999101705616e-06\n",
      "Epoch 1/1 | Step 451000/915532 | Loss: 6.319497333606705e-05\n",
      "Epoch 1/1 | Step 452000/915532 | Loss: 3.576276412786683e-07\n",
      "Epoch 1/1 | Step 453000/915532 | Loss: 3.874295657624316e-07\n",
      "Epoch 1/1 | Step 454000/915532 | Loss: 2.5704262043291237e-06\n",
      "Epoch 1/1 | Step 455000/915532 | Loss: 1.7881382063933415e-07\n",
      "Epoch 1/1 | Step 456000/915532 | Loss: 0.01362175028771162\n",
      "Epoch 1/1 | Step 457000/915532 | Loss: 0.009840492159128189\n",
      "Epoch 1/1 | Step 458000/915532 | Loss: 1.0803262284753146e-06\n",
      "Epoch 1/1 | Step 459000/915532 | Loss: 3.233493316656677e-06\n",
      "Epoch 1/1 | Step 460000/915532 | Loss: 4.6938569653320883e-07\n",
      "Epoch 1/1 | Step 461000/915532 | Loss: 4.5755419705528766e-05\n",
      "Epoch 1/1 | Step 462000/915532 | Loss: 6.982921331655234e-05\n",
      "Epoch 1/1 | Step 463000/915532 | Loss: 4.4180401346238796e-06\n",
      "Epoch 1/1 | Step 464000/915532 | Loss: 4.395835446757701e-07\n",
      "Epoch 1/1 | Step 465000/915532 | Loss: 6.481992613771581e-07\n",
      "Epoch 1/1 | Step 466000/915532 | Loss: 2.5396127966814674e-05\n",
      "Epoch 1/1 | Step 467000/915532 | Loss: 0.5259384512901306\n",
      "Epoch 1/1 | Step 468000/915532 | Loss: 0.5404849648475647\n",
      "Epoch 1/1 | Step 469000/915532 | Loss: 2.9057250117148214e-07\n",
      "Epoch 1/1 | Step 470000/915532 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 471000/915532 | Loss: 3.5017711752516334e-07\n",
      "Epoch 1/1 | Step 472000/915532 | Loss: 1.9897406673408113e-05\n",
      "Epoch 1/1 | Step 473000/915532 | Loss: 1.2144362244725926e-06\n",
      "Epoch 1/1 | Step 474000/915532 | Loss: 2.3990526187844807e-06\n",
      "Epoch 1/1 | Step 475000/915532 | Loss: 1.1175781082783942e-06\n",
      "Epoch 1/1 | Step 476000/915532 | Loss: 0.4622783362865448\n",
      "Epoch 1/1 | Step 477000/915532 | Loss: 1.1473866834421642e-06\n",
      "Epoch 1/1 | Step 478000/915532 | Loss: 0.5938226580619812\n",
      "Epoch 1/1 | Step 479000/915532 | Loss: 2.193069303757511e-05\n",
      "Epoch 1/1 | Step 480000/915532 | Loss: 0.0013854989083483815\n",
      "Epoch 1/1 | Step 481000/915532 | Loss: 2.533194845000253e-07\n",
      "Epoch 1/1 | Step 482000/915532 | Loss: 6.854161711089546e-06\n",
      "Epoch 1/1 | Step 483000/915532 | Loss: 2.8312146582720743e-07\n",
      "Epoch 1/1 | Step 484000/915532 | Loss: 5.483408585860161e-06\n",
      "Epoch 1/1 | Step 485000/915532 | Loss: 3.650781650321733e-07\n",
      "Epoch 1/1 | Step 486000/915532 | Loss: 2.7407171728555113e-05\n",
      "Epoch 1/1 | Step 487000/915532 | Loss: 8.313802391057834e-05\n",
      "Epoch 1/1 | Step 488000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 489000/915532 | Loss: 4.693691153079271e-06\n",
      "Epoch 1/1 | Step 490000/915532 | Loss: 7.599552986903291e-07\n",
      "Epoch 1/1 | Step 491000/915532 | Loss: 2.4418342945864424e-05\n",
      "Epoch 1/1 | Step 492000/915532 | Loss: 8.493611289850378e-07\n",
      "Epoch 1/1 | Step 493000/915532 | Loss: 1.855183654697612e-06\n",
      "Epoch 1/1 | Step 494000/915532 | Loss: 0.012645665556192398\n",
      "Epoch 1/1 | Step 495000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 496000/915532 | Loss: 8.866131224749552e-07\n",
      "Epoch 1/1 | Step 497000/915532 | Loss: 5.438918151412508e-07\n",
      "Epoch 1/1 | Step 498000/915532 | Loss: 2.2221858671400696e-05\n",
      "Epoch 1/1 | Step 499000/915532 | Loss: 3.427258548072132e-07\n",
      "Epoch 1/1 | Step 500000/915532 | Loss: 8.940693163594915e-08\n",
      "Epoch 1/1 | Step 501000/915532 | Loss: 2.2128017462819116e-06\n",
      "Epoch 1/1 | Step 502000/915532 | Loss: 8.746616003918462e-06\n",
      "Epoch 1/1 | Step 503000/915532 | Loss: 3.427259684940509e-07\n",
      "Epoch 1/1 | Step 504000/915532 | Loss: 1.862642307060014e-07\n",
      "Epoch 1/1 | Step 505000/915532 | Loss: 3.7997915569576435e-07\n",
      "Epoch 1/1 | Step 506000/915532 | Loss: 1.1175850431754952e-06\n",
      "Epoch 1/1 | Step 507000/915532 | Loss: 1.2740395050059306e-06\n",
      "Epoch 1/1 | Step 508000/915532 | Loss: 3.2335144624084933e-06\n",
      "Epoch 1/1 | Step 509000/915532 | Loss: 1.5646199358343438e-07\n",
      "Epoch 1/1 | Step 510000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 511000/915532 | Loss: 4.172311207639723e-07\n",
      "Epoch 1/1 | Step 512000/915532 | Loss: 3.009409010701347e-05\n",
      "Epoch 1/1 | Step 513000/915532 | Loss: 4.008296855317894e-06\n",
      "Epoch 1/1 | Step 514000/915532 | Loss: 5.289894033921883e-07\n",
      "Epoch 1/1 | Step 515000/915532 | Loss: 3.471878699201625e-06\n",
      "Epoch 1/1 | Step 516000/915532 | Loss: 4.7683562343081576e-07\n",
      "Epoch 1/1 | Step 517000/915532 | Loss: 0.4956790506839752\n",
      "Epoch 1/1 | Step 518000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 519000/915532 | Loss: 1.3932469755673083e-06\n",
      "Epoch 1/1 | Step 520000/915532 | Loss: 2.0116550558668678e-07\n",
      "Epoch 1/1 | Step 521000/915532 | Loss: 2.0786972072528442e-06\n",
      "Epoch 1/1 | Step 522000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 523000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 524000/915532 | Loss: 3.4873923141276464e-05\n",
      "Epoch 1/1 | Step 525000/915532 | Loss: 0.4350093603134155\n",
      "Epoch 1/1 | Step 526000/915532 | Loss: 4.514939973887522e-06\n",
      "Epoch 1/1 | Step 527000/915532 | Loss: 3.39737971444265e-06\n",
      "Epoch 1/1 | Step 528000/915532 | Loss: 1.2293389772821683e-06\n",
      "Epoch 1/1 | Step 529000/915532 | Loss: 8.121123187265766e-07\n",
      "Epoch 1/1 | Step 530000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 531000/915532 | Loss: 3.419761014811229e-06\n",
      "Epoch 1/1 | Step 532000/915532 | Loss: 9.015146815727348e-07\n",
      "Epoch 1/1 | Step 533000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 534000/915532 | Loss: 4.619354854185076e-07\n",
      "Epoch 1/1 | Step 535000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 536000/915532 | Loss: 0.34517306089401245\n",
      "Epoch 1/1 | Step 537000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 538000/915532 | Loss: 0.0009482440073043108\n",
      "Epoch 1/1 | Step 539000/915532 | Loss: 7.152546004363103e-07\n",
      "Epoch 1/1 | Step 540000/915532 | Loss: 3.486784635242657e-06\n",
      "Epoch 1/1 | Step 541000/915532 | Loss: 0.0006853402010165155\n",
      "Epoch 1/1 | Step 542000/915532 | Loss: 4.455300313566113e-06\n",
      "Epoch 1/1 | Step 543000/915532 | Loss: 4.619343485501304e-07\n",
      "Epoch 1/1 | Step 544000/915532 | Loss: 1.9222393348172773e-06\n",
      "Epoch 1/1 | Step 545000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 546000/915532 | Loss: 0.00019552068260964006\n",
      "Epoch 1/1 | Step 547000/915532 | Loss: 7.505927351303399e-05\n",
      "Epoch 1/1 | Step 548000/915532 | Loss: 4.0978127913149365e-07\n",
      "Epoch 1/1 | Step 549000/915532 | Loss: 2.0861605776190117e-07\n",
      "Epoch 1/1 | Step 550000/915532 | Loss: 0.00010235417721560225\n",
      "Epoch 1/1 | Step 551000/915532 | Loss: 1.5646205042685324e-07\n",
      "Epoch 1/1 | Step 552000/915532 | Loss: 0.0017608292400836945\n",
      "Epoch 1/1 | Step 553000/915532 | Loss: 6.452024535974488e-06\n",
      "Epoch 1/1 | Step 554000/915532 | Loss: 1.4156096028727916e-07\n",
      "Epoch 1/1 | Step 555000/915532 | Loss: 9.23868185509491e-07\n",
      "Epoch 1/1 | Step 556000/915532 | Loss: 1.1250326679146383e-06\n",
      "Epoch 1/1 | Step 557000/915532 | Loss: 7.703456503804773e-06\n",
      "Epoch 1/1 | Step 558000/915532 | Loss: 1.8626447229053156e-07\n",
      "Epoch 1/1 | Step 559000/915532 | Loss: 1.3052060239715502e-05\n",
      "Epoch 1/1 | Step 560000/915532 | Loss: 3.417567131691612e-05\n",
      "Epoch 1/1 | Step 561000/915532 | Loss: 0.1581226885318756\n",
      "Epoch 1/1 | Step 562000/915532 | Loss: 2.756666845016298e-06\n",
      "Epoch 1/1 | Step 563000/915532 | Loss: 9.025875624502078e-05\n",
      "Epoch 1/1 | Step 564000/915532 | Loss: 4.666257882490754e-05\n",
      "Epoch 1/1 | Step 565000/915532 | Loss: 0.0003488363290671259\n",
      "Epoch 1/1 | Step 566000/915532 | Loss: 4.544839100617537e-07\n",
      "Epoch 1/1 | Step 567000/915532 | Loss: 5.2899031288689e-07\n",
      "Epoch 1/1 | Step 568000/915532 | Loss: 2.533193139697687e-07\n",
      "Epoch 1/1 | Step 569000/915532 | Loss: 5.9604616353681195e-08\n",
      "Epoch 1/1 | Step 570000/915532 | Loss: 3.6507793765849783e-07\n",
      "Epoch 1/1 | Step 571000/915532 | Loss: 2.160666241479703e-07\n",
      "Epoch 1/1 | Step 572000/915532 | Loss: 3.0919334221835015e-06\n",
      "Epoch 1/1 | Step 573000/915532 | Loss: 0.0008523348951712251\n",
      "Epoch 1/1 | Step 574000/915532 | Loss: 9.164175480691483e-07\n",
      "Epoch 1/1 | Step 575000/915532 | Loss: 5.438906214294548e-07\n",
      "Epoch 1/1 | Step 576000/915532 | Loss: 5.2154042862184724e-08\n",
      "Epoch 1/1 | Step 577000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 578000/915532 | Loss: 4.872491444984917e-06\n",
      "Epoch 1/1 | Step 579000/915532 | Loss: 2.6615804017637856e-05\n",
      "Epoch 1/1 | Step 580000/915532 | Loss: 2.2276926756603643e-06\n",
      "Epoch 1/1 | Step 581000/915532 | Loss: 9.760208286024863e-07\n",
      "Epoch 1/1 | Step 582000/915532 | Loss: 4.254137365933275e-06\n",
      "Epoch 1/1 | Step 583000/915532 | Loss: 3.7252851825542166e-07\n",
      "Epoch 1/1 | Step 584000/915532 | Loss: 1.5646214990283625e-07\n",
      "Epoch 1/1 | Step 585000/915532 | Loss: 6.407486807802343e-07\n",
      "Epoch 1/1 | Step 586000/915532 | Loss: 1.8729329894995317e-05\n",
      "Epoch 1/1 | Step 587000/915532 | Loss: 5.215405352032576e-08\n",
      "Epoch 1/1 | Step 588000/915532 | Loss: 3.9710630517220125e-06\n",
      "Epoch 1/1 | Step 589000/915532 | Loss: 6.2879753386368975e-06\n",
      "Epoch 1/1 | Step 590000/915532 | Loss: 3.5762755601354e-07\n",
      "Epoch 1/1 | Step 591000/915532 | Loss: 1.2665984172599565e-07\n",
      "Epoch 1/1 | Step 592000/915532 | Loss: 0.6140782833099365\n",
      "Epoch 1/1 | Step 593000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 594000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 595000/915532 | Loss: 1.7685215425444767e-05\n",
      "Epoch 1/1 | Step 596000/915532 | Loss: 0.09363774210214615\n",
      "Epoch 1/1 | Step 597000/915532 | Loss: 1.2836371752200648e-05\n",
      "Epoch 1/1 | Step 598000/915532 | Loss: 2.667263515832019e-06\n",
      "Epoch 1/1 | Step 599000/915532 | Loss: 1.159948169515701e-05\n",
      "Epoch 1/1 | Step 600000/915532 | Loss: 0.5814934968948364\n",
      "Epoch 1/1 | Step 601000/915532 | Loss: 9.462169145990629e-07\n",
      "Epoch 1/1 | Step 602000/915532 | Loss: 5.283730206429027e-05\n",
      "Epoch 1/1 | Step 603000/915532 | Loss: 1.4169717360346112e-05\n",
      "Epoch 1/1 | Step 604000/915532 | Loss: 6.556477387675841e-07\n",
      "Epoch 1/1 | Step 605000/915532 | Loss: 0.17026305198669434\n",
      "Epoch 1/1 | Step 606000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 607000/915532 | Loss: 9.238692655344494e-07\n",
      "Epoch 1/1 | Step 608000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 609000/915532 | Loss: 9.521133506495971e-06\n",
      "Epoch 1/1 | Step 610000/915532 | Loss: 4.544848479781649e-07\n",
      "Epoch 1/1 | Step 611000/915532 | Loss: 0.04921996593475342\n",
      "Epoch 1/1 | Step 612000/915532 | Loss: 1.0430809993522416e-07\n",
      "Epoch 1/1 | Step 613000/915532 | Loss: 6.332979296530539e-07\n",
      "Epoch 1/1 | Step 614000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 615000/915532 | Loss: 7.823064152034931e-07\n",
      "Epoch 1/1 | Step 616000/915532 | Loss: 6.545637006638572e-05\n",
      "Epoch 1/1 | Step 617000/915532 | Loss: 1.9371493920061766e-07\n",
      "Epoch 1/1 | Step 618000/915532 | Loss: 0.0016393561381846666\n",
      "Epoch 1/1 | Step 619000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 620000/915532 | Loss: 2.4586870495113544e-07\n",
      "Epoch 1/1 | Step 621000/915532 | Loss: 1.005820536192914e-06\n",
      "Epoch 1/1 | Step 622000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 623000/915532 | Loss: 4.470332441997016e-07\n",
      "Epoch 1/1 | Step 624000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 625000/915532 | Loss: 2.548079010011861e-06\n",
      "Epoch 1/1 | Step 626000/915532 | Loss: 3.4942274851346156e-06\n",
      "Epoch 1/1 | Step 627000/915532 | Loss: 4.355601777206175e-05\n",
      "Epoch 1/1 | Step 628000/915532 | Loss: 7.688534424232785e-06\n",
      "Epoch 1/1 | Step 629000/915532 | Loss: 2.674731604201952e-06\n",
      "Epoch 1/1 | Step 630000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 631000/915532 | Loss: 0.04125671461224556\n",
      "Epoch 1/1 | Step 632000/915532 | Loss: 3.444952017161995e-05\n",
      "Epoch 1/1 | Step 633000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 634000/915532 | Loss: 8.195584655368293e-07\n",
      "Epoch 1/1 | Step 635000/915532 | Loss: 6.929020628376747e-07\n",
      "Epoch 1/1 | Step 636000/915532 | Loss: 2.905719895807124e-07\n",
      "Epoch 1/1 | Step 637000/915532 | Loss: 1.549702460579283e-06\n",
      "Epoch 1/1 | Step 638000/915532 | Loss: 6.407489649973286e-07\n",
      "Epoch 1/1 | Step 639000/915532 | Loss: 0.629895031452179\n",
      "Epoch 1/1 | Step 640000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 641000/915532 | Loss: 4.917364435641502e-07\n",
      "Epoch 1/1 | Step 642000/915532 | Loss: 4.321324240663671e-07\n",
      "Epoch 1/1 | Step 643000/915532 | Loss: 5.736931711908255e-07\n",
      "Epoch 1/1 | Step 644000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 645000/915532 | Loss: 3.352718294991064e-06\n",
      "Epoch 1/1 | Step 646000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 647000/915532 | Loss: 9.685751933830034e-08\n",
      "Epoch 1/1 | Step 648000/915532 | Loss: 1.3783435406367062e-06\n",
      "Epoch 1/1 | Step 649000/915532 | Loss: 1.1175865211043856e-07\n",
      "Epoch 1/1 | Step 650000/915532 | Loss: 5.610036168945953e-06\n",
      "Epoch 1/1 | Step 651000/915532 | Loss: 0.000363238767022267\n",
      "Epoch 1/1 | Step 652000/915532 | Loss: 4.842869429921848e-07\n",
      "Epoch 1/1 | Step 653000/915532 | Loss: 5.14088469572016e-07\n",
      "Epoch 1/1 | Step 654000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 655000/915532 | Loss: 1.5646199358343438e-07\n",
      "Epoch 1/1 | Step 656000/915532 | Loss: 0.5933471322059631\n",
      "Epoch 1/1 | Step 657000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 658000/915532 | Loss: 0.0024806992150843143\n",
      "Epoch 1/1 | Step 659000/915532 | Loss: 0.4150554835796356\n",
      "Epoch 1/1 | Step 660000/915532 | Loss: 7.975236076163128e-05\n",
      "Epoch 1/1 | Step 661000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 662000/915532 | Loss: 3.710281134772231e-06\n",
      "Epoch 1/1 | Step 663000/915532 | Loss: 7.427793661918258e-06\n",
      "Epoch 1/1 | Step 664000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 665000/915532 | Loss: 2.5704173367557814e-06\n",
      "Epoch 1/1 | Step 666000/915532 | Loss: 6.854158073110739e-06\n",
      "Epoch 1/1 | Step 667000/915532 | Loss: 2.5788775019464083e-05\n",
      "Epoch 1/1 | Step 668000/915532 | Loss: 1.5050079582579201e-06\n",
      "Epoch 1/1 | Step 669000/915532 | Loss: 7.15252667760069e-07\n",
      "Epoch 1/1 | Step 670000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 671000/915532 | Loss: 1.1920921849650767e-07\n",
      "Epoch 1/1 | Step 672000/915532 | Loss: 2.7492051231092773e-06\n",
      "Epoch 1/1 | Step 673000/915532 | Loss: 0.006602398119866848\n",
      "Epoch 1/1 | Step 674000/915532 | Loss: 0.6660463213920593\n",
      "Epoch 1/1 | Step 675000/915532 | Loss: 4.9497113650431857e-05\n",
      "Epoch 1/1 | Step 676000/915532 | Loss: 2.489020516804885e-05\n",
      "Epoch 1/1 | Step 677000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 678000/915532 | Loss: 6.705519695060502e-08\n",
      "Epoch 1/1 | Step 679000/915532 | Loss: 5.885946166017675e-07\n",
      "Epoch 1/1 | Step 680000/915532 | Loss: 6.407487376236531e-07\n",
      "Epoch 1/1 | Step 681000/915532 | Loss: 9.760233297129162e-07\n",
      "Epoch 1/1 | Step 682000/915532 | Loss: 2.5311946956207976e-05\n",
      "Epoch 1/1 | Step 683000/915532 | Loss: 0.012615585699677467\n",
      "Epoch 1/1 | Step 684000/915532 | Loss: 5.811425580759533e-07\n",
      "Epoch 1/1 | Step 685000/915532 | Loss: 1.6912678120206692e-06\n",
      "Epoch 1/1 | Step 686000/915532 | Loss: 3.352757858010591e-07\n",
      "Epoch 1/1 | Step 687000/915532 | Loss: 4.172251010459149e-06\n",
      "Epoch 1/1 | Step 688000/915532 | Loss: 1.3112928627379006e-06\n",
      "Epoch 1/1 | Step 689000/915532 | Loss: 1.8551794482846162e-06\n",
      "Epoch 1/1 | Step 690000/915532 | Loss: 0.09265062212944031\n",
      "Epoch 1/1 | Step 691000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 692000/915532 | Loss: 3.136618261123658e-06\n",
      "Epoch 1/1 | Step 693000/915532 | Loss: 0.5349668860435486\n",
      "Epoch 1/1 | Step 694000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 695000/915532 | Loss: 2.823736849677516e-06\n",
      "Epoch 1/1 | Step 696000/915532 | Loss: 0.5869866609573364\n",
      "Epoch 1/1 | Step 697000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 698000/915532 | Loss: 1.4156088923300558e-07\n",
      "Epoch 1/1 | Step 699000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 700000/915532 | Loss: 9.774774298421107e-06\n",
      "Epoch 1/1 | Step 701000/915532 | Loss: 4.456097667571157e-05\n",
      "Epoch 1/1 | Step 702000/915532 | Loss: 5.960462345910855e-08\n",
      "Epoch 1/1 | Step 703000/915532 | Loss: 0.00022157483908813447\n",
      "Epoch 1/1 | Step 704000/915532 | Loss: 2.1574302081717178e-05\n",
      "Epoch 1/1 | Step 705000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 706000/915532 | Loss: 5.289890054882562e-07\n",
      "Epoch 1/1 | Step 707000/915532 | Loss: 3.7252823403832735e-07\n",
      "Epoch 1/1 | Step 708000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 709000/915532 | Loss: 0.5637522339820862\n",
      "Epoch 1/1 | Step 710000/915532 | Loss: 5.1648414228111506e-05\n",
      "Epoch 1/1 | Step 711000/915532 | Loss: 0.21440167725086212\n",
      "Epoch 1/1 | Step 712000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 713000/915532 | Loss: 2.3841832330617763e-07\n",
      "Epoch 1/1 | Step 714000/915532 | Loss: 7.450579175838357e-08\n",
      "Epoch 1/1 | Step 715000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 716000/915532 | Loss: 0.00010007071250583977\n",
      "Epoch 1/1 | Step 717000/915532 | Loss: 0.0010095208417624235\n",
      "Epoch 1/1 | Step 718000/915532 | Loss: 0.0018957594875246286\n",
      "Epoch 1/1 | Step 719000/915532 | Loss: 8.70172880240716e-06\n",
      "Epoch 1/1 | Step 720000/915532 | Loss: 1.4603022009396227e-06\n",
      "Epoch 1/1 | Step 721000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 722000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 723000/915532 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 724000/915532 | Loss: 7.964071119204164e-05\n",
      "Epoch 1/1 | Step 725000/915532 | Loss: 1.043080430918053e-07\n",
      "Epoch 1/1 | Step 726000/915532 | Loss: 4.470347292340193e-08\n",
      "Epoch 1/1 | Step 727000/915532 | Loss: 8.940693874137651e-08\n",
      "Epoch 1/1 | Step 728000/915532 | Loss: 7.659024049644358e-06\n",
      "Epoch 1/1 | Step 729000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 730000/915532 | Loss: 6.377376848831773e-06\n",
      "Epoch 1/1 | Step 731000/915532 | Loss: 3.278248357219127e-07\n",
      "Epoch 1/1 | Step 732000/915532 | Loss: 0.0003305129357613623\n",
      "Epoch 1/1 | Step 733000/915532 | Loss: 2.458674543959205e-06\n",
      "Epoch 1/1 | Step 734000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 735000/915532 | Loss: 0.08298024535179138\n",
      "Epoch 1/1 | Step 736000/915532 | Loss: 8.717118475942698e-07\n",
      "Epoch 1/1 | Step 737000/915532 | Loss: 0.0008939840481616557\n",
      "Epoch 1/1 | Step 738000/915532 | Loss: 0.0022218027152121067\n",
      "Epoch 1/1 | Step 739000/915532 | Loss: 8.568125053898257e-07\n",
      "Epoch 1/1 | Step 740000/915532 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 741000/915532 | Loss: 5.870911991223693e-06\n",
      "Epoch 1/1 | Step 742000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 743000/915532 | Loss: 1.6391270207805064e-07\n",
      "Epoch 1/1 | Step 744000/915532 | Loss: 8.34459626730677e-07\n",
      "Epoch 1/1 | Step 745000/915532 | Loss: 5.9604616353681195e-08\n",
      "Epoch 1/1 | Step 746000/915532 | Loss: 1.043080430918053e-07\n",
      "Epoch 1/1 | Step 747000/915532 | Loss: 7.823085752534098e-07\n",
      "Epoch 1/1 | Step 748000/915532 | Loss: 1.043080430918053e-07\n",
      "Epoch 1/1 | Step 749000/915532 | Loss: 6.854513117104943e-07\n",
      "Epoch 1/1 | Step 750000/915532 | Loss: 1.862642307060014e-07\n",
      "Epoch 1/1 | Step 751000/915532 | Loss: 1.043080430918053e-07\n",
      "Epoch 1/1 | Step 752000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 753000/915532 | Loss: 3.6879391700495034e-06\n",
      "Epoch 1/1 | Step 754000/915532 | Loss: 2.682206172721635e-07\n",
      "Epoch 1/1 | Step 755000/915532 | Loss: 0.4527583718299866\n",
      "Epoch 1/1 | Step 756000/915532 | Loss: 1.1175771987836924e-06\n",
      "Epoch 1/1 | Step 757000/915532 | Loss: 5.215405352032576e-08\n",
      "Epoch 1/1 | Step 758000/915532 | Loss: 3.926338649762329e-06\n",
      "Epoch 1/1 | Step 759000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 760000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 761000/915532 | Loss: 2.8088127237424487e-06\n",
      "Epoch 1/1 | Step 762000/915532 | Loss: 6.109461310188635e-07\n",
      "Epoch 1/1 | Step 763000/915532 | Loss: 8.195635814445268e-08\n",
      "Epoch 1/1 | Step 764000/915532 | Loss: 1.639125599695035e-07\n",
      "Epoch 1/1 | Step 765000/915532 | Loss: 3.8918296922929585e-05\n",
      "Epoch 1/1 | Step 766000/915532 | Loss: 2.317091912118485e-06\n",
      "Epoch 1/1 | Step 767000/915532 | Loss: 7.376036137429764e-07\n",
      "Epoch 1/1 | Step 768000/915532 | Loss: 2.7567091365199303e-07\n",
      "Epoch 1/1 | Step 769000/915532 | Loss: 3.8742916785849957e-07\n",
      "Epoch 1/1 | Step 770000/915532 | Loss: 0.0005523449508473277\n",
      "Epoch 1/1 | Step 771000/915532 | Loss: 3.866734459734289e-06\n",
      "Epoch 1/1 | Step 772000/915532 | Loss: 9.278542711399496e-05\n",
      "Epoch 1/1 | Step 773000/915532 | Loss: 0.0016283902805298567\n",
      "Epoch 1/1 | Step 774000/915532 | Loss: 6.697810476907762e-06\n",
      "Epoch 1/1 | Step 775000/915532 | Loss: 4.060515493620187e-06\n",
      "Epoch 1/1 | Step 776000/915532 | Loss: 0.024685461074113846\n",
      "Epoch 1/1 | Step 777000/915532 | Loss: 9.46218506214791e-07\n",
      "Epoch 1/1 | Step 778000/915532 | Loss: 0.008969741873443127\n",
      "Epoch 1/1 | Step 779000/915532 | Loss: 3.7997861568328517e-07\n",
      "Epoch 1/1 | Step 780000/915532 | Loss: 4.768363908169704e-07\n",
      "Epoch 1/1 | Step 781000/915532 | Loss: 3.6507759659798467e-07\n",
      "Epoch 1/1 | Step 782000/915532 | Loss: 5.297247298585717e-06\n",
      "Epoch 1/1 | Step 783000/915532 | Loss: 1.8626425912771083e-07\n",
      "Epoch 1/1 | Step 784000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 785000/915532 | Loss: 0.015154598280787468\n",
      "Epoch 1/1 | Step 786000/915532 | Loss: 3.4942397633130895e-06\n",
      "Epoch 1/1 | Step 787000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 788000/915532 | Loss: 7.450578465295621e-08\n",
      "Epoch 1/1 | Step 789000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 790000/915532 | Loss: 4.991870241610741e-07\n",
      "Epoch 1/1 | Step 791000/915532 | Loss: 1.3513891644834075e-05\n",
      "Epoch 1/1 | Step 792000/915532 | Loss: 0.6530054211616516\n",
      "Epoch 1/1 | Step 793000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 794000/915532 | Loss: 7.450536259057117e-07\n",
      "Epoch 1/1 | Step 795000/915532 | Loss: 9.685746960030883e-08\n",
      "Epoch 1/1 | Step 796000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 797000/915532 | Loss: 0.6128714680671692\n",
      "Epoch 1/1 | Step 798000/915532 | Loss: 9.685691111371852e-07\n",
      "Epoch 1/1 | Step 799000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 800000/915532 | Loss: 1.1399288268876262e-06\n",
      "Epoch 1/1 | Step 801000/915532 | Loss: 2.4586870495113544e-07\n",
      "Epoch 1/1 | Step 802000/915532 | Loss: 8.195632972274325e-08\n",
      "Epoch 1/1 | Step 803000/915532 | Loss: 3.168674811604433e-05\n",
      "Epoch 1/1 | Step 804000/915532 | Loss: 1.0952257980534341e-06\n",
      "Epoch 1/1 | Step 805000/915532 | Loss: 1.2144332686148118e-06\n",
      "Epoch 1/1 | Step 806000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 807000/915532 | Loss: 3.1887670957075898e-06\n",
      "Epoch 1/1 | Step 808000/915532 | Loss: 6.705520405603238e-08\n",
      "Epoch 1/1 | Step 809000/915532 | Loss: 1.3411032284693647e-07\n",
      "Epoch 1/1 | Step 810000/915532 | Loss: 2.1606646782856842e-07\n",
      "Epoch 1/1 | Step 811000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 812000/915532 | Loss: 4.827790235140128e-06\n",
      "Epoch 1/1 | Step 813000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 814000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 815000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 816000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 817000/915532 | Loss: 5.960462345910855e-08\n",
      "Epoch 1/1 | Step 818000/915532 | Loss: 0.0006420405115932226\n",
      "Epoch 1/1 | Step 819000/915532 | Loss: 7.450577754752885e-08\n",
      "Epoch 1/1 | Step 820000/915532 | Loss: 1.680625973676797e-05\n",
      "Epoch 1/1 | Step 821000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 822000/915532 | Loss: 0.7040026783943176\n",
      "Epoch 1/1 | Step 823000/915532 | Loss: 1.0430806440808738e-07\n",
      "Epoch 1/1 | Step 824000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 825000/915532 | Loss: 8.940691031966708e-08\n",
      "Epoch 1/1 | Step 826000/915532 | Loss: 3.9487952108174795e-07\n",
      "Epoch 1/1 | Step 827000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 828000/915532 | Loss: 3.6507785239336954e-07\n",
      "Epoch 1/1 | Step 829000/915532 | Loss: 2.4586873337284487e-07\n",
      "Epoch 1/1 | Step 830000/915532 | Loss: 1.3857955991625204e-06\n",
      "Epoch 1/1 | Step 831000/915532 | Loss: 1.4156087502215087e-07\n",
      "Epoch 1/1 | Step 832000/915532 | Loss: 9.685748381116355e-08\n",
      "Epoch 1/1 | Step 833000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 834000/915532 | Loss: 1.2367869430818246e-06\n",
      "Epoch 1/1 | Step 835000/915532 | Loss: 2.607697808798548e-07\n",
      "Epoch 1/1 | Step 836000/915532 | Loss: 0.005908194929361343\n",
      "Epoch 1/1 | Step 837000/915532 | Loss: 9.760216244103503e-07\n",
      "Epoch 1/1 | Step 838000/915532 | Loss: 1.1920920428565296e-07\n",
      "Epoch 1/1 | Step 839000/915532 | Loss: 2.7567091365199303e-07\n",
      "Epoch 1/1 | Step 840000/915532 | Loss: 1.4901149825163884e-07\n",
      "Epoch 1/1 | Step 841000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 842000/915532 | Loss: 9.312532711192034e-06\n",
      "Epoch 1/1 | Step 843000/915532 | Loss: 1.915999200718943e-05\n",
      "Epoch 1/1 | Step 844000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 845000/915532 | Loss: 7.681085662625264e-06\n",
      "Epoch 1/1 | Step 846000/915532 | Loss: 0.00012341627734713256\n",
      "Epoch 1/1 | Step 847000/915532 | Loss: 1.4156094607642444e-07\n",
      "Epoch 1/1 | Step 848000/915532 | Loss: 3.2186260341404704e-06\n",
      "Epoch 1/1 | Step 849000/915532 | Loss: 4.3496456783032045e-05\n",
      "Epoch 1/1 | Step 850000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 851000/915532 | Loss: 0.0006953452248126268\n",
      "Epoch 1/1 | Step 852000/915532 | Loss: 4.341441308497451e-05\n",
      "Epoch 1/1 | Step 853000/915532 | Loss: 4.917376372759463e-07\n",
      "Epoch 1/1 | Step 854000/915532 | Loss: 1.3411039390121005e-07\n",
      "Epoch 1/1 | Step 855000/915532 | Loss: 0.003350839950144291\n",
      "Epoch 1/1 | Step 856000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 857000/915532 | Loss: 2.533151928219013e-06\n",
      "Epoch 1/1 | Step 858000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 859000/915532 | Loss: 4.783122221851954e-06\n",
      "Epoch 1/1 | Step 860000/915532 | Loss: 3.57626959157642e-07\n",
      "Epoch 1/1 | Step 861000/915532 | Loss: 1.4156097449813387e-07\n",
      "Epoch 1/1 | Step 862000/915532 | Loss: 0.0002191392850363627\n",
      "Epoch 1/1 | Step 863000/915532 | Loss: 3.8528029108420014e-05\n",
      "Epoch 1/1 | Step 864000/915532 | Loss: 2.756708852302836e-07\n",
      "Epoch 1/1 | Step 865000/915532 | Loss: 2.1606646782856842e-07\n",
      "Epoch 1/1 | Step 866000/915532 | Loss: 1.1920923981278975e-07\n",
      "Epoch 1/1 | Step 867000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 868000/915532 | Loss: 2.3841812435421161e-07\n",
      "Epoch 1/1 | Step 869000/915532 | Loss: 5.662425337504828e-07\n",
      "Epoch 1/1 | Step 870000/915532 | Loss: 7.450577754752885e-08\n",
      "Epoch 1/1 | Step 871000/915532 | Loss: 3.352756721142214e-07\n",
      "Epoch 1/1 | Step 872000/915532 | Loss: 9.387705404151347e-07\n",
      "Epoch 1/1 | Step 873000/915532 | Loss: 0.5422770977020264\n",
      "Epoch 1/1 | Step 874000/915532 | Loss: 6.779999921491253e-07\n",
      "Epoch 1/1 | Step 875000/915532 | Loss: 1.4156097449813387e-07\n",
      "Epoch 1/1 | Step 876000/915532 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 877000/915532 | Loss: 3.523949271766469e-05\n",
      "Epoch 1/1 | Step 878000/915532 | Loss: 2.6822064569387294e-07\n",
      "Epoch 1/1 | Step 879000/915532 | Loss: 1.125029484683182e-06\n",
      "Epoch 1/1 | Step 880000/915532 | Loss: 0.7764532566070557\n",
      "Epoch 1/1 | Step 881000/915532 | Loss: 2.352450610487722e-05\n",
      "Epoch 1/1 | Step 882000/915532 | Loss: 8.940693874137651e-08\n",
      "Epoch 1/1 | Step 883000/915532 | Loss: 3.7698946471209638e-06\n",
      "Epoch 1/1 | Step 884000/915532 | Loss: 7.599551850034914e-07\n",
      "Epoch 1/1 | Step 885000/915532 | Loss: 7.227027367662231e-07\n",
      "Epoch 1/1 | Step 886000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 887000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 888000/915532 | Loss: 3.1515392038272694e-06\n",
      "Epoch 1/1 | Step 889000/915532 | Loss: 1.333640170742001e-06\n",
      "Epoch 1/1 | Step 890000/915532 | Loss: 9.685729764896678e-07\n",
      "Epoch 1/1 | Step 891000/915532 | Loss: 8.917708328226581e-06\n",
      "Epoch 1/1 | Step 892000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 893000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 894000/915532 | Loss: 5.6398348533548415e-06\n",
      "Epoch 1/1 | Step 895000/915532 | Loss: 0.0326242558658123\n",
      "Epoch 1/1 | Step 896000/915532 | Loss: 2.45868676529426e-07\n",
      "Epoch 1/1 | Step 897000/915532 | Loss: 4.1423863876843825e-06\n",
      "Epoch 1/1 | Step 898000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 899000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 900000/915532 | Loss: 2.2351702000378282e-07\n",
      "Epoch 1/1 | Step 901000/915532 | Loss: 7.532213203376159e-05\n",
      "Epoch 1/1 | Step 902000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 903000/915532 | Loss: 0.00010928684787359089\n",
      "Epoch 1/1 | Step 904000/915532 | Loss: 5.215385385781701e-07\n",
      "Epoch 1/1 | Step 905000/915532 | Loss: 7.450576333667414e-08\n",
      "Epoch 1/1 | Step 906000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 907000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 908000/915532 | Loss: 2.3841828067361348e-07\n",
      "Epoch 1/1 | Step 909000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 910000/915532 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 911000/915532 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 912000/915532 | Loss: 9.760184411788941e-07\n",
      "Epoch 1/1 | Step 913000/915532 | Loss: 1.1175862368872913e-07\n",
      "Epoch 1/1 | Step 914000/915532 | Loss: 4.122536483919248e-05\n",
      "Epoch 1/1 | Step 915000/915532 | Loss: 4.663912932301173e-06\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate average loss and time taken for the epoch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Time Taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mepoch_time\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch_time' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader1):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader1)} | Loss: {loss.item()}\")\n",
    "\n",
    "   \n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}, Time Taken: {epoch_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.05677984567414616\n"
     ]
    }
   ],
   "source": [
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\") # Time Taken: {epoch_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m b_input_ids, b_input_mask, b_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      8\u001b[0m b_input_ids, b_input_mask \u001b[38;5;241m=\u001b[39m b_input_ids\u001b[38;5;241m.\u001b[39mto(device), b_input_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     12\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:794\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    791\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m--> 794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    799\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    800\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    805\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:447\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:184\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    181\u001b[0m bsz, src_len \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    182\u001b[0m tgt_len \u001b[38;5;241m=\u001b[39m tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m src_len\n\u001b[1;32m--> 184\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m expanded_mask\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inverted_mask\u001b[38;5;241m.\u001b[39mmasked_fill(inverted_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions1 = []\n",
    "true_labels1 = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader1:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions1.extend(preds)\n",
    "        true_labels1.extend(b_labels.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m predictions_flat1 \u001b[38;5;241m=\u001b[39m predictions1\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m predicted_data1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrop_QueryType_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_flat1\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_data1)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "predictions_flat1 = predictions1\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data1 = pd.DataFrame({\n",
    "    'Month': data.loc[test_idx, 'Month'].values, \n",
    "    'Place': data.loc[test_idx, 'place'].values,\n",
    "    'Crop_QueryType_code': predictions_flat1\n",
    "})\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_37300\\301140935.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Month                              Place  Crop_QueryType_code  \\\n",
      "0            9             UTTAR PRADESH_FATEHPUR                 5571   \n",
      "1           12             ANDHRA PRADESH_NELLORE                 5554   \n",
      "2            5                 RAJASTHAN_BHILWARA                 4410   \n",
      "3           11          UTTAR PRADESH_KUSHI NAGAR                 7874   \n",
      "4            9                  MAHARASHTRA_NASIK                 7553   \n",
      "...        ...                                ...                  ...   \n",
      "3662122      7                TAMILNADU_CUDDALORE                 5554   \n",
      "3662123     12                     PUNJAB_PATIALA                 7893   \n",
      "3662124     11                    BIHAR_DARBHANGA                 7889   \n",
      "3662125      5  UTTAR PRADESH_GAUTAM BUDDHA NAGAR                 5917   \n",
      "3662126      8              UTTAR PRADESH_ALIGARH                 1114   \n",
      "\n",
      "                               Crop_QueryType  \n",
      "0                          Paddy Dhan_Weather  \n",
      "1              Paddy Dhan_Nutrient Management  \n",
      "2                      Lemon_Plant Protection  \n",
      "3                     Wheat_Field Preparation  \n",
      "4                     Tomato_Plant Protection  \n",
      "...                                       ...  \n",
      "3662122        Paddy Dhan_Nutrient Management  \n",
      "3662123                         Wheat_Weather  \n",
      "3662124                       Wheat_Varieties  \n",
      "3662125                  Pig_Dairy Production  \n",
      "3662126  Black Gram urd bean_Plant Protection  \n",
      "\n",
      "[3662127 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n",
    "predicted_data1['Crop_QueryType']=predicted_data1['Crop_QueryType_code'].apply(lambda x: data['Crop_QueryType'].cat.categories[x])\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7756     12     WEST BENGAL_North DINAJPUR   \n",
      "7757     12            WEST BENGAL_PURULIA   \n",
      "7758     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7759     12     WEST BENGAL_South DINAJPUR   \n",
      "7760     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Index(['Chillies_Nutrient Management', 'Ash Go...  \n",
      "1     Index(['Groundnut pea nutmung phalli_Plant Pro...  \n",
      "2     Index(['Paddy Dhan_Plant Protection', 'Chillie...  \n",
      "3     Index(['Paddy Dhan_Plant Protection', 'Black G...  \n",
      "4     Index(['Chillies_Plant Protection', 'Chillies_...  \n",
      "...                                                 ...  \n",
      "7756  Index(['Potato_Plant Protection', 'Maize Makka...  \n",
      "7757  Index(['Potato_Plant Protection', 'Tomato_Plan...  \n",
      "7758  Index(['Paddy Dhan_Weather', 'Paddy Dhan_Plant...  \n",
      "7759  Index(['Potato_Plant Protection', 'Paddy Dhan_...  \n",
      "7760  Index(['Paddy Dhan_Weather', 'Potato_Plant Pro...  \n",
      "\n",
      "[7761 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: x.value_counts().index[:15]).reset_index()\n",
    "print(monthly_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 15  # Change this value to get top N Crop_QueryTypes\n",
    "top_crop_querytypes = (\n",
    "    predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType']\n",
    "    .apply(lambda x: x.value_counts().head(top_n).index.tolist())\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "top_crop_querytypes.columns = ['Month', 'Place', 'Top_Crop_QueryTypes']\n",
    "\n",
    "# Save to CSV\n",
    "top_crop_querytypes.to_csv('D:/Objective 1 code/top_crop_querytypes_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03mApply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m    data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Concatenate the QueryType values for each unique combination of Month and Place\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m monthly_predictions1 \u001b[38;5;241m=\u001b[39m monthly_predictions1\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlace\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop_QueryType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict most frequent Crop_QueryType for next year on a monthly basis\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m monthly_predictions1\u001b[38;5;241m=\u001b[39m monthly_predictions1\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop_QueryType\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Concatenate the QueryType values for each unique combination of Month and Place\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m monthly_predictions1 \u001b[38;5;241m=\u001b[39m \u001b[43mmonthly_predictions1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrop_QueryType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict most frequent Crop_QueryType for next year on a monthly basis\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(monthly_predictions1)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:230\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[0;32m    225\u001b[0m     _apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m, examples\u001b[38;5;241m=\u001b[39m_apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1846\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1838\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1839\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1844\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[1;32m-> 1846\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m monthly_predictions1\u001b[38;5;241m=\u001b[39m monthly_predictions1\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop_QueryType\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Concatenate the QueryType values for each unique combination of Month and Place\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m monthly_predictions1 \u001b[38;5;241m=\u001b[39m monthly_predictions1\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlace\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop_QueryType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict most frequent Crop_QueryType for next year on a monthly basis\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(monthly_predictions1)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "monthly_predictions1= monthly_predictions1.explode('Crop_QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions1 = monthly_predictions1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "print(monthly_predictions1)\n",
    "\n",
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_data1.to_csv('D:/Data/Places_crop_querytypes_in_India111.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7756     12     WEST BENGAL_North DINAJPUR   \n",
      "7757     12            WEST BENGAL_PURULIA   \n",
      "7758     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7759     12     WEST BENGAL_South DINAJPUR   \n",
      "7760     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Chillies_Nutrient Management, Ash Gourd Petha_...  \n",
      "1     Tomato_Plant Protection, BhindiOkraLadysfinger...  \n",
      "2     Chillies_Plant Protection, Black Gram urd bean...  \n",
      "3     Sugarcane Noble Cane_Weed Management, Paddy Dh...  \n",
      "4     Banana_Nutrient Management, Bengal Gram GramCh...  \n",
      "...                                                 ...  \n",
      "7756  Maize Makka_Plant Protection, Potato_Nutrient ...  \n",
      "7757  Tomato_Plant Protection, Paddy Dhan_Weather, P...  \n",
      "7758  Paddy Dhan_Weather, BhindiOkraLadysfinger_Fert...  \n",
      "7759  Chillies_Plant Protection, Potato_Plant Protec...  \n",
      "7760  Chillies_Plant Protection, Mustard_Weather, Pa...  \n",
      "\n",
      "[7761 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "monthly_data1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "print(monthly_data1)\n",
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_data1.to_csv('D:/Data/Places_crop_querytypes_in_India111.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions1 = np.array(predictions1)\n",
    "true_labels1 = np.array(true_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def precision(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Positives (FP)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    \n",
    "    # Calculate Precision\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(true_labels, predictions):\n",
    "    # Count the number of correct predictions\n",
    "    correct = np.sum(np.array(true_labels) == np.array(predictions))\n",
    "    # Calculate accuracy\n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels1, predictions1)\n",
    "mae = mae(true_labels1, predictions1)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1= f1_score(true_labels1, predictions1)\n",
    "recall = recall(true_labels1, predictions1)\n",
    "\n",
    "accuracy = accuracy(true_labels1, predictions1)\n",
    "precision = precision(true_labels1, predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.56078117322159\n",
      "10.487001050420169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mean_squared_error(true_labels1, predictions1))\n",
    "print(rmse)\n",
    "mae = mean_absolute_error(true_labels1, predictions1)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "      Metric       Value\n",
      "0   Accuracy    0.996455\n",
      "1  Precision    0.000000\n",
      "2     Recall    0.000000\n",
      "3   F1-Score    0.000000\n",
      "4       RMSE  210.560781\n",
      "5        MAE   10.487001\n"
     ]
    }
   ],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rmse, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rmse, mae]\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/distilbert2_model_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
