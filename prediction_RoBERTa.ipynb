{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch_directml\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\2604830758.py:2: DtypeWarning: Columns (1,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data1 = pd.read_csv('D:/Data/kcc_dataset.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BlockName Category  Year  Month  Day        Crop  \\\n",
      "0                0           0  2006      1   17        1275   \n",
      "1                0           0  2006      1   17         964   \n",
      "2                0           0  2006      1   17        1279   \n",
      "3                0           0  2006      1   17        1064   \n",
      "4                0           0  2006      1   17        1279   \n",
      "...               ...      ...   ...    ...  ...         ...   \n",
      "37962920        DELHI   Others  2023     10   13      Others   \n",
      "37962921    TLANGNUAM   Others  2023     10   13      Others   \n",
      "37962922    TLANGNUAM   Others  2023     10   13      Others   \n",
      "37962923      DARLAWN   Others  2023     10   13      Others   \n",
      "37962924  Thirunallar  Cereals  2023     10   13  Paddy Dhan   \n",
      "\n",
      "                       DistrictName           QueryType Season  \\\n",
      "0                             SAGAR                  99   RABI   \n",
      "1                             SAGAR  Disease Management   RABI   \n",
      "2                             SAGAR                  76   RABI   \n",
      "3                             SAGAR                   3   RABI   \n",
      "4                             DAMOH                  76   RABI   \n",
      "...                             ...                 ...    ...   \n",
      "37962920  New Delhi Connaught Place             Weather    NaN   \n",
      "37962921                     AIZAWL               Seeds    NaN   \n",
      "37962922                     AIZAWL               Seeds    NaN   \n",
      "37962923                     AIZAWL               Seeds    NaN   \n",
      "37962924                   KARAIKAL  Cultural Practices    NaN   \n",
      "\n",
      "                    Sector       StateName  \\\n",
      "0             HORTICULTURE  MADHYA PRADESH   \n",
      "1         ANIMAL HUSBANDRY  MADHYA PRADESH   \n",
      "2             HORTICULTURE  MADHYA PRADESH   \n",
      "3              AGRICULTURE  MADHYA PRADESH   \n",
      "4             HORTICULTURE  MADHYA PRADESH   \n",
      "...                    ...             ...   \n",
      "37962920       AGRICULTURE           DELHI   \n",
      "37962921       AGRICULTURE         MIZORAM   \n",
      "37962922       AGRICULTURE         MIZORAM   \n",
      "37962923       AGRICULTURE         MIZORAM   \n",
      "37962924       AGRICULTURE      PUDUCHERRY   \n",
      "\n",
      "                                          QueryText  \\\n",
      "0         how to control flower drop in bottelgourd   \n",
      "1               how tyo control diseases in buffalo   \n",
      "2             how to control fruit borer in brinjal   \n",
      "3          how to control of yellow moisac in moong   \n",
      "4               how to control white fly in brinjal   \n",
      "...                                             ...   \n",
      "37962920              Farmer asked query on Weather   \n",
      "37962921                                  TEST CALL   \n",
      "37962922                                  TEST CALL   \n",
      "37962923                                  test call   \n",
      "37962924              Farmer asked query on Weather   \n",
      "\n",
      "                                                     KccAns  \n",
      "0                                     spray planofix4mlpump  \n",
      "1                                                       NaN  \n",
      "2                      should be spray profenophos 35mlpump  \n",
      "3                       should be spray metasystox 35mlpump  \n",
      "4                       should be spray metasystox 35mlpump  \n",
      "...                                                     ...  \n",
      "37962920                                          -          \n",
      "37962921                                          THANK YOU  \n",
      "37962922                                          THANK YOU  \n",
      "37962923                                          thank you  \n",
      "37962924   :        :   3             31     26       11...  \n",
      "\n",
      "[37962925 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load your preprocessed dataset\n",
    "data1 = pd.read_csv('D:/Data/kcc_dataset.csv')\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "f_df = data1[(data1['Crop'].str.isnumeric() == False) & (data1['Crop'] != 'Others') & \n",
    "             (data1['QueryType'].str.isnumeric() == False) & (data1['QueryType'] != 'Others')]\n",
    "df= pd.DataFrame(f_df)\n",
    "df = df[df['Year'] >= 2013]\n",
    "# Create a new feature 'Crop_QueryType'\n",
    "df[\"place\"] = df['StateName'] + '_' + df['DistrictName']\n",
    "data = df[['Month', 'Year', 'place', 'Crop', 'QueryType']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROP-WISE QUERYTYPE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\1181383426.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['QueryType_code'] = data['QueryType'].astype('category').cat.codes\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\1181383426.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop']} {row['QueryType']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Encode QueryType as categorical variable\n",
    "data['QueryType_code'] = data['QueryType'].astype('category').cat.codes\n",
    "\n",
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop']} {row['QueryType']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "inputs = tokenizer(data['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract labels for training (Crop_QueryType codes)\n",
    "labels = torch.tensor(data['QueryType_code'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_inputs, test_inputs, train_labels, test_labels, train_idx, test_idx = train_test_split(\n",
    "    inputs['input_ids'], labels, data.index, test_size=0.2, random_state=42)\n",
    "train_masks, test_masks = train_test_split(inputs['attention_mask'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\3377841694.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(train_inputs, torch.tensor(train_masks), train_labels)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\3377841694.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(test_inputs, torch.tensor(test_masks), test_labels)\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, torch.tensor(train_masks), train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, torch.tensor(test_masks), test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=65, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=data['QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 0/839071 | Loss: 4.115621089935303\n",
      "Epoch 1/1 | Step 1000/839071 | Loss: 0.0920330062508583\n",
      "Epoch 1/1 | Step 2000/839071 | Loss: 0.005906142294406891\n",
      "Epoch 1/1 | Step 3000/839071 | Loss: 0.0018234231974929571\n",
      "Epoch 1/1 | Step 4000/839071 | Loss: 0.0014383802190423012\n",
      "Epoch 1/1 | Step 5000/839071 | Loss: 0.0004411045811139047\n",
      "Epoch 1/1 | Step 6000/839071 | Loss: 0.0019014236750081182\n",
      "Epoch 1/1 | Step 7000/839071 | Loss: 0.003261815058067441\n",
      "Epoch 1/1 | Step 8000/839071 | Loss: 0.0006131986156105995\n",
      "Epoch 1/1 | Step 9000/839071 | Loss: 0.0002484227588865906\n",
      "Epoch 1/1 | Step 10000/839071 | Loss: 8.632714161649346e-05\n",
      "Epoch 1/1 | Step 11000/839071 | Loss: 0.00021728103456553072\n",
      "Epoch 1/1 | Step 12000/839071 | Loss: 5.888478335691616e-05\n",
      "Epoch 1/1 | Step 13000/839071 | Loss: 9.625859092921019e-05\n",
      "Epoch 1/1 | Step 14000/839071 | Loss: 2.754395427473355e-05\n",
      "Epoch 1/1 | Step 15000/839071 | Loss: 9.639735071687028e-05\n",
      "Epoch 1/1 | Step 16000/839071 | Loss: 4.84245756524615e-05\n",
      "Epoch 1/1 | Step 17000/839071 | Loss: 8.594186510890722e-05\n",
      "Epoch 1/1 | Step 18000/839071 | Loss: 0.0004408791719470173\n",
      "Epoch 1/1 | Step 19000/839071 | Loss: 4.315592741477303e-05\n",
      "Epoch 1/1 | Step 20000/839071 | Loss: 0.00010482677316758782\n",
      "Epoch 1/1 | Step 21000/839071 | Loss: 0.00010687977919587865\n",
      "Epoch 1/1 | Step 22000/839071 | Loss: 4.518345667747781e-05\n",
      "Epoch 1/1 | Step 23000/839071 | Loss: 4.9620348363532685e-06\n",
      "Epoch 1/1 | Step 24000/839071 | Loss: 2.7101081286673434e-05\n",
      "Epoch 1/1 | Step 25000/839071 | Loss: 4.4340653403196484e-05\n",
      "Epoch 1/1 | Step 26000/839071 | Loss: 0.00013947846309747547\n",
      "Epoch 1/1 | Step 27000/839071 | Loss: 1.72324162122095e-05\n",
      "Epoch 1/1 | Step 28000/839071 | Loss: 1.5846984751988202e-05\n",
      "Epoch 1/1 | Step 29000/839071 | Loss: 0.001725673326291144\n",
      "Epoch 1/1 | Step 30000/839071 | Loss: 3.777432539209258e-06\n",
      "Epoch 1/1 | Step 31000/839071 | Loss: 2.9429693313431926e-06\n",
      "Epoch 1/1 | Step 32000/839071 | Loss: 1.661475266701018e-06\n",
      "Epoch 1/1 | Step 33000/839071 | Loss: 1.0929453310382087e-05\n",
      "Epoch 1/1 | Step 34000/839071 | Loss: 5.4835554692544974e-06\n",
      "Epoch 1/1 | Step 35000/839071 | Loss: 5.915651399845956e-06\n",
      "Epoch 1/1 | Step 36000/839071 | Loss: 2.630047674756497e-06\n",
      "Epoch 1/1 | Step 37000/839071 | Loss: 1.5369345419458114e-05\n",
      "Epoch 1/1 | Step 38000/839071 | Loss: 4.567141786537832e-06\n",
      "Epoch 1/1 | Step 39000/839071 | Loss: 7.71848572185263e-05\n",
      "Epoch 1/1 | Step 40000/839071 | Loss: 6.616001883230638e-06\n",
      "Epoch 1/1 | Step 41000/839071 | Loss: 1.0735759133240208e-05\n",
      "Epoch 1/1 | Step 42000/839071 | Loss: 1.929695599756087e-06\n",
      "Epoch 1/1 | Step 43000/839071 | Loss: 1.5115681890165433e-05\n",
      "Epoch 1/1 | Step 44000/839071 | Loss: 3.5240948363934876e-06\n",
      "Epoch 1/1 | Step 45000/839071 | Loss: 2.6076954782183748e-06\n",
      "Epoch 1/1 | Step 46000/839071 | Loss: 3.702905360114528e-06\n",
      "Epoch 1/1 | Step 47000/839071 | Loss: 2.1606645077554276e-06\n",
      "Epoch 1/1 | Step 48000/839071 | Loss: 1.6904567019082606e-05\n",
      "Epoch 1/1 | Step 49000/839071 | Loss: 1.0020495210483205e-05\n",
      "Epoch 1/1 | Step 50000/839071 | Loss: 1.4752105244042468e-06\n",
      "Epoch 1/1 | Step 51000/839071 | Loss: 8.642666102787189e-07\n",
      "Epoch 1/1 | Step 52000/839071 | Loss: 7.971975719556212e-06\n",
      "Epoch 1/1 | Step 53000/839071 | Loss: 4.097791588719701e-06\n",
      "Epoch 1/1 | Step 54000/839071 | Loss: 5.429142402135767e-05\n",
      "Epoch 1/1 | Step 55000/839071 | Loss: 5.2004634198965505e-06\n",
      "Epoch 1/1 | Step 56000/839071 | Loss: 1.0862524504773319e-05\n",
      "Epoch 1/1 | Step 57000/839071 | Loss: 8.627672286820598e-06\n",
      "Epoch 1/1 | Step 58000/839071 | Loss: 8.838676876621321e-05\n",
      "Epoch 1/1 | Step 59000/839071 | Loss: 3.1292361200030427e-06\n",
      "Epoch 1/1 | Step 60000/839071 | Loss: 1.0676190868252888e-05\n",
      "Epoch 1/1 | Step 61000/839071 | Loss: 2.980225417559268e-06\n",
      "Epoch 1/1 | Step 62000/839071 | Loss: 2.9280683975230204e-06\n",
      "Epoch 1/1 | Step 63000/839071 | Loss: 1.2285602679185104e-05\n",
      "Epoch 1/1 | Step 64000/839071 | Loss: 5.952852916379925e-06\n",
      "Epoch 1/1 | Step 65000/839071 | Loss: 0.0005403723916970193\n",
      "Epoch 1/1 | Step 66000/839071 | Loss: 1.630813676456455e-05\n",
      "Epoch 1/1 | Step 67000/839071 | Loss: 1.5585519577143714e-05\n",
      "Epoch 1/1 | Step 68000/839071 | Loss: 0.01441879104822874\n",
      "Epoch 1/1 | Step 69000/839071 | Loss: 5.12596670887433e-06\n",
      "Epoch 1/1 | Step 70000/839071 | Loss: 2.0787092580576427e-06\n",
      "Epoch 1/1 | Step 71000/839071 | Loss: 1.0691184797906317e-05\n",
      "Epoch 1/1 | Step 72000/839071 | Loss: 9.320567187387496e-06\n",
      "Epoch 1/1 | Step 73000/839071 | Loss: 1.96695032173011e-06\n",
      "Epoch 1/1 | Step 74000/839071 | Loss: 4.71615430797101e-06\n",
      "Epoch 1/1 | Step 75000/839071 | Loss: 2.719446911214618e-06\n",
      "Epoch 1/1 | Step 76000/839071 | Loss: 1.3709051245314186e-06\n",
      "Epoch 1/1 | Step 77000/839071 | Loss: 4.991887294636399e-07\n",
      "Epoch 1/1 | Step 78000/839071 | Loss: 3.3690317650325596e-05\n",
      "Epoch 1/1 | Step 79000/839071 | Loss: 0.00026709274970926344\n",
      "Epoch 1/1 | Step 80000/839071 | Loss: 2.2067177269491367e-05\n",
      "Epoch 1/1 | Step 81000/839071 | Loss: 5.431440513348207e-06\n",
      "Epoch 1/1 | Step 82000/839071 | Loss: 2.0042023152200272e-06\n",
      "Epoch 1/1 | Step 83000/839071 | Loss: 6.3105094341153745e-06\n",
      "Epoch 1/1 | Step 84000/839071 | Loss: 7.644160177733283e-06\n",
      "Epoch 1/1 | Step 85000/839071 | Loss: 3.7699392123613507e-06\n",
      "Epoch 1/1 | Step 86000/839071 | Loss: 1.853513458627276e-05\n",
      "Epoch 1/1 | Step 87000/839071 | Loss: 3.4942966067319503e-06\n",
      "Epoch 1/1 | Step 88000/839071 | Loss: 1.1324867728035315e-06\n",
      "Epoch 1/1 | Step 89000/839071 | Loss: 2.2649605853075627e-06\n",
      "Epoch 1/1 | Step 90000/839071 | Loss: 4.3585050661931746e-06\n",
      "Epoch 1/1 | Step 91000/839071 | Loss: 1.0765281331259757e-05\n",
      "Epoch 1/1 | Step 92000/839071 | Loss: 1.0281760296493303e-06\n",
      "Epoch 1/1 | Step 93000/839071 | Loss: 1.1026837682948099e-06\n",
      "Epoch 1/1 | Step 94000/839071 | Loss: 3.4272656534994894e-07\n",
      "Epoch 1/1 | Step 95000/839071 | Loss: 2.756714536644722e-07\n",
      "Epoch 1/1 | Step 96000/839071 | Loss: 5.818711088068085e-06\n",
      "Epoch 1/1 | Step 97000/839071 | Loss: 2.3412929294863716e-05\n",
      "Epoch 1/1 | Step 98000/839071 | Loss: 4.917379214930406e-07\n",
      "Epoch 1/1 | Step 99000/839071 | Loss: 4.246826392773073e-07\n",
      "Epoch 1/1 | Step 100000/839071 | Loss: 1.0356272923672805e-06\n",
      "Epoch 1/1 | Step 101000/839071 | Loss: 1.7953963833861053e-05\n",
      "Epoch 1/1 | Step 102000/839071 | Loss: 1.3932512956671417e-06\n",
      "Epoch 1/1 | Step 103000/839071 | Loss: 2.3841803340474144e-06\n",
      "Epoch 1/1 | Step 104000/839071 | Loss: 1.7881366147776134e-06\n",
      "Epoch 1/1 | Step 105000/839071 | Loss: 8.642668376523943e-07\n",
      "Epoch 1/1 | Step 106000/839071 | Loss: 4.7846187953837216e-05\n",
      "Epoch 1/1 | Step 107000/839071 | Loss: 6.705517989757936e-07\n",
      "Epoch 1/1 | Step 108000/839071 | Loss: 1.0430785550852306e-06\n",
      "Epoch 1/1 | Step 109000/839071 | Loss: 2.2873100533615798e-06\n",
      "Epoch 1/1 | Step 110000/839071 | Loss: 4.194635039311834e-06\n",
      "Epoch 1/1 | Step 111000/839071 | Loss: 3.35276041596444e-07\n",
      "Epoch 1/1 | Step 112000/839071 | Loss: 4.842868861487659e-07\n",
      "Epoch 1/1 | Step 113000/839071 | Loss: 1.3411042232291948e-07\n",
      "Epoch 1/1 | Step 114000/839071 | Loss: 3.948800326725177e-07\n",
      "Epoch 1/1 | Step 115000/839071 | Loss: 4.6193554226192646e-07\n",
      "Epoch 1/1 | Step 116000/839071 | Loss: 2.533195697651536e-07\n",
      "Epoch 1/1 | Step 117000/839071 | Loss: 1.5646048723283457e-06\n",
      "Epoch 1/1 | Step 118000/839071 | Loss: 1.2143336789449677e-05\n",
      "Epoch 1/1 | Step 119000/839071 | Loss: 3.725288593159348e-07\n",
      "Epoch 1/1 | Step 120000/839071 | Loss: 1.4901159772762185e-07\n",
      "Epoch 1/1 | Step 121000/839071 | Loss: 2.2351724737745826e-07\n",
      "Epoch 1/1 | Step 122000/839071 | Loss: 2.0116376617806964e-06\n",
      "Epoch 1/1 | Step 123000/839071 | Loss: 3.039768216694938e-06\n",
      "Epoch 1/1 | Step 124000/839071 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 125000/839071 | Loss: 2.7567139682105335e-07\n",
      "Epoch 1/1 | Step 126000/839071 | Loss: 1.639127447106148e-07\n",
      "Epoch 1/1 | Step 127000/839071 | Loss: 8.195637946073475e-08\n",
      "Epoch 1/1 | Step 128000/839071 | Loss: 0.0003844269667752087\n",
      "Epoch 1/1 | Step 129000/839071 | Loss: 5.759161922469502e-06\n",
      "Epoch 1/1 | Step 130000/839071 | Loss: 1.0206895240116864e-05\n",
      "Epoch 1/1 | Step 131000/839071 | Loss: 1.259145278709184e-06\n",
      "Epoch 1/1 | Step 132000/839071 | Loss: 2.8908170861541294e-06\n",
      "Epoch 1/1 | Step 133000/839071 | Loss: 1.5720656847406644e-06\n",
      "Epoch 1/1 | Step 134000/839071 | Loss: 1.08778021967737e-06\n",
      "Epoch 1/1 | Step 135000/839071 | Loss: 1.2107012480555568e-05\n",
      "Epoch 1/1 | Step 136000/839071 | Loss: 4.2169840526185e-06\n",
      "Epoch 1/1 | Step 137000/839071 | Loss: 3.538999862939818e-06\n",
      "Epoch 1/1 | Step 138000/839071 | Loss: 4.097817054571351e-07\n",
      "Epoch 1/1 | Step 139000/839071 | Loss: 8.962936590251047e-06\n",
      "Epoch 1/1 | Step 140000/839071 | Loss: 7.778214239806402e-06\n",
      "Epoch 1/1 | Step 141000/839071 | Loss: 1.1175856116096838e-06\n",
      "Epoch 1/1 | Step 142000/839071 | Loss: 2.033997134276433e-06\n",
      "Epoch 1/1 | Step 143000/839071 | Loss: 3.3676415114314295e-06\n",
      "Epoch 1/1 | Step 144000/839071 | Loss: 9.611241011953098e-07\n",
      "Epoch 1/1 | Step 145000/839071 | Loss: 1.7955794646695722e-06\n",
      "Epoch 1/1 | Step 146000/839071 | Loss: 4.023311532819207e-07\n",
      "Epoch 1/1 | Step 147000/839071 | Loss: 2.831186520779738e-06\n",
      "Epoch 1/1 | Step 148000/839071 | Loss: 6.854525622657093e-07\n",
      "Epoch 1/1 | Step 149000/839071 | Loss: 3.8145949474710505e-06\n",
      "Epoch 1/1 | Step 150000/839071 | Loss: 7.525063097091333e-07\n",
      "Epoch 1/1 | Step 151000/839071 | Loss: 1.5646213569198153e-07\n",
      "Epoch 1/1 | Step 152000/839071 | Loss: 1.0579778972896747e-06\n",
      "Epoch 1/1 | Step 153000/839071 | Loss: 2.9205627924966393e-06\n",
      "Epoch 1/1 | Step 154000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 155000/839071 | Loss: 8.784093552094419e-06\n",
      "Epoch 1/1 | Step 156000/839071 | Loss: 1.2523601071734447e-05\n",
      "Epoch 1/1 | Step 157000/839071 | Loss: 9.536727247905219e-07\n",
      "Epoch 1/1 | Step 158000/839071 | Loss: 1.221893057845591e-06\n",
      "Epoch 1/1 | Step 159000/839071 | Loss: 1.6465755834360607e-06\n",
      "Epoch 1/1 | Step 160000/839071 | Loss: 1.3082079021842219e-05\n",
      "Epoch 1/1 | Step 161000/839071 | Loss: 8.369877468794584e-05\n",
      "Epoch 1/1 | Step 162000/839071 | Loss: 5.058902388554998e-06\n",
      "Epoch 1/1 | Step 163000/839071 | Loss: 1.0207274954154855e-06\n",
      "Epoch 1/1 | Step 164000/839071 | Loss: 5.177644197829068e-05\n",
      "Epoch 1/1 | Step 165000/839071 | Loss: 3.2260377338388935e-06\n",
      "Epoch 1/1 | Step 166000/839071 | Loss: 1.4603108411392896e-06\n",
      "Epoch 1/1 | Step 167000/839071 | Loss: 7.107663350325311e-06\n",
      "Epoch 1/1 | Step 168000/839071 | Loss: 1.4512371308228467e-05\n",
      "Epoch 1/1 | Step 169000/839071 | Loss: 2.0963405404472724e-05\n",
      "Epoch 1/1 | Step 170000/839071 | Loss: 5.774107194156386e-06\n",
      "Epoch 1/1 | Step 171000/839071 | Loss: 2.682207878024201e-07\n",
      "Epoch 1/1 | Step 172000/839071 | Loss: 5.885949008188618e-07\n",
      "Epoch 1/1 | Step 173000/839071 | Loss: 4.49256776846596e-06\n",
      "Epoch 1/1 | Step 174000/839071 | Loss: 2.4586910285506747e-07\n",
      "Epoch 1/1 | Step 175000/839071 | Loss: 5.3866306188865565e-06\n",
      "Epoch 1/1 | Step 176000/839071 | Loss: 2.27241798711475e-06\n",
      "Epoch 1/1 | Step 177000/839071 | Loss: 1.6912657656575902e-06\n",
      "Epoch 1/1 | Step 178000/839071 | Loss: 3.256068885093555e-05\n",
      "Epoch 1/1 | Step 179000/839071 | Loss: 5.885953555662127e-07\n",
      "Epoch 1/1 | Step 180000/839071 | Loss: 1.020724425870867e-06\n",
      "Epoch 1/1 | Step 181000/839071 | Loss: 2.1606678046737215e-07\n",
      "Epoch 1/1 | Step 182000/839071 | Loss: 6.645594567089574e-06\n",
      "Epoch 1/1 | Step 183000/839071 | Loss: 1.080327251656854e-06\n",
      "Epoch 1/1 | Step 184000/839071 | Loss: 4.350994458945934e-06\n",
      "Epoch 1/1 | Step 185000/839071 | Loss: 9.685753354915505e-08\n",
      "Epoch 1/1 | Step 186000/839071 | Loss: 1.0430810704065152e-07\n",
      "Epoch 1/1 | Step 187000/839071 | Loss: 2.3096789902865567e-07\n",
      "Epoch 1/1 | Step 188000/839071 | Loss: 6.929020628376747e-07\n",
      "Epoch 1/1 | Step 189000/839071 | Loss: 4.619343485501304e-07\n",
      "Epoch 1/1 | Step 190000/839071 | Loss: 2.682205888504541e-07\n",
      "Epoch 1/1 | Step 191000/839071 | Loss: 7.450579175838357e-08\n",
      "Epoch 1/1 | Step 192000/839071 | Loss: 1.266597990934315e-07\n",
      "Epoch 1/1 | Step 193000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 194000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 195000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 196000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 197000/839071 | Loss: 5.066388553132128e-07\n",
      "Epoch 1/1 | Step 198000/839071 | Loss: 2.2127858301246306e-06\n",
      "Epoch 1/1 | Step 199000/839071 | Loss: 3.129241292754159e-07\n",
      "Epoch 1/1 | Step 200000/839071 | Loss: 4.470348002882929e-08\n",
      "Epoch 1/1 | Step 201000/839071 | Loss: 1.4156094607642444e-07\n",
      "Epoch 1/1 | Step 202000/839071 | Loss: 6.705520405603238e-08\n",
      "Epoch 1/1 | Step 203000/839071 | Loss: 8.121079986267432e-07\n",
      "Epoch 1/1 | Step 204000/839071 | Loss: 2.3841836593874177e-07\n",
      "Epoch 1/1 | Step 205000/839071 | Loss: 2.1606670941309858e-07\n",
      "Epoch 1/1 | Step 206000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 207000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 208000/839071 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 209000/839071 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 210000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 211000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 212000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 213000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 214000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 215000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 216000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 217000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 218000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 219000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 220000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 221000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 222000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 223000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 224000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 225000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 226000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 227000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 228000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 229000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 230000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 231000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 232000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 233000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 234000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 235000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 236000/839071 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 237000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 238000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 239000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 240000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 241000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 242000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 243000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 244000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 245000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 246000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 247000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 248000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 249000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 250000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 251000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 252000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 253000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 254000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 255000/839071 | Loss: 5.9604616353681195e-08\n",
      "Epoch 1/1 | Step 256000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 257000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 258000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 259000/839071 | Loss: 9.849111847870518e-06\n",
      "Epoch 1/1 | Step 260000/839071 | Loss: 2.875874997698702e-06\n",
      "Epoch 1/1 | Step 261000/839071 | Loss: 1.8626445807967684e-07\n",
      "Epoch 1/1 | Step 262000/839071 | Loss: 1.2665984172599565e-07\n",
      "Epoch 1/1 | Step 263000/839071 | Loss: 9.685751933830034e-08\n",
      "Epoch 1/1 | Step 264000/839071 | Loss: 9.685754065458241e-08\n",
      "Epoch 1/1 | Step 265000/839071 | Loss: 1.9371499604403652e-07\n",
      "Epoch 1/1 | Step 266000/839071 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 267000/839071 | Loss: 5.960463766996327e-08\n",
      "Epoch 1/1 | Step 268000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 269000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 270000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 271000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 272000/839071 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 273000/839071 | Loss: 1.7881369274164172e-07\n",
      "Epoch 1/1 | Step 274000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 275000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 276000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 277000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 278000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 279000/839071 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 280000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 281000/839071 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 282000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 283000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 284000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 285000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 286000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 287000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 288000/839071 | Loss: 5.215393343860342e-07\n",
      "Epoch 1/1 | Step 289000/839071 | Loss: 3.948804305764497e-07\n",
      "Epoch 1/1 | Step 290000/839071 | Loss: 3.501768048863596e-07\n",
      "Epoch 1/1 | Step 291000/839071 | Loss: 5.587928058048419e-07\n",
      "Epoch 1/1 | Step 292000/839071 | Loss: 4.276601885067066e-06\n",
      "Epoch 1/1 | Step 293000/839071 | Loss: 3.032363792954129e-06\n",
      "Epoch 1/1 | Step 294000/839071 | Loss: 1.2293435247556772e-06\n",
      "Epoch 1/1 | Step 295000/839071 | Loss: 6.705514579152805e-07\n",
      "Epoch 1/1 | Step 296000/839071 | Loss: 6.377391855494352e-06\n",
      "Epoch 1/1 | Step 297000/839071 | Loss: 2.099977973557543e-05\n",
      "Epoch 1/1 | Step 298000/839071 | Loss: 7.89760633779224e-07\n",
      "Epoch 1/1 | Step 299000/839071 | Loss: 4.842875114263734e-07\n",
      "Epoch 1/1 | Step 300000/839071 | Loss: 3.8742990682294476e-07\n",
      "Epoch 1/1 | Step 301000/839071 | Loss: 5.662430453412526e-07\n",
      "Epoch 1/1 | Step 302000/839071 | Loss: 1.0207237437498407e-06\n",
      "Epoch 1/1 | Step 303000/839071 | Loss: 7.748590178380255e-07\n",
      "Epoch 1/1 | Step 304000/839071 | Loss: 1.4156101713069802e-07\n",
      "Epoch 1/1 | Step 305000/839071 | Loss: 1.4901156930591242e-07\n",
      "Epoch 1/1 | Step 306000/839071 | Loss: 1.3038406905252486e-06\n",
      "Epoch 1/1 | Step 307000/839071 | Loss: 2.4586890390310145e-07\n",
      "Epoch 1/1 | Step 308000/839071 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 309000/839071 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 310000/839071 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 311000/839071 | Loss: 4.7234980229404755e-06\n",
      "Epoch 1/1 | Step 312000/839071 | Loss: 7.450579175838357e-08\n",
      "Epoch 1/1 | Step 313000/839071 | Loss: 4.3958291939816263e-07\n",
      "Epoch 1/1 | Step 314000/839071 | Loss: 3.3825090213213116e-06\n",
      "Epoch 1/1 | Step 315000/839071 | Loss: 1.998000698222313e-05\n",
      "Epoch 1/1 | Step 316000/839071 | Loss: 5.960463766996327e-08\n",
      "Epoch 1/1 | Step 317000/839071 | Loss: 3.129240440102876e-07\n",
      "Epoch 1/1 | Step 318000/839071 | Loss: 3.427259684940509e-07\n",
      "Epoch 1/1 | Step 319000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 320000/839071 | Loss: 2.0041757125000004e-06\n",
      "Epoch 1/1 | Step 321000/839071 | Loss: 1.3411039390121005e-07\n",
      "Epoch 1/1 | Step 322000/839071 | Loss: 5.289901992000523e-07\n",
      "Epoch 1/1 | Step 323000/839071 | Loss: 0.00026929430896416306\n",
      "Epoch 1/1 | Step 324000/839071 | Loss: 3.993470272689592e-06\n",
      "Epoch 1/1 | Step 325000/839071 | Loss: 1.2740467809635447e-06\n",
      "Epoch 1/1 | Step 326000/839071 | Loss: 2.0563422822306165e-06\n",
      "Epoch 1/1 | Step 327000/839071 | Loss: 2.197905814682599e-06\n",
      "Epoch 1/1 | Step 328000/839071 | Loss: 8.6200298028416e-06\n",
      "Epoch 1/1 | Step 329000/839071 | Loss: 1.0505304999242071e-06\n",
      "Epoch 1/1 | Step 330000/839071 | Loss: 5.066388553132128e-07\n",
      "Epoch 1/1 | Step 331000/839071 | Loss: 0.00017543876310810447\n",
      "Epoch 1/1 | Step 332000/839071 | Loss: 1.1697355830619927e-06\n",
      "Epoch 1/1 | Step 333000/839071 | Loss: 6.407492492144229e-07\n",
      "Epoch 1/1 | Step 334000/839071 | Loss: 7.450579886381092e-08\n",
      "Epoch 1/1 | Step 335000/839071 | Loss: 3.4346328448009444e-06\n",
      "Epoch 1/1 | Step 336000/839071 | Loss: 5.215405352032576e-08\n",
      "Epoch 1/1 | Step 337000/839071 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 338000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 339000/839071 | Loss: 8.19563723553074e-08\n",
      "Epoch 1/1 | Step 340000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 341000/839071 | Loss: 1.36344181100867e-06\n",
      "Epoch 1/1 | Step 342000/839071 | Loss: 1.6018592532418552e-06\n",
      "Epoch 1/1 | Step 343000/839071 | Loss: 6.63099910980236e-07\n",
      "Epoch 1/1 | Step 344000/839071 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 345000/839071 | Loss: 7.823063583600742e-07\n",
      "Epoch 1/1 | Step 346000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 347000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 348000/839071 | Loss: 7.912025466794148e-06\n",
      "Epoch 1/1 | Step 349000/839071 | Loss: 3.740125521289883e-06\n",
      "Epoch 1/1 | Step 350000/839071 | Loss: 1.1697360378093435e-06\n",
      "Epoch 1/1 | Step 351000/839071 | Loss: 7.450577754752885e-08\n",
      "Epoch 1/1 | Step 352000/839071 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 353000/839071 | Loss: 3.278247504567844e-07\n",
      "Epoch 1/1 | Step 354000/839071 | Loss: 2.53319598186863e-07\n",
      "Epoch 1/1 | Step 355000/839071 | Loss: 3.948798621422611e-07\n",
      "Epoch 1/1 | Step 356000/839071 | Loss: 1.752878051775042e-05\n",
      "Epoch 1/1 | Step 357000/839071 | Loss: 4.194657321932027e-06\n",
      "Epoch 1/1 | Step 358000/839071 | Loss: 1.0803327086250647e-06\n",
      "Epoch 1/1 | Step 359000/839071 | Loss: 3.799791272740549e-07\n",
      "Epoch 1/1 | Step 360000/839071 | Loss: 3.725287456290971e-07\n",
      "Epoch 1/1 | Step 361000/839071 | Loss: 3.5762769812208717e-07\n",
      "Epoch 1/1 | Step 362000/839071 | Loss: 7.450579175838357e-08\n",
      "Epoch 1/1 | Step 363000/839071 | Loss: 0.0001384128991048783\n",
      "Epoch 1/1 | Step 364000/839071 | Loss: 1.0199019015999511e-05\n",
      "Epoch 1/1 | Step 365000/839071 | Loss: 3.6134520087216515e-06\n",
      "Epoch 1/1 | Step 366000/839071 | Loss: 3.7326549318095203e-06\n",
      "Epoch 1/1 | Step 367000/839071 | Loss: 4.172314049810666e-07\n",
      "Epoch 1/1 | Step 368000/839071 | Loss: 2.0861597249677288e-07\n",
      "Epoch 1/1 | Step 369000/839071 | Loss: 2.7790083549916744e-06\n",
      "Epoch 1/1 | Step 370000/839071 | Loss: 3.2037414143815113e-07\n",
      "Epoch 1/1 | Step 371000/839071 | Loss: 1.1286625522188842e-05\n",
      "Epoch 1/1 | Step 372000/839071 | Loss: 1.884968924059649e-06\n",
      "Epoch 1/1 | Step 373000/839071 | Loss: 1.139929281634977e-06\n",
      "Epoch 1/1 | Step 374000/839071 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 375000/839071 | Loss: 2.980227122861834e-07\n",
      "Epoch 1/1 | Step 376000/839071 | Loss: 2.1606658151540614e-07\n",
      "Epoch 1/1 | Step 377000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 378000/839071 | Loss: 0.0005690555553883314\n",
      "Epoch 1/1 | Step 379000/839071 | Loss: 5.364410071706516e-07\n",
      "Epoch 1/1 | Step 380000/839071 | Loss: 3.874298499795259e-07\n",
      "Epoch 1/1 | Step 381000/839071 | Loss: 1.8775197077047778e-06\n",
      "Epoch 1/1 | Step 382000/839071 | Loss: 0.0004368220397736877\n",
      "Epoch 1/1 | Step 383000/839071 | Loss: 0.00010047354589914903\n",
      "Epoch 1/1 | Step 384000/839071 | Loss: 4.753411758429138e-06\n",
      "Epoch 1/1 | Step 385000/839071 | Loss: 2.905721885326784e-07\n",
      "Epoch 1/1 | Step 386000/839071 | Loss: 2.3841842278216063e-07\n",
      "Epoch 1/1 | Step 387000/839071 | Loss: 3.4123295336030424e-06\n",
      "Epoch 1/1 | Step 388000/839071 | Loss: 1.5720630699433968e-06\n",
      "Epoch 1/1 | Step 389000/839071 | Loss: 1.2293430700083263e-06\n",
      "Epoch 1/1 | Step 390000/839071 | Loss: 1.0422642844787333e-05\n",
      "Epoch 1/1 | Step 391000/839071 | Loss: 8.940690321423972e-08\n",
      "Epoch 1/1 | Step 392000/839071 | Loss: 1.31490141939139e-05\n",
      "Epoch 1/1 | Step 393000/839071 | Loss: 2.53319399234897e-07\n",
      "Epoch 1/1 | Step 394000/839071 | Loss: 4.470347647611561e-08\n",
      "Epoch 1/1 | Step 395000/839071 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 396000/839071 | Loss: 6.705519695060502e-08\n",
      "Epoch 1/1 | Step 397000/839071 | Loss: 1.7881384906104358e-07\n",
      "Epoch 1/1 | Step 398000/839071 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 399000/839071 | Loss: 3.2782480730020325e-07\n",
      "Epoch 1/1 | Step 400000/839071 | Loss: 3.129236461063556e-07\n",
      "Epoch 1/1 | Step 401000/839071 | Loss: 5.2154042862184724e-08\n",
      "Epoch 1/1 | Step 402000/839071 | Loss: 8.195632972274325e-08\n",
      "Epoch 1/1 | Step 403000/839071 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 404000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 405000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 406000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 407000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 408000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 409000/839071 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 410000/839071 | Loss: 8.940693874137651e-08\n",
      "Epoch 1/1 | Step 411000/839071 | Loss: 2.1159296466066735e-06\n",
      "Epoch 1/1 | Step 412000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 413000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 414000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 415000/839071 | Loss: 3.6507805134533555e-07\n",
      "Epoch 1/1 | Step 416000/839071 | Loss: 1.7881384906104358e-07\n",
      "Epoch 1/1 | Step 417000/839071 | Loss: 1.2367917179290089e-06\n",
      "Epoch 1/1 | Step 418000/839071 | Loss: 6.034727903170278e-06\n",
      "Epoch 1/1 | Step 419000/839071 | Loss: 1.2963948847755091e-06\n",
      "Epoch 1/1 | Step 420000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 421000/839071 | Loss: 1.1175868053214799e-07\n",
      "Epoch 1/1 | Step 422000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 423000/839071 | Loss: 1.2665981330428622e-07\n",
      "Epoch 1/1 | Step 424000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 425000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 426000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 427000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 428000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 429000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 430000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 431000/839071 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 432000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 433000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 434000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 435000/839071 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 436000/839071 | Loss: 9.834748198045418e-07\n",
      "Epoch 1/1 | Step 437000/839071 | Loss: 1.0877832892219885e-06\n",
      "Epoch 1/1 | Step 438000/839071 | Loss: 2.682207878024201e-07\n",
      "Epoch 1/1 | Step 439000/839071 | Loss: 1.7061777271010214e-06\n",
      "Epoch 1/1 | Step 440000/839071 | Loss: 0.0022787973284721375\n",
      "Epoch 1/1 | Step 441000/839071 | Loss: 5.453756784845609e-06\n",
      "Epoch 1/1 | Step 442000/839071 | Loss: 2.1457451566675445e-06\n",
      "Epoch 1/1 | Step 443000/839071 | Loss: 4.7012445065774955e-06\n",
      "Epoch 1/1 | Step 444000/839071 | Loss: 3.1738954930915497e-06\n",
      "Epoch 1/1 | Step 445000/839071 | Loss: 9.476268314756453e-05\n",
      "Epoch 1/1 | Step 446000/839071 | Loss: 5.081121798866661e-06\n",
      "Epoch 1/1 | Step 447000/839071 | Loss: 4.3958360151918896e-07\n",
      "Epoch 1/1 | Step 448000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 449000/839071 | Loss: 9.31318311359064e-07\n",
      "Epoch 1/1 | Step 450000/839071 | Loss: 6.631001951973303e-07\n",
      "Epoch 1/1 | Step 451000/839071 | Loss: 9.685751933830034e-08\n",
      "Epoch 1/1 | Step 452000/839071 | Loss: 1.7881387748275301e-07\n",
      "Epoch 1/1 | Step 453000/839071 | Loss: 6.780010721740837e-07\n",
      "Epoch 1/1 | Step 454000/839071 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 455000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 456000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 457000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 458000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 459000/839071 | Loss: 0.0011729191755875945\n",
      "Epoch 1/1 | Step 460000/839071 | Loss: 5.9601838984235656e-06\n",
      "Epoch 1/1 | Step 461000/839071 | Loss: 2.1606669520224386e-07\n",
      "Epoch 1/1 | Step 462000/839071 | Loss: 2.0116546295412263e-07\n",
      "Epoch 1/1 | Step 463000/839071 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 464000/839071 | Loss: 1.4901152667334827e-07\n",
      "Epoch 1/1 | Step 465000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 466000/839071 | Loss: 1.9371481130292523e-07\n",
      "Epoch 1/1 | Step 467000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 468000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 469000/839071 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 470000/839071 | Loss: 4.4703469370688254e-08\n",
      "Epoch 1/1 | Step 471000/839071 | Loss: 3.7252892326478104e-08\n",
      "Epoch 1/1 | Step 472000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 473000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 474000/839071 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 475000/839071 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 476000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 477000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 478000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 479000/839071 | Loss: 8.627444003650453e-06\n",
      "Epoch 1/1 | Step 480000/839071 | Loss: 3.042155185539741e-05\n",
      "Epoch 1/1 | Step 481000/839071 | Loss: 5.960444582342461e-07\n",
      "Epoch 1/1 | Step 482000/839071 | Loss: 7.994081897777505e-06\n",
      "Epoch 1/1 | Step 483000/839071 | Loss: 1.2225686987221707e-05\n",
      "Epoch 1/1 | Step 484000/839071 | Loss: 7.420546353387181e-06\n",
      "Epoch 1/1 | Step 485000/839071 | Loss: 2.9464052204275504e-05\n",
      "Epoch 1/1 | Step 486000/839071 | Loss: 6.630820735153975e-06\n",
      "Epoch 1/1 | Step 487000/839071 | Loss: 1.573471490701195e-05\n",
      "Epoch 1/1 | Step 488000/839071 | Loss: 1.057974486684543e-06\n",
      "Epoch 1/1 | Step 489000/839071 | Loss: 9.015186037686362e-07\n",
      "Epoch 1/1 | Step 490000/839071 | Loss: 6.258479174903187e-07\n",
      "Epoch 1/1 | Step 491000/839071 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 492000/839071 | Loss: 5.140892085364612e-07\n",
      "Epoch 1/1 | Step 493000/839071 | Loss: 3.129241292754159e-07\n",
      "Epoch 1/1 | Step 494000/839071 | Loss: 1.415609887089886e-07\n",
      "Epoch 1/1 | Step 495000/839071 | Loss: 5.982636594126234e-06\n",
      "Epoch 1/1 | Step 496000/839071 | Loss: 3.94880316889612e-07\n",
      "Epoch 1/1 | Step 497000/839071 | Loss: 4.0978164861371624e-07\n",
      "Epoch 1/1 | Step 498000/839071 | Loss: 1.1376650036254432e-05\n",
      "Epoch 1/1 | Step 499000/839071 | Loss: 1.3336504025573959e-06\n",
      "Epoch 1/1 | Step 500000/839071 | Loss: 2.622583679112722e-06\n",
      "Epoch 1/1 | Step 501000/839071 | Loss: 1.9294124285806902e-05\n",
      "Epoch 1/1 | Step 502000/839071 | Loss: 9.685751933830034e-08\n",
      "Epoch 1/1 | Step 503000/839071 | Loss: 3.2782529046926356e-07\n",
      "Epoch 1/1 | Step 504000/839071 | Loss: 5.0096648919861764e-05\n",
      "Epoch 1/1 | Step 505000/839071 | Loss: 4.053065367770614e-06\n",
      "Epoch 1/1 | Step 506000/839071 | Loss: 1.90858499991009e-05\n",
      "Epoch 1/1 | Step 507000/839071 | Loss: 5.438919288280886e-07\n",
      "Epoch 1/1 | Step 508000/839071 | Loss: 4.0233118170363014e-07\n",
      "Epoch 1/1 | Step 509000/839071 | Loss: 1.7285281046497403e-06\n",
      "Epoch 1/1 | Step 510000/839071 | Loss: 2.0861619987044833e-07\n",
      "Epoch 1/1 | Step 511000/839071 | Loss: 2.980231101901154e-07\n",
      "Epoch 1/1 | Step 512000/839071 | Loss: 5.960463766996327e-08\n",
      "Epoch 1/1 | Step 513000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 514000/839071 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 515000/839071 | Loss: 4.840546171180904e-05\n",
      "Epoch 1/1 | Step 516000/839071 | Loss: 1.4528584415529622e-06\n",
      "Epoch 1/1 | Step 517000/839071 | Loss: 2.1985246348776855e-05\n",
      "Epoch 1/1 | Step 518000/839071 | Loss: 3.1366712391900364e-06\n",
      "Epoch 1/1 | Step 519000/839071 | Loss: 6.804218719480559e-05\n",
      "Epoch 1/1 | Step 520000/839071 | Loss: 6.070336894481443e-05\n",
      "Epoch 1/1 | Step 521000/839071 | Loss: 4.395839425797021e-07\n",
      "Epoch 1/1 | Step 522000/839071 | Loss: 2.2873061880090972e-06\n",
      "Epoch 1/1 | Step 523000/839071 | Loss: 3.1888214380160207e-06\n",
      "Epoch 1/1 | Step 524000/839071 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 525000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 526000/839071 | Loss: 5.960463766996327e-08\n",
      "Epoch 1/1 | Step 527000/839071 | Loss: 2.0116539189984906e-07\n",
      "Epoch 1/1 | Step 528000/839071 | Loss: 1.1175863789958385e-07\n",
      "Epoch 1/1 | Step 529000/839071 | Loss: 6.705519695060502e-08\n",
      "Epoch 1/1 | Step 530000/839071 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 531000/839071 | Loss: 9.685750512744562e-08\n",
      "Epoch 1/1 | Step 532000/839071 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 533000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 534000/839071 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 535000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 536000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 537000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 538000/839071 | Loss: 0.0\n",
      "Epoch 1/1 | Step 539000/839071 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 540000/839071 | Loss: 1.2397038517519832e-05\n",
      "Epoch 1/1 | Step 541000/839071 | Loss: 0.0003064796910621226\n",
      "Epoch 1/1 | Step 542000/839071 | Loss: 1.2665984172599565e-07\n",
      "Epoch 1/1 | Step 543000/839071 | Loss: 1.1175869474300271e-07\n",
      "Epoch 1/1 | Step 544000/839071 | Loss: 1.1920926112907182e-07\n",
      "Epoch 1/1 | Step 545000/839071 | Loss: 1.683816208242206e-06\n",
      "Epoch 1/1 | Step 546000/839071 | Loss: 0.00011209146759938449\n",
      "Epoch 1/1 | Step 547000/839071 | Loss: 1.4602995861423551e-06\n",
      "Epoch 1/1 | Step 548000/839071 | Loss: 6.325397407636046e-06\n",
      "Epoch 1/1 | Step 549000/839071 | Loss: 7.078045882735751e-07\n",
      "Epoch 1/1 | Step 550000/839071 | Loss: 7.152532930376765e-07\n",
      "Epoch 1/1 | Step 551000/839071 | Loss: 2.8758645385096315e-06\n",
      "Epoch 1/1 | Step 552000/839071 | Loss: 6.876538009237265e-06\n",
      "Epoch 1/1 | Step 553000/839071 | Loss: 3.410761564737186e-05\n",
      "Epoch 1/1 | Step 554000/839071 | Loss: 1.8030211776931537e-06\n",
      "Epoch 1/1 | Step 555000/839071 | Loss: 5.96044912981597e-07\n",
      "Epoch 1/1 | Step 556000/839071 | Loss: 1.6912750879782834e-06\n",
      "Epoch 1/1 | Step 557000/839071 | Loss: 1.2814969068131177e-06\n",
      "Epoch 1/1 | Step 558000/839071 | Loss: 7.227055789371661e-07\n",
      "Epoch 1/1 | Step 559000/839071 | Loss: 7.710897079959977e-06\n",
      "Epoch 1/1 | Step 560000/839071 | Loss: 6.631004225710058e-07\n",
      "Epoch 1/1 | Step 561000/839071 | Loss: 2.0861614302702947e-07\n",
      "Epoch 1/1 | Step 562000/839071 | Loss: 4.321328219702991e-07\n",
      "Epoch 1/1 | Step 563000/839071 | Loss: 1.3411040811206476e-07\n",
      "Epoch 1/1 | Step 564000/839071 | Loss: 2.3740600227029063e-05\n",
      "Epoch 1/1 | Step 565000/839071 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 566000/839071 | Loss: 1.8626438702540327e-07\n",
      "Epoch 1/1 | Step 567000/839071 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 568000/839071 | Loss: 1.9711334971361794e-05\n",
      "Epoch 1/1 | Step 569000/839071 | Loss: 3.598578132368857e-06\n",
      "Epoch 1/1 | Step 570000/839071 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 571000/839071 | Loss: 2.279837417518138e-06\n",
      "Epoch 1/1 | Step 572000/839071 | Loss: 2.2351740014414645e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1318\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1330\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:575\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:486\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader)} | Loss: {loss.item()}\")\n",
    "\n",
    "    # End time for the epoch\n",
    "    epoch_end_time = time.time()\n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}, Time Taken: {epoch_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(b_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "predictions_flat = predictions\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data = pd.DataFrame({\n",
    "    'Month': data.loc[test_idx, 'Month'].values, \n",
    "    'Place': data.loc[test_idx, 'place'].values,\n",
    "    'Crop' : data.loc[test_idx, 'Crop'].values,\n",
    "    'QueryType_code': predictions_flat\n",
    "})\n",
    "print(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QueryType'] = data['QueryType'].astype('category')\n",
    "predicted_data['QueryType']=predicted_data['QueryType_code'].apply(lambda x: data['QueryType'].cat.categories[x])\n",
    "print(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions = predicted_data.groupby(['Month', 'Place', 'Crop'])['QueryType'].apply(lambda x: x.value_counts().index[:10]).reset_index()\n",
    "print(monthly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_predictions= monthly_predictions.explode('QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions = monthly_predictions.groupby(['Month', 'Place','Crop'])['QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_predictions[['Month', 'Place','Crop','QueryType']].to_csv('D:/Data/Top_predicted_cropwise_querytypes_in_India_111.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels, predictions)\n",
    "mae = mae(true_labels, predictions)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "recall = recall(true_labels, predictions)\n",
    "\n",
    "accuracy= accuracy(true_labels, predictions)\n",
    "precision= precision(true_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rsme, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rsme, mae]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/roberts1_model_performance_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROP-QUERYTYPE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Month                            place  \\\n",
      "3039763       1        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039867       2        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039869       2        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040001       4        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040002       4        ANDHRA PRADESH_SRIKAKULAM   \n",
      "...         ...                              ...   \n",
      "37962914     10  DELHI_New Delhi Connaught Place   \n",
      "37962916     10  DELHI_New Delhi Connaught Place   \n",
      "37962917     10  DELHI_New Delhi Connaught Place   \n",
      "37962919     10  DELHI_New Delhi Connaught Place   \n",
      "37962924     10              PUDUCHERRY_KARAIKAL   \n",
      "\n",
      "                                             Crop_QueryType  \n",
      "3039763               Sunflower suryamukhi_Water Management  \n",
      "3039867   Groundnut pea nutmung phalli_Fertilizer Use an...  \n",
      "3039869   Groundnut pea nutmung phalli_Fertilizer Use an...  \n",
      "3040001                        Runner Bean_Plant Protection  \n",
      "3040002                            Coconut_Plant Protection  \n",
      "...                                                     ...  \n",
      "37962914                           Wheat_Government Schemes  \n",
      "37962916                                        Wheat_Seeds  \n",
      "37962917                              Wheat_Organic Farming  \n",
      "37962919                     Spinach Palak_Plant Protection  \n",
      "37962924                      Paddy Dhan_Cultural Practices  \n",
      "\n",
      "[16781410 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\3142573208.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "df[\"Crop_QueryType\"] = df[\"Crop\"] + \"_\" +df[\"QueryType\"]\n",
    "data=df[['Month','place','Crop_QueryType']]\n",
    "\n",
    "print(data)\n",
    "# Encode Crop_QueryType as categorical variable\n",
    "data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\2514243442.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "inputs1 = tokenizer(data['text1'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract labels for training (Crop_QueryType codes)\n",
    "labels1 = torch.tensor(data['Crop_QueryType_code'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data without stratification\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1, train_idx1, test_idx1 = train_test_split(\n",
    "    inputs1['input_ids'], labels1, data.index, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue as normal with train and test sets\n",
    "train_masks1, test_masks1 = train_test_split(inputs1['attention_mask'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\419209612.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\419209612.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
    "train_sampler1 = RandomSampler(train_data1)\n",
    "train_dataloader1 = DataLoader(train_data1, sampler=train_sampler1, batch_size=batch_size)\n",
    "\n",
    "test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n",
    "test_sampler1 = SequentialSampler(test_data1)\n",
    "test_dataloader1 = DataLoader(test_data1, sampler=test_sampler1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8001, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=data['Crop_QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 0/839071 | Loss: 8.96476936340332\n",
      "Epoch 1/1 | Step 1000/839071 | Loss: 3.105717420578003\n",
      "Epoch 1/1 | Step 2000/839071 | Loss: 2.4748892784118652\n",
      "Epoch 1/1 | Step 3000/839071 | Loss: 1.6189265251159668\n",
      "Epoch 1/1 | Step 4000/839071 | Loss: 1.269652009010315\n",
      "Epoch 1/1 | Step 5000/839071 | Loss: 0.4091491997241974\n",
      "Epoch 1/1 | Step 6000/839071 | Loss: 0.5361371636390686\n",
      "Epoch 1/1 | Step 7000/839071 | Loss: 0.34057489037513733\n",
      "Epoch 1/1 | Step 8000/839071 | Loss: 0.5223244428634644\n",
      "Epoch 1/1 | Step 9000/839071 | Loss: 1.172743558883667\n",
      "Epoch 1/1 | Step 10000/839071 | Loss: 0.4117088317871094\n",
      "Epoch 1/1 | Step 11000/839071 | Loss: 1.5317280292510986\n",
      "Epoch 1/1 | Step 12000/839071 | Loss: 0.2253333181142807\n",
      "Epoch 1/1 | Step 13000/839071 | Loss: 0.5722909569740295\n",
      "Epoch 1/1 | Step 14000/839071 | Loss: 0.769890546798706\n",
      "Epoch 1/1 | Step 15000/839071 | Loss: 0.020391954109072685\n",
      "Epoch 1/1 | Step 16000/839071 | Loss: 0.13320358097553253\n",
      "Epoch 1/1 | Step 17000/839071 | Loss: 0.0209311842918396\n",
      "Epoch 1/1 | Step 18000/839071 | Loss: 0.43010544776916504\n",
      "Epoch 1/1 | Step 19000/839071 | Loss: 0.4241594076156616\n",
      "Epoch 1/1 | Step 20000/839071 | Loss: 0.540745735168457\n",
      "Epoch 1/1 | Step 21000/839071 | Loss: 0.1158691793680191\n",
      "Epoch 1/1 | Step 22000/839071 | Loss: 0.0396730862557888\n",
      "Epoch 1/1 | Step 23000/839071 | Loss: 0.0015353241469711065\n",
      "Epoch 1/1 | Step 24000/839071 | Loss: 0.002352912211790681\n",
      "Epoch 1/1 | Step 25000/839071 | Loss: 0.5363467931747437\n",
      "Epoch 1/1 | Step 26000/839071 | Loss: 0.028035759925842285\n",
      "Epoch 1/1 | Step 27000/839071 | Loss: 0.0043987431563436985\n",
      "Epoch 1/1 | Step 28000/839071 | Loss: 0.6859934329986572\n",
      "Epoch 1/1 | Step 29000/839071 | Loss: 0.004701496567577124\n",
      "Epoch 1/1 | Step 30000/839071 | Loss: 0.0003442240704316646\n",
      "Epoch 1/1 | Step 31000/839071 | Loss: 0.24730005860328674\n",
      "Epoch 1/1 | Step 32000/839071 | Loss: 0.8560335040092468\n",
      "Epoch 1/1 | Step 33000/839071 | Loss: 0.00043007326894439757\n",
      "Epoch 1/1 | Step 34000/839071 | Loss: 0.09750250726938248\n",
      "Epoch 1/1 | Step 35000/839071 | Loss: 0.5704634785652161\n",
      "Epoch 1/1 | Step 36000/839071 | Loss: 0.030459335073828697\n",
      "Epoch 1/1 | Step 37000/839071 | Loss: 0.00648578442633152\n",
      "Epoch 1/1 | Step 38000/839071 | Loss: 0.007034815847873688\n",
      "Epoch 1/1 | Step 39000/839071 | Loss: 0.0017898350488394499\n",
      "Epoch 1/1 | Step 40000/839071 | Loss: 0.018297988921403885\n",
      "Epoch 1/1 | Step 41000/839071 | Loss: 0.01752917654812336\n",
      "Epoch 1/1 | Step 42000/839071 | Loss: 0.43631914258003235\n",
      "Epoch 1/1 | Step 43000/839071 | Loss: 0.001928986981511116\n",
      "Epoch 1/1 | Step 44000/839071 | Loss: 0.00034867250360548496\n",
      "Epoch 1/1 | Step 45000/839071 | Loss: 0.3598018288612366\n",
      "Epoch 1/1 | Step 46000/839071 | Loss: 0.5773112177848816\n",
      "Epoch 1/1 | Step 47000/839071 | Loss: 0.000148512699524872\n",
      "Epoch 1/1 | Step 48000/839071 | Loss: 0.017848409712314606\n",
      "Epoch 1/1 | Step 49000/839071 | Loss: 0.28089848160743713\n",
      "Epoch 1/1 | Step 50000/839071 | Loss: 0.0009535663994029164\n",
      "Epoch 1/1 | Step 51000/839071 | Loss: 0.006449404172599316\n",
      "Epoch 1/1 | Step 52000/839071 | Loss: 0.001131390337832272\n",
      "Epoch 1/1 | Step 53000/839071 | Loss: 0.4046659469604492\n",
      "Epoch 1/1 | Step 54000/839071 | Loss: 0.0002746423124335706\n",
      "Epoch 1/1 | Step 55000/839071 | Loss: 0.00018350548634771258\n",
      "Epoch 1/1 | Step 56000/839071 | Loss: 0.0007986588170751929\n",
      "Epoch 1/1 | Step 57000/839071 | Loss: 0.6925951838493347\n",
      "Epoch 1/1 | Step 58000/839071 | Loss: 0.00043637663475237787\n",
      "Epoch 1/1 | Step 59000/839071 | Loss: 0.4000548720359802\n",
      "Epoch 1/1 | Step 60000/839071 | Loss: 0.0007601763936690986\n",
      "Epoch 1/1 | Step 61000/839071 | Loss: 0.0010478972690179944\n",
      "Epoch 1/1 | Step 62000/839071 | Loss: 0.00025107909459620714\n",
      "Epoch 1/1 | Step 63000/839071 | Loss: 0.1199861615896225\n",
      "Epoch 1/1 | Step 64000/839071 | Loss: 0.0006151517736725509\n",
      "Epoch 1/1 | Step 65000/839071 | Loss: 0.14370867609977722\n",
      "Epoch 1/1 | Step 66000/839071 | Loss: 0.003844044404104352\n",
      "Epoch 1/1 | Step 67000/839071 | Loss: 0.5812246203422546\n",
      "Epoch 1/1 | Step 68000/839071 | Loss: 0.4675597548484802\n",
      "Epoch 1/1 | Step 69000/839071 | Loss: 0.0005555013194680214\n",
      "Epoch 1/1 | Step 70000/839071 | Loss: 0.00031773612136021256\n",
      "Epoch 1/1 | Step 71000/839071 | Loss: 0.0008489540196023881\n",
      "Epoch 1/1 | Step 72000/839071 | Loss: 0.000698267889674753\n",
      "Epoch 1/1 | Step 73000/839071 | Loss: 0.0009606675594113767\n",
      "Epoch 1/1 | Step 74000/839071 | Loss: 0.000310244329739362\n",
      "Epoch 1/1 | Step 75000/839071 | Loss: 0.043600812554359436\n",
      "Epoch 1/1 | Step 76000/839071 | Loss: 0.0002661312173586339\n",
      "Epoch 1/1 | Step 77000/839071 | Loss: 0.003924907185137272\n",
      "Epoch 1/1 | Step 78000/839071 | Loss: 0.00021860148990526795\n",
      "Epoch 1/1 | Step 79000/839071 | Loss: 0.00023045473790261894\n",
      "Epoch 1/1 | Step 80000/839071 | Loss: 0.000270986114628613\n",
      "Epoch 1/1 | Step 81000/839071 | Loss: 0.0002369968278799206\n",
      "Epoch 1/1 | Step 82000/839071 | Loss: 0.02930360659956932\n",
      "Epoch 1/1 | Step 83000/839071 | Loss: 0.5539403557777405\n",
      "Epoch 1/1 | Step 84000/839071 | Loss: 0.0016453738790005445\n",
      "Epoch 1/1 | Step 85000/839071 | Loss: 0.2054651379585266\n",
      "Epoch 1/1 | Step 86000/839071 | Loss: 0.0003083623305428773\n",
      "Epoch 1/1 | Step 87000/839071 | Loss: 0.0001855021109804511\n",
      "Epoch 1/1 | Step 88000/839071 | Loss: 0.0020441787783056498\n",
      "Epoch 1/1 | Step 89000/839071 | Loss: 0.46832889318466187\n",
      "Epoch 1/1 | Step 90000/839071 | Loss: 0.0007078175549395382\n",
      "Epoch 1/1 | Step 91000/839071 | Loss: 0.5469039082527161\n",
      "Epoch 1/1 | Step 92000/839071 | Loss: 8.416114724241197e-05\n",
      "Epoch 1/1 | Step 93000/839071 | Loss: 0.0003697509819176048\n",
      "Epoch 1/1 | Step 94000/839071 | Loss: 0.017137762159109116\n",
      "Epoch 1/1 | Step 95000/839071 | Loss: 0.001992261502891779\n",
      "Epoch 1/1 | Step 96000/839071 | Loss: 0.0024839304387569427\n",
      "Epoch 1/1 | Step 97000/839071 | Loss: 0.0006864472525194287\n",
      "Epoch 1/1 | Step 98000/839071 | Loss: 0.28221413493156433\n",
      "Epoch 1/1 | Step 99000/839071 | Loss: 0.00673701660707593\n",
      "Epoch 1/1 | Step 100000/839071 | Loss: 0.6503476500511169\n",
      "Epoch 1/1 | Step 101000/839071 | Loss: 0.0006023971945978701\n",
      "Epoch 1/1 | Step 102000/839071 | Loss: 0.0032158512622117996\n",
      "Epoch 1/1 | Step 103000/839071 | Loss: 0.00028408021898940206\n",
      "Epoch 1/1 | Step 104000/839071 | Loss: 0.0001146336508099921\n",
      "Epoch 1/1 | Step 105000/839071 | Loss: 0.0008978457190096378\n",
      "Epoch 1/1 | Step 106000/839071 | Loss: 0.573113739490509\n",
      "Epoch 1/1 | Step 107000/839071 | Loss: 0.0006435614195652306\n",
      "Epoch 1/1 | Step 108000/839071 | Loss: 0.0033430929761379957\n",
      "Epoch 1/1 | Step 109000/839071 | Loss: 0.001246248953975737\n",
      "Epoch 1/1 | Step 110000/839071 | Loss: 0.00017501651018392295\n",
      "Epoch 1/1 | Step 111000/839071 | Loss: 0.006410010624676943\n",
      "Epoch 1/1 | Step 112000/839071 | Loss: 0.0005471696495078504\n",
      "Epoch 1/1 | Step 113000/839071 | Loss: 0.00047578010708093643\n",
      "Epoch 1/1 | Step 114000/839071 | Loss: 0.0002582398010417819\n",
      "Epoch 1/1 | Step 115000/839071 | Loss: 0.0006394860683940351\n",
      "Epoch 1/1 | Step 116000/839071 | Loss: 0.01166568323969841\n",
      "Epoch 1/1 | Step 117000/839071 | Loss: 8.359076309716329e-05\n",
      "Epoch 1/1 | Step 118000/839071 | Loss: 0.0011883060215041041\n",
      "Epoch 1/1 | Step 119000/839071 | Loss: 0.000136527611175552\n",
      "Epoch 1/1 | Step 120000/839071 | Loss: 0.0009331281180493534\n",
      "Epoch 1/1 | Step 121000/839071 | Loss: 0.0010974513133987784\n",
      "Epoch 1/1 | Step 122000/839071 | Loss: 0.0001786349603207782\n",
      "Epoch 1/1 | Step 123000/839071 | Loss: 0.00013594255142379552\n",
      "Epoch 1/1 | Step 124000/839071 | Loss: 0.1555466651916504\n",
      "Epoch 1/1 | Step 125000/839071 | Loss: 0.004982110112905502\n",
      "Epoch 1/1 | Step 126000/839071 | Loss: 0.00019689112377818674\n",
      "Epoch 1/1 | Step 127000/839071 | Loss: 0.011637476272881031\n",
      "Epoch 1/1 | Step 128000/839071 | Loss: 0.0003352163184899837\n",
      "Epoch 1/1 | Step 129000/839071 | Loss: 0.00039729021955281496\n",
      "Epoch 1/1 | Step 130000/839071 | Loss: 0.0007489666459150612\n",
      "Epoch 1/1 | Step 131000/839071 | Loss: 0.002164962701499462\n",
      "Epoch 1/1 | Step 132000/839071 | Loss: 0.0016087335534393787\n",
      "Epoch 1/1 | Step 133000/839071 | Loss: 0.0009060713346116245\n",
      "Epoch 1/1 | Step 134000/839071 | Loss: 4.104373510926962e-05\n",
      "Epoch 1/1 | Step 135000/839071 | Loss: 0.0029171211645007133\n",
      "Epoch 1/1 | Step 136000/839071 | Loss: 0.00031814826070331037\n",
      "Epoch 1/1 | Step 137000/839071 | Loss: 0.000804524403065443\n",
      "Epoch 1/1 | Step 138000/839071 | Loss: 0.00044697197154164314\n",
      "Epoch 1/1 | Step 139000/839071 | Loss: 0.6110576391220093\n",
      "Epoch 1/1 | Step 140000/839071 | Loss: 0.0008066462469287217\n",
      "Epoch 1/1 | Step 141000/839071 | Loss: 0.00016877327288966626\n",
      "Epoch 1/1 | Step 142000/839071 | Loss: 0.0005658324807882309\n",
      "Epoch 1/1 | Step 143000/839071 | Loss: 0.16953137516975403\n",
      "Epoch 1/1 | Step 144000/839071 | Loss: 0.012443020939826965\n",
      "Epoch 1/1 | Step 145000/839071 | Loss: 0.0003336949157528579\n",
      "Epoch 1/1 | Step 146000/839071 | Loss: 0.00013094767928123474\n",
      "Epoch 1/1 | Step 147000/839071 | Loss: 0.0005005580023862422\n",
      "Epoch 1/1 | Step 148000/839071 | Loss: 0.00011308470857329667\n",
      "Epoch 1/1 | Step 149000/839071 | Loss: 0.00036028437898494303\n",
      "Epoch 1/1 | Step 150000/839071 | Loss: 0.000636527081951499\n",
      "Epoch 1/1 | Step 151000/839071 | Loss: 0.04427183419466019\n",
      "Epoch 1/1 | Step 152000/839071 | Loss: 0.0003310679749120027\n",
      "Epoch 1/1 | Step 153000/839071 | Loss: 6.868213677080348e-05\n",
      "Epoch 1/1 | Step 154000/839071 | Loss: 0.0013775683473795652\n",
      "Epoch 1/1 | Step 155000/839071 | Loss: 0.00040906600770540535\n",
      "Epoch 1/1 | Step 156000/839071 | Loss: 0.007661492098122835\n",
      "Epoch 1/1 | Step 157000/839071 | Loss: 0.00043156935134902596\n",
      "Epoch 1/1 | Step 158000/839071 | Loss: 0.0006156606832519174\n",
      "Epoch 1/1 | Step 159000/839071 | Loss: 0.008748437277972698\n",
      "Epoch 1/1 | Step 160000/839071 | Loss: 0.00042525125900283456\n",
      "Epoch 1/1 | Step 161000/839071 | Loss: 0.0003257619682699442\n",
      "Epoch 1/1 | Step 162000/839071 | Loss: 0.0008633437682874501\n",
      "Epoch 1/1 | Step 163000/839071 | Loss: 0.0006348690367303789\n",
      "Epoch 1/1 | Step 164000/839071 | Loss: 0.0005059154354967177\n",
      "Epoch 1/1 | Step 165000/839071 | Loss: 0.00013544924149755388\n",
      "Epoch 1/1 | Step 166000/839071 | Loss: 0.014179038815200329\n",
      "Epoch 1/1 | Step 167000/839071 | Loss: 0.000716598704457283\n",
      "Epoch 1/1 | Step 168000/839071 | Loss: 0.00025860301684588194\n",
      "Epoch 1/1 | Step 169000/839071 | Loss: 0.005386493168771267\n",
      "Epoch 1/1 | Step 170000/839071 | Loss: 0.25767993927001953\n",
      "Epoch 1/1 | Step 171000/839071 | Loss: 0.2040083408355713\n",
      "Epoch 1/1 | Step 172000/839071 | Loss: 9.669416613178328e-05\n",
      "Epoch 1/1 | Step 173000/839071 | Loss: 0.49258071184158325\n",
      "Epoch 1/1 | Step 174000/839071 | Loss: 0.0032549984753131866\n",
      "Epoch 1/1 | Step 175000/839071 | Loss: 0.7876387238502502\n",
      "Epoch 1/1 | Step 176000/839071 | Loss: 0.5277751088142395\n",
      "Epoch 1/1 | Step 177000/839071 | Loss: 0.5949662327766418\n",
      "Epoch 1/1 | Step 178000/839071 | Loss: 0.00023666011111345142\n",
      "Epoch 1/1 | Step 179000/839071 | Loss: 0.00011099703260697424\n",
      "Epoch 1/1 | Step 180000/839071 | Loss: 0.5587214231491089\n",
      "Epoch 1/1 | Step 181000/839071 | Loss: 0.0007997399661689997\n",
      "Epoch 1/1 | Step 182000/839071 | Loss: 0.0002651591785252094\n",
      "Epoch 1/1 | Step 183000/839071 | Loss: 8.514209184795618e-05\n",
      "Epoch 1/1 | Step 184000/839071 | Loss: 0.006532532162964344\n",
      "Epoch 1/1 | Step 185000/839071 | Loss: 0.0002849932643584907\n",
      "Epoch 1/1 | Step 186000/839071 | Loss: 0.24028022587299347\n",
      "Epoch 1/1 | Step 187000/839071 | Loss: 0.0005057552480138838\n",
      "Epoch 1/1 | Step 188000/839071 | Loss: 0.2956506609916687\n",
      "Epoch 1/1 | Step 189000/839071 | Loss: 0.0008645096677355468\n",
      "Epoch 1/1 | Step 190000/839071 | Loss: 0.029023094102740288\n",
      "Epoch 1/1 | Step 191000/839071 | Loss: 0.0006721848039887846\n",
      "Epoch 1/1 | Step 192000/839071 | Loss: 0.0003499286831356585\n",
      "Epoch 1/1 | Step 193000/839071 | Loss: 0.000440426665591076\n",
      "Epoch 1/1 | Step 194000/839071 | Loss: 0.000186100514838472\n",
      "Epoch 1/1 | Step 195000/839071 | Loss: 0.0001868709223344922\n",
      "Epoch 1/1 | Step 196000/839071 | Loss: 5.899013558519073e-05\n",
      "Epoch 1/1 | Step 197000/839071 | Loss: 0.0002330401912331581\n",
      "Epoch 1/1 | Step 198000/839071 | Loss: 0.00034357389085926116\n",
      "Epoch 1/1 | Step 199000/839071 | Loss: 0.604026198387146\n",
      "Epoch 1/1 | Step 200000/839071 | Loss: 0.00033914766390807927\n",
      "Epoch 1/1 | Step 201000/839071 | Loss: 0.004876762628555298\n",
      "Epoch 1/1 | Step 202000/839071 | Loss: 6.838121043983847e-05\n",
      "Epoch 1/1 | Step 203000/839071 | Loss: 0.0007121225935406983\n",
      "Epoch 1/1 | Step 204000/839071 | Loss: 0.3844394087791443\n",
      "Epoch 1/1 | Step 205000/839071 | Loss: 9.373955253977329e-05\n",
      "Epoch 1/1 | Step 206000/839071 | Loss: 0.235068678855896\n",
      "Epoch 1/1 | Step 207000/839071 | Loss: 0.000632425129879266\n",
      "Epoch 1/1 | Step 208000/839071 | Loss: 0.5705640912055969\n",
      "Epoch 1/1 | Step 209000/839071 | Loss: 0.0014332626014947891\n",
      "Epoch 1/1 | Step 210000/839071 | Loss: 0.000122285055113025\n",
      "Epoch 1/1 | Step 211000/839071 | Loss: 0.0006359231192618608\n",
      "Epoch 1/1 | Step 212000/839071 | Loss: 4.099065699847415e-05\n",
      "Epoch 1/1 | Step 213000/839071 | Loss: 0.0015530314994975924\n",
      "Epoch 1/1 | Step 214000/839071 | Loss: 0.0024258701596409082\n",
      "Epoch 1/1 | Step 215000/839071 | Loss: 0.00029996049124747515\n",
      "Epoch 1/1 | Step 216000/839071 | Loss: 0.003553957911208272\n",
      "Epoch 1/1 | Step 217000/839071 | Loss: 7.824649946996942e-05\n",
      "Epoch 1/1 | Step 218000/839071 | Loss: 1.3880029655410908e-05\n",
      "Epoch 1/1 | Step 219000/839071 | Loss: 0.017694871872663498\n",
      "Epoch 1/1 | Step 220000/839071 | Loss: 0.0006090563256293535\n",
      "Epoch 1/1 | Step 221000/839071 | Loss: 0.3655603528022766\n",
      "Epoch 1/1 | Step 222000/839071 | Loss: 0.0008726457599550486\n",
      "Epoch 1/1 | Step 223000/839071 | Loss: 2.5956775061786175e-05\n",
      "Epoch 1/1 | Step 224000/839071 | Loss: 0.004091533366590738\n",
      "Epoch 1/1 | Step 225000/839071 | Loss: 0.00013674733054358512\n",
      "Epoch 1/1 | Step 226000/839071 | Loss: 0.45557156205177307\n",
      "Epoch 1/1 | Step 227000/839071 | Loss: 0.5360769629478455\n",
      "Epoch 1/1 | Step 228000/839071 | Loss: 0.00013955723261460662\n",
      "Epoch 1/1 | Step 229000/839071 | Loss: 0.17346706986427307\n",
      "Epoch 1/1 | Step 230000/839071 | Loss: 0.18194781243801117\n",
      "Epoch 1/1 | Step 231000/839071 | Loss: 0.00020134804071858525\n",
      "Epoch 1/1 | Step 232000/839071 | Loss: 0.0006378669640980661\n",
      "Epoch 1/1 | Step 233000/839071 | Loss: 9.939302981365472e-05\n",
      "Epoch 1/1 | Step 234000/839071 | Loss: 0.0003969749668613076\n",
      "Epoch 1/1 | Step 235000/839071 | Loss: 0.00046827716869302094\n",
      "Epoch 1/1 | Step 236000/839071 | Loss: 0.005454580299556255\n",
      "Epoch 1/1 | Step 237000/839071 | Loss: 4.6511078835465014e-05\n",
      "Epoch 1/1 | Step 238000/839071 | Loss: 0.07606926560401917\n",
      "Epoch 1/1 | Step 239000/839071 | Loss: 0.0015806257724761963\n",
      "Epoch 1/1 | Step 240000/839071 | Loss: 0.00048124114982783794\n",
      "Epoch 1/1 | Step 241000/839071 | Loss: 8.328616240760311e-05\n",
      "Epoch 1/1 | Step 242000/839071 | Loss: 0.41754835844039917\n",
      "Epoch 1/1 | Step 243000/839071 | Loss: 0.00016979366773739457\n",
      "Epoch 1/1 | Step 244000/839071 | Loss: 8.425029955105856e-05\n",
      "Epoch 1/1 | Step 245000/839071 | Loss: 0.001974363112822175\n",
      "Epoch 1/1 | Step 246000/839071 | Loss: 0.020454656332731247\n",
      "Epoch 1/1 | Step 247000/839071 | Loss: 6.752837362000719e-05\n",
      "Epoch 1/1 | Step 248000/839071 | Loss: 0.0003842748992610723\n",
      "Epoch 1/1 | Step 249000/839071 | Loss: 0.0002116893301717937\n",
      "Epoch 1/1 | Step 250000/839071 | Loss: 0.0012499456061050296\n",
      "Epoch 1/1 | Step 251000/839071 | Loss: 0.00042462890269234776\n",
      "Epoch 1/1 | Step 252000/839071 | Loss: 0.0017926549771800637\n",
      "Epoch 1/1 | Step 253000/839071 | Loss: 0.014336535707116127\n",
      "Epoch 1/1 | Step 254000/839071 | Loss: 3.938882582588121e-05\n",
      "Epoch 1/1 | Step 255000/839071 | Loss: 6.999730976531282e-05\n",
      "Epoch 1/1 | Step 256000/839071 | Loss: 0.5696175694465637\n",
      "Epoch 1/1 | Step 257000/839071 | Loss: 0.0001148740338976495\n",
      "Epoch 1/1 | Step 258000/839071 | Loss: 0.00010913317237282172\n",
      "Epoch 1/1 | Step 259000/839071 | Loss: 0.00011986925528617576\n",
      "Epoch 1/1 | Step 260000/839071 | Loss: 0.0005513033829629421\n",
      "Epoch 1/1 | Step 261000/839071 | Loss: 0.0002284533838974312\n",
      "Epoch 1/1 | Step 262000/839071 | Loss: 8.433729817625135e-05\n",
      "Epoch 1/1 | Step 263000/839071 | Loss: 0.1337151974439621\n",
      "Epoch 1/1 | Step 264000/839071 | Loss: 0.5963491201400757\n",
      "Epoch 1/1 | Step 265000/839071 | Loss: 8.98726430023089e-05\n",
      "Epoch 1/1 | Step 266000/839071 | Loss: 0.00010884777293540537\n",
      "Epoch 1/1 | Step 267000/839071 | Loss: 0.0002096831303788349\n",
      "Epoch 1/1 | Step 268000/839071 | Loss: 9.230856085196137e-05\n",
      "Epoch 1/1 | Step 269000/839071 | Loss: 0.00012861372670158744\n",
      "Epoch 1/1 | Step 270000/839071 | Loss: 0.0026545836590230465\n",
      "Epoch 1/1 | Step 271000/839071 | Loss: 0.0004122937098145485\n",
      "Epoch 1/1 | Step 272000/839071 | Loss: 0.0013231065822765231\n",
      "Epoch 1/1 | Step 273000/839071 | Loss: 0.0001799835154088214\n",
      "Epoch 1/1 | Step 274000/839071 | Loss: 0.0017292966367676854\n",
      "Epoch 1/1 | Step 275000/839071 | Loss: 0.0002492663625162095\n",
      "Epoch 1/1 | Step 276000/839071 | Loss: 0.00012168006651336327\n",
      "Epoch 1/1 | Step 277000/839071 | Loss: 0.00011016803910024464\n",
      "Epoch 1/1 | Step 278000/839071 | Loss: 3.642365845735185e-05\n",
      "Epoch 1/1 | Step 279000/839071 | Loss: 0.010530244559049606\n",
      "Epoch 1/1 | Step 280000/839071 | Loss: 7.99388435552828e-05\n",
      "Epoch 1/1 | Step 281000/839071 | Loss: 0.00010676792589947581\n",
      "Epoch 1/1 | Step 282000/839071 | Loss: 0.00023116116062738\n",
      "Epoch 1/1 | Step 283000/839071 | Loss: 0.03641599789261818\n",
      "Epoch 1/1 | Step 284000/839071 | Loss: 0.00010846761870197952\n",
      "Epoch 1/1 | Step 285000/839071 | Loss: 7.978293433552608e-05\n",
      "Epoch 1/1 | Step 286000/839071 | Loss: 0.0006048806244507432\n",
      "Epoch 1/1 | Step 287000/839071 | Loss: 0.0013643766287714243\n",
      "Epoch 1/1 | Step 288000/839071 | Loss: 0.019850118085741997\n",
      "Epoch 1/1 | Step 289000/839071 | Loss: 0.000617680954746902\n",
      "Epoch 1/1 | Step 290000/839071 | Loss: 0.5341078639030457\n",
      "Epoch 1/1 | Step 291000/839071 | Loss: 6.770873005734757e-05\n",
      "Epoch 1/1 | Step 292000/839071 | Loss: 4.2331779695814475e-05\n",
      "Epoch 1/1 | Step 293000/839071 | Loss: 3.435238977544941e-05\n",
      "Epoch 1/1 | Step 294000/839071 | Loss: 0.0004629185132216662\n",
      "Epoch 1/1 | Step 295000/839071 | Loss: 0.0005496169906109571\n",
      "Epoch 1/1 | Step 296000/839071 | Loss: 0.00013559652143158019\n",
      "Epoch 1/1 | Step 297000/839071 | Loss: 0.0006842959555797279\n",
      "Epoch 1/1 | Step 298000/839071 | Loss: 0.0006357263191603124\n",
      "Epoch 1/1 | Step 299000/839071 | Loss: 0.0015333426417782903\n",
      "Epoch 1/1 | Step 300000/839071 | Loss: 0.00026177187100984156\n",
      "Epoch 1/1 | Step 301000/839071 | Loss: 0.0023230721708387136\n",
      "Epoch 1/1 | Step 302000/839071 | Loss: 0.0022867191582918167\n",
      "Epoch 1/1 | Step 303000/839071 | Loss: 4.4155880459584296e-05\n",
      "Epoch 1/1 | Step 304000/839071 | Loss: 0.006998821161687374\n",
      "Epoch 1/1 | Step 305000/839071 | Loss: 0.00214576069265604\n",
      "Epoch 1/1 | Step 306000/839071 | Loss: 0.00024236712488345802\n",
      "Epoch 1/1 | Step 307000/839071 | Loss: 0.4098827838897705\n",
      "Epoch 1/1 | Step 308000/839071 | Loss: 0.01323366817086935\n",
      "Epoch 1/1 | Step 309000/839071 | Loss: 0.0014666304923593998\n",
      "Epoch 1/1 | Step 310000/839071 | Loss: 0.002573588630184531\n",
      "Epoch 1/1 | Step 311000/839071 | Loss: 0.416049599647522\n",
      "Epoch 1/1 | Step 312000/839071 | Loss: 0.32861068844795227\n",
      "Epoch 1/1 | Step 313000/839071 | Loss: 0.00015385235019493848\n",
      "Epoch 1/1 | Step 314000/839071 | Loss: 4.7275978431571275e-05\n",
      "Epoch 1/1 | Step 315000/839071 | Loss: 0.6698313355445862\n",
      "Epoch 1/1 | Step 316000/839071 | Loss: 6.80762532283552e-05\n",
      "Epoch 1/1 | Step 317000/839071 | Loss: 0.0027428159955888987\n",
      "Epoch 1/1 | Step 318000/839071 | Loss: 0.00016235382645390928\n",
      "Epoch 1/1 | Step 319000/839071 | Loss: 2.9047587304376066e-05\n",
      "Epoch 1/1 | Step 320000/839071 | Loss: 9.058224532054737e-05\n",
      "Epoch 1/1 | Step 321000/839071 | Loss: 6.160357588669285e-05\n",
      "Epoch 1/1 | Step 322000/839071 | Loss: 0.11437566578388214\n",
      "Epoch 1/1 | Step 323000/839071 | Loss: 0.0012494524708017707\n",
      "Epoch 1/1 | Step 324000/839071 | Loss: 0.0011290980037301779\n",
      "Epoch 1/1 | Step 325000/839071 | Loss: 3.877616472891532e-05\n",
      "Epoch 1/1 | Step 326000/839071 | Loss: 0.0003465529880486429\n",
      "Epoch 1/1 | Step 327000/839071 | Loss: 0.00012047016207361594\n",
      "Epoch 1/1 | Step 328000/839071 | Loss: 0.005872257053852081\n",
      "Epoch 1/1 | Step 329000/839071 | Loss: 3.2363364880438894e-05\n",
      "Epoch 1/1 | Step 330000/839071 | Loss: 0.4505484998226166\n",
      "Epoch 1/1 | Step 331000/839071 | Loss: 6.857689004391432e-05\n",
      "Epoch 1/1 | Step 332000/839071 | Loss: 0.001707511837594211\n",
      "Epoch 1/1 | Step 333000/839071 | Loss: 0.0010564106050878763\n",
      "Epoch 1/1 | Step 334000/839071 | Loss: 0.000281004817225039\n",
      "Epoch 1/1 | Step 335000/839071 | Loss: 4.9915102863451466e-05\n",
      "Epoch 1/1 | Step 336000/839071 | Loss: 0.00010950886644423008\n",
      "Epoch 1/1 | Step 337000/839071 | Loss: 0.06312438100576401\n",
      "Epoch 1/1 | Step 338000/839071 | Loss: 0.00019704624719452113\n",
      "Epoch 1/1 | Step 339000/839071 | Loss: 3.868663407047279e-05\n",
      "Epoch 1/1 | Step 340000/839071 | Loss: 0.0007654243963770568\n",
      "Epoch 1/1 | Step 341000/839071 | Loss: 0.011763697490096092\n",
      "Epoch 1/1 | Step 342000/839071 | Loss: 0.00022387666103895754\n",
      "Epoch 1/1 | Step 343000/839071 | Loss: 2.9019049179623835e-05\n",
      "Epoch 1/1 | Step 344000/839071 | Loss: 0.00012341105320956558\n",
      "Epoch 1/1 | Step 345000/839071 | Loss: 0.00026480189990252256\n",
      "Epoch 1/1 | Step 346000/839071 | Loss: 0.2171148657798767\n",
      "Epoch 1/1 | Step 347000/839071 | Loss: 0.40925899147987366\n",
      "Epoch 1/1 | Step 348000/839071 | Loss: 0.004743896424770355\n",
      "Epoch 1/1 | Step 349000/839071 | Loss: 8.406762935919687e-05\n",
      "Epoch 1/1 | Step 350000/839071 | Loss: 0.0010700818384066224\n",
      "Epoch 1/1 | Step 351000/839071 | Loss: 7.768704381305724e-05\n",
      "Epoch 1/1 | Step 352000/839071 | Loss: 0.025919459760189056\n",
      "Epoch 1/1 | Step 353000/839071 | Loss: 0.0056462036445736885\n",
      "Epoch 1/1 | Step 354000/839071 | Loss: 0.0006391445640474558\n",
      "Epoch 1/1 | Step 355000/839071 | Loss: 0.0001426288072252646\n",
      "Epoch 1/1 | Step 356000/839071 | Loss: 0.37390100955963135\n",
      "Epoch 1/1 | Step 357000/839071 | Loss: 6.576609303010628e-05\n",
      "Epoch 1/1 | Step 358000/839071 | Loss: 0.0003383143339306116\n",
      "Epoch 1/1 | Step 359000/839071 | Loss: 6.061421299818903e-05\n",
      "Epoch 1/1 | Step 360000/839071 | Loss: 0.0026047725696116686\n",
      "Epoch 1/1 | Step 361000/839071 | Loss: 4.2850904719671234e-05\n",
      "Epoch 1/1 | Step 362000/839071 | Loss: 5.1871164032490924e-05\n",
      "Epoch 1/1 | Step 363000/839071 | Loss: 8.489866741001606e-05\n",
      "Epoch 1/1 | Step 364000/839071 | Loss: 0.0003258713986724615\n",
      "Epoch 1/1 | Step 365000/839071 | Loss: 0.0041615841910243034\n",
      "Epoch 1/1 | Step 366000/839071 | Loss: 8.407078712480143e-05\n",
      "Epoch 1/1 | Step 367000/839071 | Loss: 0.00041650410275906324\n",
      "Epoch 1/1 | Step 368000/839071 | Loss: 0.3548887372016907\n",
      "Epoch 1/1 | Step 369000/839071 | Loss: 4.123552571400069e-05\n",
      "Epoch 1/1 | Step 370000/839071 | Loss: 0.33599600195884705\n",
      "Epoch 1/1 | Step 371000/839071 | Loss: 3.0054206945351325e-05\n",
      "Epoch 1/1 | Step 372000/839071 | Loss: 3.8591573684243485e-05\n",
      "Epoch 1/1 | Step 373000/839071 | Loss: 0.03462705388665199\n",
      "Epoch 1/1 | Step 374000/839071 | Loss: 0.0011398213682696223\n",
      "Epoch 1/1 | Step 375000/839071 | Loss: 0.0020024534314870834\n",
      "Epoch 1/1 | Step 376000/839071 | Loss: 0.00039178429869934916\n",
      "Epoch 1/1 | Step 377000/839071 | Loss: 0.00016794144175946712\n",
      "Epoch 1/1 | Step 378000/839071 | Loss: 0.0001154519704869017\n",
      "Epoch 1/1 | Step 379000/839071 | Loss: 3.556762385414913e-05\n",
      "Epoch 1/1 | Step 380000/839071 | Loss: 7.706817268626764e-05\n",
      "Epoch 1/1 | Step 381000/839071 | Loss: 0.007954179309308529\n",
      "Epoch 1/1 | Step 382000/839071 | Loss: 1.43942470458569e-05\n",
      "Epoch 1/1 | Step 383000/839071 | Loss: 8.417954813921824e-05\n",
      "Epoch 1/1 | Step 384000/839071 | Loss: 6.192371074575931e-05\n",
      "Epoch 1/1 | Step 385000/839071 | Loss: 0.00022038996394257993\n",
      "Epoch 1/1 | Step 386000/839071 | Loss: 0.42842334508895874\n",
      "Epoch 1/1 | Step 387000/839071 | Loss: 0.00027620530454441905\n",
      "Epoch 1/1 | Step 388000/839071 | Loss: 0.003944030962884426\n",
      "Epoch 1/1 | Step 389000/839071 | Loss: 4.078988786204718e-05\n",
      "Epoch 1/1 | Step 390000/839071 | Loss: 0.00047485766117461026\n",
      "Epoch 1/1 | Step 391000/839071 | Loss: 0.002330116694793105\n",
      "Epoch 1/1 | Step 392000/839071 | Loss: 0.0007371528772637248\n",
      "Epoch 1/1 | Step 393000/839071 | Loss: 7.652226486243308e-05\n",
      "Epoch 1/1 | Step 394000/839071 | Loss: 7.23905541235581e-05\n",
      "Epoch 1/1 | Step 395000/839071 | Loss: 0.0003975695581175387\n",
      "Epoch 1/1 | Step 396000/839071 | Loss: 0.0031659018713980913\n",
      "Epoch 1/1 | Step 397000/839071 | Loss: 0.0003093298291787505\n",
      "Epoch 1/1 | Step 398000/839071 | Loss: 0.002450319239869714\n",
      "Epoch 1/1 | Step 399000/839071 | Loss: 3.550067776814103e-05\n",
      "Epoch 1/1 | Step 400000/839071 | Loss: 0.0003986240772064775\n",
      "Epoch 1/1 | Step 401000/839071 | Loss: 0.00014315411681309342\n",
      "Epoch 1/1 | Step 402000/839071 | Loss: 0.584424614906311\n",
      "Epoch 1/1 | Step 403000/839071 | Loss: 0.0010629311436787248\n",
      "Epoch 1/1 | Step 404000/839071 | Loss: 0.00026184131274931133\n",
      "Epoch 1/1 | Step 405000/839071 | Loss: 8.108336623990908e-05\n",
      "Epoch 1/1 | Step 406000/839071 | Loss: 2.8728620236506686e-05\n",
      "Epoch 1/1 | Step 407000/839071 | Loss: 3.612606815295294e-05\n",
      "Epoch 1/1 | Step 408000/839071 | Loss: 4.1988532757386565e-05\n",
      "Epoch 1/1 | Step 409000/839071 | Loss: 0.0010075356112793088\n",
      "Epoch 1/1 | Step 410000/839071 | Loss: 2.1821608243044466e-05\n",
      "Epoch 1/1 | Step 411000/839071 | Loss: 0.00012701576633844525\n",
      "Epoch 1/1 | Step 412000/839071 | Loss: 2.9764138162136078e-05\n",
      "Epoch 1/1 | Step 413000/839071 | Loss: 8.173845708370209e-05\n",
      "Epoch 1/1 | Step 414000/839071 | Loss: 3.0418785172514617e-05\n",
      "Epoch 1/1 | Step 415000/839071 | Loss: 0.00021167160593904555\n",
      "Epoch 1/1 | Step 416000/839071 | Loss: 5.830443114973605e-05\n",
      "Epoch 1/1 | Step 417000/839071 | Loss: 0.0002927037130575627\n",
      "Epoch 1/1 | Step 418000/839071 | Loss: 0.0004914146265946329\n",
      "Epoch 1/1 | Step 419000/839071 | Loss: 8.65385663928464e-05\n",
      "Epoch 1/1 | Step 420000/839071 | Loss: 0.00029168243054300547\n",
      "Epoch 1/1 | Step 421000/839071 | Loss: 0.006565646268427372\n",
      "Epoch 1/1 | Step 422000/839071 | Loss: 0.34744417667388916\n",
      "Epoch 1/1 | Step 423000/839071 | Loss: 0.0003974901628680527\n",
      "Epoch 1/1 | Step 424000/839071 | Loss: 0.1826694905757904\n",
      "Epoch 1/1 | Step 425000/839071 | Loss: 4.272664591553621e-05\n",
      "Epoch 1/1 | Step 426000/839071 | Loss: 0.00010968300921376795\n",
      "Epoch 1/1 | Step 427000/839071 | Loss: 0.0026466248091310263\n",
      "Epoch 1/1 | Step 428000/839071 | Loss: 2.5919800464180298e-05\n",
      "Epoch 1/1 | Step 429000/839071 | Loss: 5.404829062172212e-05\n",
      "Epoch 1/1 | Step 430000/839071 | Loss: 0.00011346633255016059\n",
      "Epoch 1/1 | Step 431000/839071 | Loss: 3.5447683330858126e-05\n",
      "Epoch 1/1 | Step 432000/839071 | Loss: 6.077045691199601e-05\n",
      "Epoch 1/1 | Step 433000/839071 | Loss: 0.00014011452731210738\n",
      "Epoch 1/1 | Step 434000/839071 | Loss: 8.22280126158148e-05\n",
      "Epoch 1/1 | Step 435000/839071 | Loss: 0.5029327869415283\n",
      "Epoch 1/1 | Step 436000/839071 | Loss: 9.47555890888907e-05\n",
      "Epoch 1/1 | Step 437000/839071 | Loss: 0.6559092998504639\n",
      "Epoch 1/1 | Step 438000/839071 | Loss: 8.652442193124443e-05\n",
      "Epoch 1/1 | Step 439000/839071 | Loss: 0.00015158993483055383\n",
      "Epoch 1/1 | Step 440000/839071 | Loss: 5.0554262998048216e-05\n",
      "Epoch 1/1 | Step 441000/839071 | Loss: 3.865904363919981e-05\n",
      "Epoch 1/1 | Step 442000/839071 | Loss: 7.134917541407049e-05\n",
      "Epoch 1/1 | Step 443000/839071 | Loss: 0.6013258695602417\n",
      "Epoch 1/1 | Step 444000/839071 | Loss: 3.898285285686143e-05\n",
      "Epoch 1/1 | Step 445000/839071 | Loss: 0.00018825882580131292\n",
      "Epoch 1/1 | Step 446000/839071 | Loss: 9.625696111470461e-05\n",
      "Epoch 1/1 | Step 447000/839071 | Loss: 0.21979117393493652\n",
      "Epoch 1/1 | Step 448000/839071 | Loss: 8.661571337142959e-05\n",
      "Epoch 1/1 | Step 449000/839071 | Loss: 9.134949505096301e-05\n",
      "Epoch 1/1 | Step 450000/839071 | Loss: 0.00012318805966060609\n",
      "Epoch 1/1 | Step 451000/839071 | Loss: 0.00019015735597349703\n",
      "Epoch 1/1 | Step 452000/839071 | Loss: 2.5650870156823657e-05\n",
      "Epoch 1/1 | Step 453000/839071 | Loss: 4.449891275726259e-05\n",
      "Epoch 1/1 | Step 454000/839071 | Loss: 1.7486152501078323e-05\n",
      "Epoch 1/1 | Step 455000/839071 | Loss: 0.0006226706318557262\n",
      "Epoch 1/1 | Step 456000/839071 | Loss: 4.050431016366929e-05\n",
      "Epoch 1/1 | Step 457000/839071 | Loss: 8.893062477000058e-05\n",
      "Epoch 1/1 | Step 458000/839071 | Loss: 4.200389957986772e-05\n",
      "Epoch 1/1 | Step 459000/839071 | Loss: 0.00018393275968264788\n",
      "Epoch 1/1 | Step 460000/839071 | Loss: 0.016915790736675262\n",
      "Epoch 1/1 | Step 461000/839071 | Loss: 0.00011376834299881011\n",
      "Epoch 1/1 | Step 462000/839071 | Loss: 8.813240856397897e-05\n",
      "Epoch 1/1 | Step 463000/839071 | Loss: 0.00020183958986308426\n",
      "Epoch 1/1 | Step 464000/839071 | Loss: 0.026736769825220108\n",
      "Epoch 1/1 | Step 465000/839071 | Loss: 4.120693483855575e-05\n",
      "Epoch 1/1 | Step 466000/839071 | Loss: 5.5159205658128485e-05\n",
      "Epoch 1/1 | Step 467000/839071 | Loss: 4.906498361378908e-05\n",
      "Epoch 1/1 | Step 468000/839071 | Loss: 0.0009601147612556815\n",
      "Epoch 1/1 | Step 469000/839071 | Loss: 0.0002893524069804698\n",
      "Epoch 1/1 | Step 470000/839071 | Loss: 6.87883366481401e-05\n",
      "Epoch 1/1 | Step 471000/839071 | Loss: 5.494118158821948e-05\n",
      "Epoch 1/1 | Step 472000/839071 | Loss: 3.504643609630875e-05\n",
      "Epoch 1/1 | Step 473000/839071 | Loss: 5.584565951721743e-05\n",
      "Epoch 1/1 | Step 474000/839071 | Loss: 1.9616900317487307e-05\n",
      "Epoch 1/1 | Step 475000/839071 | Loss: 0.00020950201724190265\n",
      "Epoch 1/1 | Step 476000/839071 | Loss: 0.0004418629687279463\n",
      "Epoch 1/1 | Step 477000/839071 | Loss: 0.07900213450193405\n",
      "Epoch 1/1 | Step 478000/839071 | Loss: 2.6865745894610882e-05\n",
      "Epoch 1/1 | Step 479000/839071 | Loss: 0.00013082557416055351\n",
      "Epoch 1/1 | Step 480000/839071 | Loss: 0.0003390167548786849\n",
      "Epoch 1/1 | Step 481000/839071 | Loss: 4.610595715348609e-05\n",
      "Epoch 1/1 | Step 482000/839071 | Loss: 7.257077231770381e-05\n",
      "Epoch 1/1 | Step 483000/839071 | Loss: 0.0022424976341426373\n",
      "Epoch 1/1 | Step 484000/839071 | Loss: 0.0003571174165699631\n",
      "Epoch 1/1 | Step 485000/839071 | Loss: 0.0009569789399392903\n",
      "Epoch 1/1 | Step 486000/839071 | Loss: 0.6631491184234619\n",
      "Epoch 1/1 | Step 487000/839071 | Loss: 0.00018574816931504756\n",
      "Epoch 1/1 | Step 488000/839071 | Loss: 0.0006935535930097103\n",
      "Epoch 1/1 | Step 489000/839071 | Loss: 0.0003013530804309994\n",
      "Epoch 1/1 | Step 490000/839071 | Loss: 0.002658768091350794\n",
      "Epoch 1/1 | Step 491000/839071 | Loss: 6.364917499013245e-05\n",
      "Epoch 1/1 | Step 492000/839071 | Loss: 0.0008839024812914431\n",
      "Epoch 1/1 | Step 493000/839071 | Loss: 0.00021820366964675486\n",
      "Epoch 1/1 | Step 494000/839071 | Loss: 0.004236363340169191\n",
      "Epoch 1/1 | Step 495000/839071 | Loss: 1.77616675500758e-05\n",
      "Epoch 1/1 | Step 496000/839071 | Loss: 0.00010601006943034008\n",
      "Epoch 1/1 | Step 497000/839071 | Loss: 5.528003384824842e-05\n",
      "Epoch 1/1 | Step 498000/839071 | Loss: 6.497930735349655e-05\n",
      "Epoch 1/1 | Step 499000/839071 | Loss: 0.0001950853766174987\n",
      "Epoch 1/1 | Step 500000/839071 | Loss: 0.00010714100790210068\n",
      "Epoch 1/1 | Step 501000/839071 | Loss: 0.003449914278462529\n",
      "Epoch 1/1 | Step 502000/839071 | Loss: 0.0003658074128907174\n",
      "Epoch 1/1 | Step 503000/839071 | Loss: 0.0007196645601652563\n",
      "Epoch 1/1 | Step 504000/839071 | Loss: 0.4195890724658966\n",
      "Epoch 1/1 | Step 505000/839071 | Loss: 0.0005561714060604572\n",
      "Epoch 1/1 | Step 506000/839071 | Loss: 0.0866021141409874\n",
      "Epoch 1/1 | Step 507000/839071 | Loss: 9.322303958470002e-05\n",
      "Epoch 1/1 | Step 508000/839071 | Loss: 0.0013775129336863756\n",
      "Epoch 1/1 | Step 509000/839071 | Loss: 8.736013842280954e-05\n",
      "Epoch 1/1 | Step 510000/839071 | Loss: 0.06928759068250656\n",
      "Epoch 1/1 | Step 511000/839071 | Loss: 0.001216917298734188\n",
      "Epoch 1/1 | Step 512000/839071 | Loss: 2.2476931917481124e-05\n",
      "Epoch 1/1 | Step 513000/839071 | Loss: 0.014823630452156067\n",
      "Epoch 1/1 | Step 514000/839071 | Loss: 0.00014546191960107535\n",
      "Epoch 1/1 | Step 515000/839071 | Loss: 1.604082717676647e-05\n",
      "Epoch 1/1 | Step 516000/839071 | Loss: 0.012391790747642517\n",
      "Epoch 1/1 | Step 517000/839071 | Loss: 2.3669106667512096e-05\n",
      "Epoch 1/1 | Step 518000/839071 | Loss: 8.55130419950001e-05\n",
      "Epoch 1/1 | Step 519000/839071 | Loss: 1.102674377762014e-05\n",
      "Epoch 1/1 | Step 520000/839071 | Loss: 0.00010727407061494887\n",
      "Epoch 1/1 | Step 521000/839071 | Loss: 0.00018447023467160761\n",
      "Epoch 1/1 | Step 522000/839071 | Loss: 0.0003548327076714486\n",
      "Epoch 1/1 | Step 523000/839071 | Loss: 0.5780618786811829\n",
      "Epoch 1/1 | Step 524000/839071 | Loss: 1.3381102689891122e-05\n",
      "Epoch 1/1 | Step 525000/839071 | Loss: 0.00011316729069221765\n",
      "Epoch 1/1 | Step 526000/839071 | Loss: 0.5653465986251831\n",
      "Epoch 1/1 | Step 527000/839071 | Loss: 4.874079240835272e-05\n",
      "Epoch 1/1 | Step 528000/839071 | Loss: 2.93613502435619e-05\n",
      "Epoch 1/1 | Step 529000/839071 | Loss: 0.00028557691257447004\n",
      "Epoch 1/1 | Step 530000/839071 | Loss: 0.00011839134822366759\n",
      "Epoch 1/1 | Step 531000/839071 | Loss: 0.00010189379099756479\n",
      "Epoch 1/1 | Step 532000/839071 | Loss: 0.00011653077672235668\n",
      "Epoch 1/1 | Step 533000/839071 | Loss: 0.000558144529350102\n",
      "Epoch 1/1 | Step 534000/839071 | Loss: 0.0001787817309377715\n",
      "Epoch 1/1 | Step 535000/839071 | Loss: 0.00023826261167414486\n",
      "Epoch 1/1 | Step 536000/839071 | Loss: 5.98752849327866e-05\n",
      "Epoch 1/1 | Step 537000/839071 | Loss: 0.00010887414828175679\n",
      "Epoch 1/1 | Step 538000/839071 | Loss: 1.894510569400154e-05\n",
      "Epoch 1/1 | Step 539000/839071 | Loss: 0.0017584245651960373\n",
      "Epoch 1/1 | Step 540000/839071 | Loss: 0.00023252985556609929\n",
      "Epoch 1/1 | Step 541000/839071 | Loss: 0.0001621183764655143\n",
      "Epoch 1/1 | Step 542000/839071 | Loss: 0.00010565985576249659\n",
      "Epoch 1/1 | Step 543000/839071 | Loss: 3.257840216974728e-05\n",
      "Epoch 1/1 | Step 544000/839071 | Loss: 0.00037643438554368913\n",
      "Epoch 1/1 | Step 545000/839071 | Loss: 6.370196933858097e-05\n",
      "Epoch 1/1 | Step 546000/839071 | Loss: 7.356461719609797e-05\n",
      "Epoch 1/1 | Step 547000/839071 | Loss: 0.00022707688913214952\n",
      "Epoch 1/1 | Step 548000/839071 | Loss: 4.358077421784401e-05\n",
      "Epoch 1/1 | Step 549000/839071 | Loss: 8.332854486070573e-05\n",
      "Epoch 1/1 | Step 550000/839071 | Loss: 0.040556401014328\n",
      "Epoch 1/1 | Step 551000/839071 | Loss: 0.0002790678117889911\n",
      "Epoch 1/1 | Step 552000/839071 | Loss: 0.0016903809737414122\n",
      "Epoch 1/1 | Step 553000/839071 | Loss: 8.119658741634339e-05\n",
      "Epoch 1/1 | Step 554000/839071 | Loss: 0.0028290736954659224\n",
      "Epoch 1/1 | Step 555000/839071 | Loss: 0.00022583398094866425\n",
      "Epoch 1/1 | Step 556000/839071 | Loss: 0.31834831833839417\n",
      "Epoch 1/1 | Step 557000/839071 | Loss: 6.028557982062921e-05\n",
      "Epoch 1/1 | Step 558000/839071 | Loss: 0.0007196452934294939\n",
      "Epoch 1/1 | Step 559000/839071 | Loss: 0.00020474527264013886\n",
      "Epoch 1/1 | Step 560000/839071 | Loss: 0.46636125445365906\n",
      "Epoch 1/1 | Step 561000/839071 | Loss: 0.0013944290112704039\n",
      "Epoch 1/1 | Step 562000/839071 | Loss: 0.003941051661968231\n",
      "Epoch 1/1 | Step 563000/839071 | Loss: 5.922616401221603e-05\n",
      "Epoch 1/1 | Step 564000/839071 | Loss: 0.0001502176746726036\n",
      "Epoch 1/1 | Step 565000/839071 | Loss: 0.0010549315484240651\n",
      "Epoch 1/1 | Step 566000/839071 | Loss: 5.903891724301502e-05\n",
      "Epoch 1/1 | Step 567000/839071 | Loss: 9.678222340880893e-06\n",
      "Epoch 1/1 | Step 568000/839071 | Loss: 0.0019099402707070112\n",
      "Epoch 1/1 | Step 569000/839071 | Loss: 6.0374375607352704e-05\n",
      "Epoch 1/1 | Step 570000/839071 | Loss: 6.052248863852583e-05\n",
      "Epoch 1/1 | Step 571000/839071 | Loss: 0.0001594910427229479\n",
      "Epoch 1/1 | Step 572000/839071 | Loss: 0.004033525008708239\n",
      "Epoch 1/1 | Step 573000/839071 | Loss: 0.0003417730040382594\n",
      "Epoch 1/1 | Step 574000/839071 | Loss: 0.0005828523426316679\n",
      "Epoch 1/1 | Step 575000/839071 | Loss: 5.4076601372798905e-05\n",
      "Epoch 1/1 | Step 576000/839071 | Loss: 8.397526835324243e-05\n",
      "Epoch 1/1 | Step 577000/839071 | Loss: 0.00034967070678249\n",
      "Epoch 1/1 | Step 578000/839071 | Loss: 5.631991371046752e-05\n",
      "Epoch 1/1 | Step 579000/839071 | Loss: 0.001992615172639489\n",
      "Epoch 1/1 | Step 580000/839071 | Loss: 9.283564577344805e-05\n",
      "Epoch 1/1 | Step 581000/839071 | Loss: 0.0029013678431510925\n",
      "Epoch 1/1 | Step 582000/839071 | Loss: 0.00012081431486876681\n",
      "Epoch 1/1 | Step 583000/839071 | Loss: 0.00015171337872743607\n",
      "Epoch 1/1 | Step 584000/839071 | Loss: 0.03189637139439583\n",
      "Epoch 1/1 | Step 585000/839071 | Loss: 5.6122000387404114e-05\n",
      "Epoch 1/1 | Step 586000/839071 | Loss: 8.339453052030876e-05\n",
      "Epoch 1/1 | Step 587000/839071 | Loss: 0.0003475146659184247\n",
      "Epoch 1/1 | Step 588000/839071 | Loss: 0.00010826718789758161\n",
      "Epoch 1/1 | Step 589000/839071 | Loss: 7.730378274573013e-05\n",
      "Epoch 1/1 | Step 590000/839071 | Loss: 0.4852137565612793\n",
      "Epoch 1/1 | Step 591000/839071 | Loss: 0.36934754252433777\n",
      "Epoch 1/1 | Step 592000/839071 | Loss: 0.0006234642351046205\n",
      "Epoch 1/1 | Step 593000/839071 | Loss: 6.213738743099384e-06\n",
      "Epoch 1/1 | Step 594000/839071 | Loss: 0.00024411012418568134\n",
      "Epoch 1/1 | Step 595000/839071 | Loss: 8.968815382104367e-05\n",
      "Epoch 1/1 | Step 596000/839071 | Loss: 3.3055133826565e-05\n",
      "Epoch 1/1 | Step 597000/839071 | Loss: 5.2960946050006896e-05\n",
      "Epoch 1/1 | Step 598000/839071 | Loss: 0.0001396786974510178\n",
      "Epoch 1/1 | Step 599000/839071 | Loss: 4.048324626637623e-05\n",
      "Epoch 1/1 | Step 600000/839071 | Loss: 0.0002293277793796733\n",
      "Epoch 1/1 | Step 601000/839071 | Loss: 1.8760205421131104e-05\n",
      "Epoch 1/1 | Step 602000/839071 | Loss: 6.546842632815242e-05\n",
      "Epoch 1/1 | Step 603000/839071 | Loss: 0.3198094964027405\n",
      "Epoch 1/1 | Step 604000/839071 | Loss: 0.0014647040516138077\n",
      "Epoch 1/1 | Step 605000/839071 | Loss: 5.2910723752574995e-05\n",
      "Epoch 1/1 | Step 606000/839071 | Loss: 0.00018053725943900645\n",
      "Epoch 1/1 | Step 607000/839071 | Loss: 5.3147257858654484e-05\n",
      "Epoch 1/1 | Step 608000/839071 | Loss: 0.0013026987435296178\n",
      "Epoch 1/1 | Step 609000/839071 | Loss: 0.00015928530774544924\n",
      "Epoch 1/1 | Step 610000/839071 | Loss: 4.945115142618306e-05\n",
      "Epoch 1/1 | Step 611000/839071 | Loss: 0.0008936935919336975\n",
      "Epoch 1/1 | Step 612000/839071 | Loss: 0.000238755761529319\n",
      "Epoch 1/1 | Step 613000/839071 | Loss: 0.00010483899677637964\n",
      "Epoch 1/1 | Step 614000/839071 | Loss: 2.6411073122289963e-05\n",
      "Epoch 1/1 | Step 615000/839071 | Loss: 0.0001976072380784899\n",
      "Epoch 1/1 | Step 616000/839071 | Loss: 0.0001692584773991257\n",
      "Epoch 1/1 | Step 617000/839071 | Loss: 0.0001456506724935025\n",
      "Epoch 1/1 | Step 618000/839071 | Loss: 0.5909259915351868\n",
      "Epoch 1/1 | Step 619000/839071 | Loss: 0.0003046690544579178\n",
      "Epoch 1/1 | Step 620000/839071 | Loss: 0.000667616433929652\n",
      "Epoch 1/1 | Step 621000/839071 | Loss: 0.0004222110437694937\n",
      "Epoch 1/1 | Step 622000/839071 | Loss: 4.44832730863709e-05\n",
      "Epoch 1/1 | Step 623000/839071 | Loss: 0.3129884600639343\n",
      "Epoch 1/1 | Step 624000/839071 | Loss: 1.2106894246244337e-05\n",
      "Epoch 1/1 | Step 625000/839071 | Loss: 0.0003065876953769475\n",
      "Epoch 1/1 | Step 626000/839071 | Loss: 4.034538142150268e-05\n",
      "Epoch 1/1 | Step 627000/839071 | Loss: 0.00011690155224641785\n",
      "Epoch 1/1 | Step 628000/839071 | Loss: 0.00018609405378811061\n",
      "Epoch 1/1 | Step 629000/839071 | Loss: 0.0001255492097698152\n",
      "Epoch 1/1 | Step 630000/839071 | Loss: 4.656201781472191e-05\n",
      "Epoch 1/1 | Step 631000/839071 | Loss: 0.00012277771020308137\n",
      "Epoch 1/1 | Step 632000/839071 | Loss: 9.13829353521578e-05\n",
      "Epoch 1/1 | Step 633000/839071 | Loss: 0.0004039575287606567\n",
      "Epoch 1/1 | Step 634000/839071 | Loss: 0.00063683110056445\n",
      "Epoch 1/1 | Step 635000/839071 | Loss: 4.0676699427422136e-05\n",
      "Epoch 1/1 | Step 636000/839071 | Loss: 0.2099461704492569\n",
      "Epoch 1/1 | Step 637000/839071 | Loss: 0.002530643017962575\n",
      "Epoch 1/1 | Step 638000/839071 | Loss: 4.6748737076995894e-05\n",
      "Epoch 1/1 | Step 639000/839071 | Loss: 5.0804781494662166e-05\n",
      "Epoch 1/1 | Step 640000/839071 | Loss: 5.880572280148044e-05\n",
      "Epoch 1/1 | Step 641000/839071 | Loss: 7.265590102178976e-05\n",
      "Epoch 1/1 | Step 642000/839071 | Loss: 8.467519364785403e-05\n",
      "Epoch 1/1 | Step 643000/839071 | Loss: 9.413166117155924e-05\n",
      "Epoch 1/1 | Step 644000/839071 | Loss: 4.0854916733223945e-05\n",
      "Epoch 1/1 | Step 645000/839071 | Loss: 0.00017806168762035668\n",
      "Epoch 1/1 | Step 646000/839071 | Loss: 0.0010066009126603603\n",
      "Epoch 1/1 | Step 647000/839071 | Loss: 0.00011349125998094678\n",
      "Epoch 1/1 | Step 648000/839071 | Loss: 0.6041008830070496\n",
      "Epoch 1/1 | Step 649000/839071 | Loss: 2.7677750040311366e-05\n",
      "Epoch 1/1 | Step 650000/839071 | Loss: 5.877252624486573e-05\n",
      "Epoch 1/1 | Step 651000/839071 | Loss: 0.0004713171801995486\n",
      "Epoch 1/1 | Step 652000/839071 | Loss: 7.327338244067505e-05\n",
      "Epoch 1/1 | Step 653000/839071 | Loss: 0.6028050780296326\n",
      "Epoch 1/1 | Step 654000/839071 | Loss: 0.0005530822090804577\n",
      "Epoch 1/1 | Step 655000/839071 | Loss: 0.0003209706046618521\n",
      "Epoch 1/1 | Step 656000/839071 | Loss: 0.0003693201288115233\n",
      "Epoch 1/1 | Step 657000/839071 | Loss: 8.924490248318762e-05\n",
      "Epoch 1/1 | Step 658000/839071 | Loss: 7.506487600039691e-05\n",
      "Epoch 1/1 | Step 659000/839071 | Loss: 3.840259159915149e-05\n",
      "Epoch 1/1 | Step 660000/839071 | Loss: 0.652224063873291\n",
      "Epoch 1/1 | Step 661000/839071 | Loss: 0.00034412392415106297\n",
      "Epoch 1/1 | Step 662000/839071 | Loss: 0.0003958789457101375\n",
      "Epoch 1/1 | Step 663000/839071 | Loss: 5.7210170780308545e-05\n",
      "Epoch 1/1 | Step 664000/839071 | Loss: 0.0016914872685447335\n",
      "Epoch 1/1 | Step 665000/839071 | Loss: 0.0014008607249706984\n",
      "Epoch 1/1 | Step 666000/839071 | Loss: 1.6383430192945525e-05\n",
      "Epoch 1/1 | Step 667000/839071 | Loss: 0.0003602590295486152\n",
      "Epoch 1/1 | Step 668000/839071 | Loss: 0.00018198785255663097\n",
      "Epoch 1/1 | Step 669000/839071 | Loss: 4.3622545490507036e-05\n",
      "Epoch 1/1 | Step 670000/839071 | Loss: 1.4267546248447616e-05\n",
      "Epoch 1/1 | Step 671000/839071 | Loss: 0.49522873759269714\n",
      "Epoch 1/1 | Step 672000/839071 | Loss: 7.938063936308026e-05\n",
      "Epoch 1/1 | Step 673000/839071 | Loss: 0.12974824011325836\n",
      "Epoch 1/1 | Step 674000/839071 | Loss: 0.0061416300013661385\n",
      "Epoch 1/1 | Step 675000/839071 | Loss: 0.0007549645961262286\n",
      "Epoch 1/1 | Step 676000/839071 | Loss: 2.7871850761584938e-05\n",
      "Epoch 1/1 | Step 677000/839071 | Loss: 2.5100047423620708e-05\n",
      "Epoch 1/1 | Step 678000/839071 | Loss: 0.00015493997489102185\n",
      "Epoch 1/1 | Step 679000/839071 | Loss: 0.0010768333449959755\n",
      "Epoch 1/1 | Step 680000/839071 | Loss: 0.0003816329990513623\n",
      "Epoch 1/1 | Step 681000/839071 | Loss: 1.839509786805138e-05\n",
      "Epoch 1/1 | Step 682000/839071 | Loss: 0.00019916638848371804\n",
      "Epoch 1/1 | Step 683000/839071 | Loss: 9.804974251892418e-05\n",
      "Epoch 1/1 | Step 684000/839071 | Loss: 4.945946784573607e-05\n",
      "Epoch 1/1 | Step 685000/839071 | Loss: 1.862545104813762e-05\n",
      "Epoch 1/1 | Step 686000/839071 | Loss: 0.00011495158832985908\n",
      "Epoch 1/1 | Step 687000/839071 | Loss: 0.00017701738397590816\n",
      "Epoch 1/1 | Step 688000/839071 | Loss: 0.03721078857779503\n",
      "Epoch 1/1 | Step 689000/839071 | Loss: 0.00010883630602620542\n",
      "Epoch 1/1 | Step 690000/839071 | Loss: 0.43115654587745667\n",
      "Epoch 1/1 | Step 691000/839071 | Loss: 0.000348061032127589\n",
      "Epoch 1/1 | Step 692000/839071 | Loss: 0.00011686803918564692\n",
      "Epoch 1/1 | Step 693000/839071 | Loss: 0.00024322033277712762\n",
      "Epoch 1/1 | Step 694000/839071 | Loss: 0.000596058729570359\n",
      "Epoch 1/1 | Step 695000/839071 | Loss: 0.0001901706273201853\n",
      "Epoch 1/1 | Step 696000/839071 | Loss: 0.000341003731591627\n",
      "Epoch 1/1 | Step 697000/839071 | Loss: 0.0004458346520550549\n",
      "Epoch 1/1 | Step 698000/839071 | Loss: 0.049531303346157074\n",
      "Epoch 1/1 | Step 699000/839071 | Loss: 5.8458943385630846e-05\n",
      "Epoch 1/1 | Step 700000/839071 | Loss: 2.35565585171571e-05\n",
      "Epoch 1/1 | Step 701000/839071 | Loss: 0.0251168143004179\n",
      "Epoch 1/1 | Step 702000/839071 | Loss: 3.41249760822393e-05\n",
      "Epoch 1/1 | Step 703000/839071 | Loss: 1.3574730473919772e-05\n",
      "Epoch 1/1 | Step 704000/839071 | Loss: 0.001701431698165834\n",
      "Epoch 1/1 | Step 705000/839071 | Loss: 7.511967851314694e-05\n",
      "Epoch 1/1 | Step 706000/839071 | Loss: 3.0373859772225842e-05\n",
      "Epoch 1/1 | Step 707000/839071 | Loss: 3.509061207296327e-05\n",
      "Epoch 1/1 | Step 708000/839071 | Loss: 0.00020531848713289946\n",
      "Epoch 1/1 | Step 709000/839071 | Loss: 0.0023606896866112947\n",
      "Epoch 1/1 | Step 710000/839071 | Loss: 7.563647523056716e-05\n",
      "Epoch 1/1 | Step 711000/839071 | Loss: 0.3887489438056946\n",
      "Epoch 1/1 | Step 712000/839071 | Loss: 1.3761060472461395e-05\n",
      "Epoch 1/1 | Step 713000/839071 | Loss: 3.940334136132151e-05\n",
      "Epoch 1/1 | Step 714000/839071 | Loss: 1.5302770407288335e-05\n",
      "Epoch 1/1 | Step 715000/839071 | Loss: 9.780565596884117e-05\n",
      "Epoch 1/1 | Step 716000/839071 | Loss: 0.0006470678490586579\n",
      "Epoch 1/1 | Step 717000/839071 | Loss: 3.628675767686218e-05\n",
      "Epoch 1/1 | Step 718000/839071 | Loss: 0.0002656550204847008\n",
      "Epoch 1/1 | Step 719000/839071 | Loss: 0.0005511121125891805\n",
      "Epoch 1/1 | Step 720000/839071 | Loss: 0.0001312379608862102\n",
      "Epoch 1/1 | Step 721000/839071 | Loss: 0.0008249428356066346\n",
      "Epoch 1/1 | Step 722000/839071 | Loss: 0.03493773564696312\n",
      "Epoch 1/1 | Step 723000/839071 | Loss: 0.5309814810752869\n",
      "Epoch 1/1 | Step 724000/839071 | Loss: 0.0001480991777498275\n",
      "Epoch 1/1 | Step 725000/839071 | Loss: 0.00015942478785291314\n",
      "Epoch 1/1 | Step 726000/839071 | Loss: 0.0004220121190883219\n",
      "Epoch 1/1 | Step 727000/839071 | Loss: 2.801325717882719e-05\n",
      "Epoch 1/1 | Step 728000/839071 | Loss: 0.0006818502442911267\n",
      "Epoch 1/1 | Step 729000/839071 | Loss: 0.00020129032782278955\n",
      "Epoch 1/1 | Step 730000/839071 | Loss: 5.714920916943811e-05\n",
      "Epoch 1/1 | Step 731000/839071 | Loss: 1.7091129848267883e-05\n",
      "Epoch 1/1 | Step 732000/839071 | Loss: 0.0001035011955536902\n",
      "Epoch 1/1 | Step 733000/839071 | Loss: 1.9355993572389707e-05\n",
      "Epoch 1/1 | Step 734000/839071 | Loss: 0.00315546034835279\n",
      "Epoch 1/1 | Step 735000/839071 | Loss: 9.327321458840743e-05\n",
      "Epoch 1/1 | Step 736000/839071 | Loss: 0.0006565392250195146\n",
      "Epoch 1/1 | Step 737000/839071 | Loss: 2.4831828341120854e-05\n",
      "Epoch 1/1 | Step 738000/839071 | Loss: 0.00021331245079636574\n",
      "Epoch 1/1 | Step 739000/839071 | Loss: 0.0002281747292727232\n",
      "Epoch 1/1 | Step 740000/839071 | Loss: 8.156587136909366e-05\n",
      "Epoch 1/1 | Step 741000/839071 | Loss: 0.004328662995249033\n",
      "Epoch 1/1 | Step 742000/839071 | Loss: 0.012769481167197227\n",
      "Epoch 1/1 | Step 743000/839071 | Loss: 0.4051319658756256\n",
      "Epoch 1/1 | Step 744000/839071 | Loss: 0.0001611337356735021\n",
      "Epoch 1/1 | Step 745000/839071 | Loss: 6.291134195635095e-05\n",
      "Epoch 1/1 | Step 746000/839071 | Loss: 2.0361590941320173e-05\n",
      "Epoch 1/1 | Step 747000/839071 | Loss: 7.915741298347712e-05\n",
      "Epoch 1/1 | Step 748000/839071 | Loss: 0.00017128553008660674\n",
      "Epoch 1/1 | Step 749000/839071 | Loss: 0.0005799403879791498\n",
      "Epoch 1/1 | Step 750000/839071 | Loss: 1.8379643734078854e-05\n",
      "Epoch 1/1 | Step 751000/839071 | Loss: 0.00012952415272593498\n",
      "Epoch 1/1 | Step 752000/839071 | Loss: 0.0005974976811558008\n",
      "Epoch 1/1 | Step 753000/839071 | Loss: 0.00025095665478147566\n",
      "Epoch 1/1 | Step 754000/839071 | Loss: 3.5123863199260086e-05\n",
      "Epoch 1/1 | Step 755000/839071 | Loss: 0.0011820639483630657\n",
      "Epoch 1/1 | Step 756000/839071 | Loss: 0.3544398844242096\n",
      "Epoch 1/1 | Step 757000/839071 | Loss: 4.533110040938482e-05\n",
      "Epoch 1/1 | Step 758000/839071 | Loss: 8.111247007036582e-05\n",
      "Epoch 1/1 | Step 759000/839071 | Loss: 0.0004964454565197229\n",
      "Epoch 1/1 | Step 760000/839071 | Loss: 0.0020084327552467585\n",
      "Epoch 1/1 | Step 761000/839071 | Loss: 0.00039768993156030774\n",
      "Epoch 1/1 | Step 762000/839071 | Loss: 1.1235320926061831e-05\n",
      "Epoch 1/1 | Step 763000/839071 | Loss: 1.698695450613741e-05\n",
      "Epoch 1/1 | Step 764000/839071 | Loss: 0.0007877100724726915\n",
      "Epoch 1/1 | Step 765000/839071 | Loss: 1.0155044037674088e-05\n",
      "Epoch 1/1 | Step 766000/839071 | Loss: 3.0410925319301896e-05\n",
      "Epoch 1/1 | Step 767000/839071 | Loss: 0.4054791331291199\n",
      "Epoch 1/1 | Step 768000/839071 | Loss: 3.341374031151645e-05\n",
      "Epoch 1/1 | Step 769000/839071 | Loss: 8.515827175870072e-06\n",
      "Epoch 1/1 | Step 770000/839071 | Loss: 5.972202052362263e-05\n",
      "Epoch 1/1 | Step 771000/839071 | Loss: 1.564586091262754e-05\n",
      "Epoch 1/1 | Step 772000/839071 | Loss: 4.158607407589443e-05\n",
      "Epoch 1/1 | Step 773000/839071 | Loss: 0.010104188695549965\n",
      "Epoch 1/1 | Step 774000/839071 | Loss: 7.070602441672236e-05\n",
      "Epoch 1/1 | Step 775000/839071 | Loss: 2.330332063138485e-05\n",
      "Epoch 1/1 | Step 776000/839071 | Loss: 0.29089415073394775\n",
      "Epoch 1/1 | Step 777000/839071 | Loss: 0.0005484521971084177\n",
      "Epoch 1/1 | Step 778000/839071 | Loss: 0.32447609305381775\n",
      "Epoch 1/1 | Step 779000/839071 | Loss: 0.0002392405876889825\n",
      "Epoch 1/1 | Step 780000/839071 | Loss: 0.0022495908197015524\n",
      "Epoch 1/1 | Step 781000/839071 | Loss: 5.149013304617256e-05\n",
      "Epoch 1/1 | Step 782000/839071 | Loss: 1.118317959480919e-05\n",
      "Epoch 1/1 | Step 783000/839071 | Loss: 4.294184691389091e-05\n",
      "Epoch 1/1 | Step 784000/839071 | Loss: 0.0001234824158018455\n",
      "Epoch 1/1 | Step 785000/839071 | Loss: 0.0004953808966092765\n",
      "Epoch 1/1 | Step 786000/839071 | Loss: 0.015563927590847015\n",
      "Epoch 1/1 | Step 787000/839071 | Loss: 0.00013699269038625062\n",
      "Epoch 1/1 | Step 788000/839071 | Loss: 0.0004377660807222128\n",
      "Epoch 1/1 | Step 789000/839071 | Loss: 0.00013810856034979224\n",
      "Epoch 1/1 | Step 790000/839071 | Loss: 0.00016718299593776464\n",
      "Epoch 1/1 | Step 791000/839071 | Loss: 0.0003816852404270321\n",
      "Epoch 1/1 | Step 792000/839071 | Loss: 0.09155577421188354\n",
      "Epoch 1/1 | Step 793000/839071 | Loss: 0.0011355709284543991\n",
      "Epoch 1/1 | Step 794000/839071 | Loss: 0.0004871587734669447\n",
      "Epoch 1/1 | Step 795000/839071 | Loss: 2.2536471078637987e-05\n",
      "Epoch 1/1 | Step 796000/839071 | Loss: 9.834428055910394e-05\n",
      "Epoch 1/1 | Step 797000/839071 | Loss: 9.321694960817695e-05\n",
      "Epoch 1/1 | Step 798000/839071 | Loss: 8.106834866339341e-05\n",
      "Epoch 1/1 | Step 799000/839071 | Loss: 7.901570643298328e-05\n",
      "Epoch 1/1 | Step 800000/839071 | Loss: 4.9386784667149186e-05\n",
      "Epoch 1/1 | Step 801000/839071 | Loss: 1.0244434633932542e-05\n",
      "Epoch 1/1 | Step 802000/839071 | Loss: 0.0008451109752058983\n",
      "Epoch 1/1 | Step 803000/839071 | Loss: 8.895882274373434e-06\n",
      "Epoch 1/1 | Step 804000/839071 | Loss: 2.767800287983846e-05\n",
      "Epoch 1/1 | Step 805000/839071 | Loss: 4.928320777253248e-05\n",
      "Epoch 1/1 | Step 806000/839071 | Loss: 0.0002370337169850245\n",
      "Epoch 1/1 | Step 807000/839071 | Loss: 0.0022341846488416195\n",
      "Epoch 1/1 | Step 808000/839071 | Loss: 1.5340438039856963e-05\n",
      "Epoch 1/1 | Step 809000/839071 | Loss: 7.833139534341171e-05\n",
      "Epoch 1/1 | Step 810000/839071 | Loss: 5.721531852032058e-05\n",
      "Epoch 1/1 | Step 811000/839071 | Loss: 0.00016425541252829134\n",
      "Epoch 1/1 | Step 812000/839071 | Loss: 1.0341263077862095e-05\n",
      "Epoch 1/1 | Step 813000/839071 | Loss: 6.908360228408128e-05\n",
      "Epoch 1/1 | Step 814000/839071 | Loss: 0.00028240904794074595\n",
      "Epoch 1/1 | Step 815000/839071 | Loss: 0.00012503660400398076\n",
      "Epoch 1/1 | Step 816000/839071 | Loss: 2.9070106393191963e-05\n",
      "Epoch 1/1 | Step 817000/839071 | Loss: 0.0005140167195349932\n",
      "Epoch 1/1 | Step 818000/839071 | Loss: 0.29984530806541443\n",
      "Epoch 1/1 | Step 819000/839071 | Loss: 0.01002874132245779\n",
      "Epoch 1/1 | Step 820000/839071 | Loss: 2.679672252270393e-05\n",
      "Epoch 1/1 | Step 821000/839071 | Loss: 0.01270357146859169\n",
      "Epoch 1/1 | Step 822000/839071 | Loss: 0.00030879906262271106\n",
      "Epoch 1/1 | Step 823000/839071 | Loss: 3.198154445271939e-05\n",
      "Epoch 1/1 | Step 824000/839071 | Loss: 5.182397580938414e-05\n",
      "Epoch 1/1 | Step 825000/839071 | Loss: 0.0002058033278444782\n",
      "Epoch 1/1 | Step 826000/839071 | Loss: 0.00017997701070271432\n",
      "Epoch 1/1 | Step 827000/839071 | Loss: 9.151075937552378e-05\n",
      "Epoch 1/1 | Step 828000/839071 | Loss: 9.009255154523998e-05\n",
      "Epoch 1/1 | Step 829000/839071 | Loss: 2.214192682004068e-05\n",
      "Epoch 1/1 | Step 830000/839071 | Loss: 2.0451339878491126e-05\n",
      "Epoch 1/1 | Step 831000/839071 | Loss: 4.1622195567470044e-05\n",
      "Epoch 1/1 | Step 832000/839071 | Loss: 4.635831282939762e-05\n",
      "Epoch 1/1 | Step 833000/839071 | Loss: 2.342370498809032e-05\n",
      "Epoch 1/1 | Step 834000/839071 | Loss: 0.001214334974065423\n",
      "Epoch 1/1 | Step 835000/839071 | Loss: 2.4399691028520465e-05\n",
      "Epoch 1/1 | Step 836000/839071 | Loss: 0.00017706691869534552\n",
      "Epoch 1/1 | Step 837000/839071 | Loss: 0.657374918460846\n",
      "Epoch 1/1 | Step 838000/839071 | Loss: 7.904060475993901e-05\n",
      "Epoch 1/1 | Step 839000/839071 | Loss: 0.0008134192321449518\n",
      "Epoch 1, Average Loss: 0.0742058294729718\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader1):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader)} | Loss: {loss.item()}\")\n",
    "\n",
    "    # End time for the epoch\n",
    "    # epoch_end_time = time.time()\n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\") \n",
    "          # Time Taken: {epoch_time:.2f} seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions1 = []\n",
    "true_labels1 = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader1:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions1.extend(preds)\n",
    "        true_labels1.extend(b_labels.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Month                   Place  Crop_QueryType_code\n",
      "0            2    TELANGANA_SANGAREDDY                 5468\n",
      "1            7     MAHARASHTRA_SOLAPUR                 1885\n",
      "2           10         HARYANA_JHAJJAR                 7763\n",
      "3            7    UTTAR PRADESH_JALAUN                 1353\n",
      "4            4     WEST BENGAL_BANKURA                 7433\n",
      "...        ...                     ...                  ...\n",
      "3356277      7        RAJASTHAN_JAIPUR                 5662\n",
      "3356278     10  MAHARASHTRA_AHMADNAGAR                  864\n",
      "3356279      8       KARNATAKA_BELLARY                 4973\n",
      "3356280     10         RAJASTHAN_ALWAR                 2331\n",
      "3356281      1            ASSAM_NAGAON                 2181\n",
      "\n",
      "[3356282 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "predictions_flat1 = predictions1\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data1= pd.DataFrame({\n",
    "    'Month': data.loc[test_idx1, 'Month'].values, \n",
    "    'Place': data.loc[test_idx1, 'place'].values,\n",
    "    'Crop_QueryType_code': predictions_flat1\n",
    "})\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_41744\\301140935.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Month                   Place  Crop_QueryType_code  \\\n",
      "0            2    TELANGANA_SANGAREDDY                 5468   \n",
      "1            7     MAHARASHTRA_SOLAPUR                 1885   \n",
      "2           10         HARYANA_JHAJJAR                 7763   \n",
      "3            7    UTTAR PRADESH_JALAUN                 1353   \n",
      "4            4     WEST BENGAL_BANKURA                 7433   \n",
      "...        ...                     ...                  ...   \n",
      "3356277      7        RAJASTHAN_JAIPUR                 5662   \n",
      "3356278     10  MAHARASHTRA_AHMADNAGAR                  864   \n",
      "3356279      8       KARNATAKA_BELLARY                 4973   \n",
      "3356280     10         RAJASTHAN_ALWAR                 2331   \n",
      "3356281      1            ASSAM_NAGAON                 2181   \n",
      "\n",
      "                                            Crop_QueryType  \n",
      "0                              Paddy Dhan_Plant Protection  \n",
      "1                                Chillies_Plant Protection  \n",
      "2                            Wheat_Sowing Time and Weather  \n",
      "3                       Buck Wheat Kaspat_Plant Protection  \n",
      "4                                  Tomato_Plant Protection  \n",
      "...                                                    ...  \n",
      "3356277  Pearl Millet BajraBulrush MilletSpiked Millet_...  \n",
      "3356278     Bengal Gram GramChick PeaKabuliChana_Varieties  \n",
      "3356279  Moth Bean kidney bean deww gram_Nutrient Manag...  \n",
      "3356280                      Coriander_Nutrient Management  \n",
      "3356281            Coconut_Fertilizer Use and Availability  \n",
      "\n",
      "[3356282 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n",
    "predicted_data1['Crop_QueryType']=predicted_data1['Crop_QueryType_code'].apply(lambda x: data['Crop_QueryType'].cat.categories[x])\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7741     12     WEST BENGAL_North DINAJPUR   \n",
      "7742     12            WEST BENGAL_PURULIA   \n",
      "7743     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7744     12     WEST BENGAL_South DINAJPUR   \n",
      "7745     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Index(['Sugarcane Noble Cane_Plant Protection'...  \n",
      "1     Index(['Groundnut pea nutmung phalli_Plant Pro...  \n",
      "2     Index(['Paddy Dhan_Plant Protection', 'Chillie...  \n",
      "3     Index(['Paddy Dhan_Plant Protection', 'Chillie...  \n",
      "4     Index(['Chillies_Plant Protection', 'Chillies_...  \n",
      "...                                                 ...  \n",
      "7741  Index(['Potato_Plant Protection', 'Maize Makka...  \n",
      "7742  Index(['Potato_Plant Protection', 'Chillies_Pl...  \n",
      "7743  Index(['Paddy Dhan_Plant Protection', 'Paddy D...  \n",
      "7744  Index(['Potato_Plant Protection', 'Paddy Dhan_...  \n",
      "7745  Index(['Paddy Dhan_Plant Protection', 'Paddy D...  \n",
      "\n",
      "[7746 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: x.value_counts().index[:15]).reset_index()\n",
    "print(monthly_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7741     12     WEST BENGAL_North DINAJPUR   \n",
      "7742     12            WEST BENGAL_PURULIA   \n",
      "7743     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7744     12     WEST BENGAL_South DINAJPUR   \n",
      "7745     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Sugarcane Noble Cane_Plant Protection, POULTRY...  \n",
      "1     Groundnut pea nutmung phalli_Plant Protection,...  \n",
      "2     Paddy Dhan_Plant Protection, Chillies_Plant Pr...  \n",
      "3     Paddy Dhan_Plant Protection, Chillies_Plant Pr...  \n",
      "4     Chillies_Plant Protection, Chillies_Nutrient M...  \n",
      "...                                                 ...  \n",
      "7741  Potato_Plant Protection, Maize Makka_Plant Pro...  \n",
      "7742  Potato_Plant Protection, Chillies_Plant Protec...  \n",
      "7743  Paddy Dhan_Plant Protection, Paddy Dhan_Weathe...  \n",
      "7744  Potato_Plant Protection, Paddy Dhan_Plant Prot...  \n",
      "7745  Paddy Dhan_Plant Protection, Paddy Dhan_Weathe...  \n",
      "\n",
      "[7746 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "monthly_predictions1= monthly_predictions1.explode('Crop_QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions1 = monthly_predictions1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "print(monthly_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7741     12     WEST BENGAL_North DINAJPUR   \n",
      "7742     12            WEST BENGAL_PURULIA   \n",
      "7743     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7744     12     WEST BENGAL_South DINAJPUR   \n",
      "7745     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Sugarcane Noble Cane_Plant Protection, POULTRY...  \n",
      "1     Pigeon pea red gramarhartur_Plant Protection, ...  \n",
      "2     Mango_Plant Protection, Beans _Nutrient Manage...  \n",
      "3     Paddy Dhan_Plant Protection, Maize Makka_Plant...  \n",
      "4     Black Gram urd bean_Varieties, Chillies_Nutrie...  \n",
      "...                                                 ...  \n",
      "7741  Mustard_Nutrient Management, Cucumber_Cultural...  \n",
      "7742  Brinjal_Plant Protection, Pumpkin_Cultural Pra...  \n",
      "7743  Potato_Plant Protection, Khesari chickling vet...  \n",
      "7744  Paddy Dhan_Plant Protection, Broad Bean_Plant ...  \n",
      "7745  Paddy Dhan_Weather, Jack Fruit_Plant Protectio...  \n",
      "\n",
      "[7746 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_data1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "print(monthly_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_data1.to_csv('D:/Data/Places_crop_querytypes_in_India11.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions1 = np.array(predictions1)\n",
    "true_labels1 = np.array(true_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def precision(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Positives (FP)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    \n",
    "    # Calculate Precision\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "def accuracy(true_labels, predictions):\n",
    "    # Count the number of correct predictions\n",
    "    correct = np.sum(np.array(true_labels) == np.array(predictions))\n",
    "    # Calculate accuracy\n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels1, predictions1)\n",
    "mae = mae(true_labels1, predictions1)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1= f1_score(true_labels1, predictions1)\n",
    "recall = recall(true_labels1, predictions1)\n",
    "\n",
    "accuracy = accuracy(true_labels1, predictions1)\n",
    "precision = precision(true_labels1, predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "      Metric       Value\n",
      "0   Accuracy    0.996934\n",
      "1  Precision    0.000000\n",
      "2     Recall    0.000000\n",
      "3   F1-Score    0.000000\n",
      "4       RMSE  124.883257\n",
      "5        MAE    4.056527\n"
     ]
    }
   ],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rmse, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rmse, mae]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/roberta2_model_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.8832574519403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mean_squared_error(true_labels1, predictions1))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
