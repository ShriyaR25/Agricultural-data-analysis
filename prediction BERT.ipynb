{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\2452469146.py:2: DtypeWarning: Columns (1,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data1 = pd.read_csv('D:/kcc_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load your preprocessed dataset\n",
    "data1 = pd.read_csv('D:/kcc_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "f_df = data1[(data1['Crop'].str.isnumeric() == False) & (data1['Crop'] != 'Others') & \n",
    "             (data1['QueryType'].str.isnumeric() == False) & (data1['QueryType'] != 'Others')]\n",
    "df= pd.DataFrame(f_df)\n",
    "df = df[df['Year'] >= 2013]\n",
    "# Create a new feature 'Crop_QueryType'\n",
    "df[\"place\"] = df['StateName'] + '_' + df['DistrictName']\n",
    "data = df[['Month', 'Year', 'place', 'Crop', 'QueryType']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BlockName          Category  Year  Month  Day  \\\n",
      "3039763   PATHAPATNAM          Oilseeds  2013      1    7   \n",
      "3039867          0             Oilseeds  2013      2    8   \n",
      "3039869          GARA          Oilseeds  2013      2    8   \n",
      "3040001         BURJA        Vegetables  2013      4    5   \n",
      "3040002   KOTABOMMILI  Plantation Crops  2013      4    5   \n",
      "...               ...               ...   ...    ...  ...   \n",
      "40774809        DELHI           Cereals  2024      8   19   \n",
      "40774810        DELHI           Cereals  2024      8   19   \n",
      "40774813        DELHI           Cereals  2024      8   19   \n",
      "40774814        DELHI           Cereals  2024      8   19   \n",
      "40774815        DELHI           Cereals  2024      8   19   \n",
      "\n",
      "                                  Crop               DistrictName  \\\n",
      "3039763           Sunflower suryamukhi                 SRIKAKULAM   \n",
      "3039867   Groundnut pea nutmung phalli                 SRIKAKULAM   \n",
      "3039869   Groundnut pea nutmung phalli                 SRIKAKULAM   \n",
      "3040001                    Runner Bean                 SRIKAKULAM   \n",
      "3040002                        Coconut                 SRIKAKULAM   \n",
      "...                                ...                        ...   \n",
      "40774809                    Paddy Dhan  New Delhi Connaught Place   \n",
      "40774810                    Paddy Dhan  New Delhi Connaught Place   \n",
      "40774813                    Paddy Dhan  New Delhi Connaught Place   \n",
      "40774814                    Paddy Dhan  New Delhi Connaught Place   \n",
      "40774815                    Paddy Dhan  New Delhi Connaught Place   \n",
      "\n",
      "                                QueryType Season        Sector  \\\n",
      "3039763                  Water Management    NaN   AGRICULTURE   \n",
      "3039867   Fertilizer Use and Availability    NaN   AGRICULTURE   \n",
      "3039869   Fertilizer Use and Availability    NaN   AGRICULTURE   \n",
      "3040001                  Plant Protection    NaN  HORTICULTURE   \n",
      "3040002                  Plant Protection    NaN  HORTICULTURE   \n",
      "...                                   ...    ...           ...   \n",
      "40774809                 Plant Protection    NaN   AGRICULTURE   \n",
      "40774810                 Plant Protection    NaN   AGRICULTURE   \n",
      "40774813                 Plant Protection    NaN   AGRICULTURE   \n",
      "40774814                 Plant Protection    NaN   AGRICULTURE   \n",
      "40774815                 Plant Protection    NaN   AGRICULTURE   \n",
      "\n",
      "               StateName                                        QueryText  \\\n",
      "3039763   ANDHRA PRADESH       asked about water management in sun flower   \n",
      "3039867   ANDHRA PRADESH      asked about fertilizer dosage in ground nut   \n",
      "3039869   ANDHRA PRADESH                asked about groundnut fertilizers   \n",
      "3040001   ANDHRA PRADESH                                             mite   \n",
      "3040002   ANDHRA PRADESH                            asked about ganoderma   \n",
      "...                  ...                                              ...   \n",
      "40774809           DELHI       Farmer  asked about control of Stem borer    \n",
      "40774810           DELHI       Farmer  asked about control of Stem borer    \n",
      "40774813           DELHI    ASKED ABOUT TO NUTRIENT MANAGEMENT IN PADDY     \n",
      "40774814           DELHI  FARMER ASKED ABOUT TO DISEASE CONTROL IN PADDY    \n",
      "40774815           DELHI   FARMER ASKED ABOUT TO INSECT CONTROL IN PADDY    \n",
      "\n",
      "                                             KccAns  \\\n",
      "3039763                       6 to 8 week intervels   \n",
      "3039867      information is given as per panchangam   \n",
      "3039869           given him information as per data   \n",
      "3040001                              spray dicophol   \n",
      "3040002           given him information as per data   \n",
      "...                                             ...   \n",
      "40774809        Ferterra    04 ww GR    5             \n",
      "40774810        Ferterra    04 ww GR    5             \n",
      "40774813                NPK 0:52:34   8-10            \n",
      "40774814                          25  2               \n",
      "40774815                          50    2             \n",
      "\n",
      "                                    place  \\\n",
      "3039763         ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039867         ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039869         ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040001         ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040002         ANDHRA PRADESH_SRIKAKULAM   \n",
      "...                                   ...   \n",
      "40774809  DELHI_New Delhi Connaught Place   \n",
      "40774810  DELHI_New Delhi Connaught Place   \n",
      "40774813  DELHI_New Delhi Connaught Place   \n",
      "40774814  DELHI_New Delhi Connaught Place   \n",
      "40774815  DELHI_New Delhi Connaught Place   \n",
      "\n",
      "                                             Crop_QueryType  \n",
      "3039763               Sunflower suryamukhi_Water Management  \n",
      "3039867   Groundnut pea nutmung phalli_Fertilizer Use an...  \n",
      "3039869   Groundnut pea nutmung phalli_Fertilizer Use an...  \n",
      "3040001                        Runner Bean_Plant Protection  \n",
      "3040002                            Coconut_Plant Protection  \n",
      "...                                                     ...  \n",
      "40774809                        Paddy Dhan_Plant Protection  \n",
      "40774810                        Paddy Dhan_Plant Protection  \n",
      "40774813                        Paddy Dhan_Plant Protection  \n",
      "40774814                        Paddy Dhan_Plant Protection  \n",
      "40774815                        Paddy Dhan_Plant Protection  \n",
      "\n",
      "[18310634 rows x 15 columns]\n",
      "          Month                            place  \\\n",
      "3039763       1        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039867       2        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3039869       2        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040001       4        ANDHRA PRADESH_SRIKAKULAM   \n",
      "3040002       4        ANDHRA PRADESH_SRIKAKULAM   \n",
      "...         ...                              ...   \n",
      "40774809      8  DELHI_New Delhi Connaught Place   \n",
      "40774810      8  DELHI_New Delhi Connaught Place   \n",
      "40774813      8  DELHI_New Delhi Connaught Place   \n",
      "40774814      8  DELHI_New Delhi Connaught Place   \n",
      "40774815      8  DELHI_New Delhi Connaught Place   \n",
      "\n",
      "                                             Crop_QueryType  \\\n",
      "3039763               Sunflower suryamukhi_Water Management   \n",
      "3039867   Groundnut pea nutmung phalli_Fertilizer Use an...   \n",
      "3039869   Groundnut pea nutmung phalli_Fertilizer Use an...   \n",
      "3040001                        Runner Bean_Plant Protection   \n",
      "3040002                            Coconut_Plant Protection   \n",
      "...                                                     ...   \n",
      "40774809                        Paddy Dhan_Plant Protection   \n",
      "40774810                        Paddy Dhan_Plant Protection   \n",
      "40774813                        Paddy Dhan_Plant Protection   \n",
      "40774814                        Paddy Dhan_Plant Protection   \n",
      "40774815                        Paddy Dhan_Plant Protection   \n",
      "\n",
      "          Crop_QueryType_code  \\\n",
      "3039763                  7316   \n",
      "3039867                  3451   \n",
      "3039869                  3451   \n",
      "3040001                  6542   \n",
      "3040002                  2232   \n",
      "...                       ...   \n",
      "40774809                 5556   \n",
      "40774810                 5556   \n",
      "40774813                 5556   \n",
      "40774814                 5556   \n",
      "40774815                 5556   \n",
      "\n",
      "                                                      text1  \n",
      "3039763   1 ANDHRA PRADESH_SRIKAKULAM Sunflower suryamuk...  \n",
      "3039867   2 ANDHRA PRADESH_SRIKAKULAM Groundnut pea nutm...  \n",
      "3039869   2 ANDHRA PRADESH_SRIKAKULAM Groundnut pea nutm...  \n",
      "3040001   4 ANDHRA PRADESH_SRIKAKULAM Runner Bean_Plant ...  \n",
      "3040002   4 ANDHRA PRADESH_SRIKAKULAM Coconut_Plant Prot...  \n",
      "...                                                     ...  \n",
      "40774809  8 DELHI_New Delhi Connaught Place Paddy Dhan_P...  \n",
      "40774810  8 DELHI_New Delhi Connaught Place Paddy Dhan_P...  \n",
      "40774813  8 DELHI_New Delhi Connaught Place Paddy Dhan_P...  \n",
      "40774814  8 DELHI_New Delhi Connaught Place Paddy Dhan_P...  \n",
      "40774815  8 DELHI_New Delhi Connaught Place Paddy Dhan_P...  \n",
      "\n",
      "[18310634 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop wise Quertypes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\702583409.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['QueryType_code'] = data['QueryType'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "# Encode Crop_QueryType as categorical variable\n",
    "data['QueryType_code'] = data['QueryType'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\489744648.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop']} {row['QueryType']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop']} {row['QueryType']}\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(data['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract labels for training (Crop_QueryType codes)\n",
    "labels = torch.tensor(data['QueryType_code'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_inputs, test_inputs, train_labels, test_labels, train_idx, test_idx = train_test_split(\n",
    "    inputs['input_ids'], labels, data.index, test_size=0.2, random_state=42)\n",
    "train_masks, test_masks = train_test_split(inputs['attention_mask'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\3377841694.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(train_inputs, torch.tensor(train_masks), train_labels)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\3377841694.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(test_inputs, torch.tensor(test_masks), test_labels)\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, torch.tensor(train_masks), train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, torch.tensor(test_masks), test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=data['QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 0/915532 | Loss: 4.133554935455322\n",
      "Epoch 1/1 | Step 1000/915532 | Loss: 0.03110203705728054\n",
      "Epoch 1/1 | Step 2000/915532 | Loss: 0.0046039461158216\n",
      "Epoch 1/1 | Step 3000/915532 | Loss: 0.15722256898880005\n",
      "Epoch 1/1 | Step 4000/915532 | Loss: 0.0027088611386716366\n",
      "Epoch 1/1 | Step 5000/915532 | Loss: 0.008516019210219383\n",
      "Epoch 1/1 | Step 6000/915532 | Loss: 0.0005387722048908472\n",
      "Epoch 1/1 | Step 7000/915532 | Loss: 0.0004260171263013035\n",
      "Epoch 1/1 | Step 8000/915532 | Loss: 0.0001441077038180083\n",
      "Epoch 1/1 | Step 9000/915532 | Loss: 0.00011544212611624971\n",
      "Epoch 1/1 | Step 10000/915532 | Loss: 0.0026145889423787594\n",
      "Epoch 1/1 | Step 11000/915532 | Loss: 3.7147608964005485e-05\n",
      "Epoch 1/1 | Step 12000/915532 | Loss: 0.00011600961443036795\n",
      "Epoch 1/1 | Step 13000/915532 | Loss: 6.922431930433959e-05\n",
      "Epoch 1/1 | Step 14000/915532 | Loss: 5.69638614251744e-05\n",
      "Epoch 1/1 | Step 15000/915532 | Loss: 0.0008609023643657565\n",
      "Epoch 1/1 | Step 16000/915532 | Loss: 6.749507883796468e-05\n",
      "Epoch 1/1 | Step 17000/915532 | Loss: 5.271777990856208e-05\n",
      "Epoch 1/1 | Step 18000/915532 | Loss: 2.6284766136086546e-05\n",
      "Epoch 1/1 | Step 19000/915532 | Loss: 5.022817276767455e-05\n",
      "Epoch 1/1 | Step 20000/915532 | Loss: 2.2880149117554538e-05\n",
      "Epoch 1/1 | Step 21000/915532 | Loss: 5.7302735513076186e-05\n",
      "Epoch 1/1 | Step 22000/915532 | Loss: 1.5481988157262094e-05\n",
      "Epoch 1/1 | Step 23000/915532 | Loss: 3.6200650356477126e-05\n",
      "Epoch 1/1 | Step 24000/915532 | Loss: 1.1905862265848555e-05\n",
      "Epoch 1/1 | Step 25000/915532 | Loss: 4.9477988795842975e-05\n",
      "Epoch 1/1 | Step 26000/915532 | Loss: 2.325233435840346e-05\n",
      "Epoch 1/1 | Step 27000/915532 | Loss: 1.7396743714925833e-05\n",
      "Epoch 1/1 | Step 28000/915532 | Loss: 5.803967724204995e-06\n",
      "Epoch 1/1 | Step 29000/915532 | Loss: 1.7016536730807275e-05\n",
      "Epoch 1/1 | Step 30000/915532 | Loss: 7.070533683872782e-06\n",
      "Epoch 1/1 | Step 31000/915532 | Loss: 4.813035047845915e-06\n",
      "Epoch 1/1 | Step 32000/915532 | Loss: 1.4088529496802948e-05\n",
      "Epoch 1/1 | Step 33000/915532 | Loss: 5.9976537158945575e-06\n",
      "Epoch 1/1 | Step 34000/915532 | Loss: 4.157389412284829e-06\n",
      "Epoch 1/1 | Step 35000/915532 | Loss: 1.3127724741934799e-05\n",
      "Epoch 1/1 | Step 36000/915532 | Loss: 6.757606570317876e-06\n",
      "Epoch 1/1 | Step 37000/915532 | Loss: 6.52663493383443e-06\n",
      "Epoch 1/1 | Step 38000/915532 | Loss: 3.6212637496646494e-05\n",
      "Epoch 1/1 | Step 39000/915532 | Loss: 3.3452931802457897e-06\n",
      "Epoch 1/1 | Step 40000/915532 | Loss: 2.058522659353912e-05\n",
      "Epoch 1/1 | Step 41000/915532 | Loss: 2.9429697860905435e-06\n",
      "Epoch 1/1 | Step 42000/915532 | Loss: 1.4587426448997576e-05\n",
      "Epoch 1/1 | Step 43000/915532 | Loss: 1.2799484466086142e-05\n",
      "Epoch 1/1 | Step 44000/915532 | Loss: 1.5869707112869946e-06\n",
      "Epoch 1/1 | Step 45000/915532 | Loss: 1.7136283076979453e-06\n",
      "Epoch 1/1 | Step 46000/915532 | Loss: 3.0174617222655797e-06\n",
      "Epoch 1/1 | Step 47000/915532 | Loss: 8.568160865252139e-07\n",
      "Epoch 1/1 | Step 48000/915532 | Loss: 1.6614759488220443e-06\n",
      "Epoch 1/1 | Step 49000/915532 | Loss: 2.5108324734901544e-06\n",
      "Epoch 1/1 | Step 50000/915532 | Loss: 1.4901131635269849e-06\n",
      "Epoch 1/1 | Step 51000/915532 | Loss: 5.8858136071648914e-06\n",
      "Epoch 1/1 | Step 52000/915532 | Loss: 4.276605068298522e-06\n",
      "Epoch 1/1 | Step 53000/915532 | Loss: 5.587930331785174e-07\n",
      "Epoch 1/1 | Step 54000/915532 | Loss: 1.6689209587639198e-06\n",
      "Epoch 1/1 | Step 55000/915532 | Loss: 3.784853561228374e-06\n",
      "Epoch 1/1 | Step 56000/915532 | Loss: 1.0654312063707039e-06\n",
      "Epoch 1/1 | Step 57000/915532 | Loss: 7.599580271744344e-07\n",
      "Epoch 1/1 | Step 58000/915532 | Loss: 6.965993179619545e-06\n",
      "Epoch 1/1 | Step 59000/915532 | Loss: 8.791678283159854e-07\n",
      "Epoch 1/1 | Step 60000/915532 | Loss: 4.3213339040448773e-07\n",
      "Epoch 1/1 | Step 61000/915532 | Loss: 3.9487094909418374e-06\n",
      "Epoch 1/1 | Step 62000/915532 | Loss: 1.6242172478087014e-06\n",
      "Epoch 1/1 | Step 63000/915532 | Loss: 1.0654248399077915e-06\n",
      "Epoch 1/1 | Step 64000/915532 | Loss: 1.1175869474300271e-07\n",
      "Epoch 1/1 | Step 65000/915532 | Loss: 1.1771877552746446e-06\n",
      "Epoch 1/1 | Step 66000/915532 | Loss: 4.269052169547649e-06\n",
      "Epoch 1/1 | Step 67000/915532 | Loss: 4.1723231447576836e-07\n",
      "Epoch 1/1 | Step 68000/915532 | Loss: 2.428873358439887e-06\n",
      "Epoch 1/1 | Step 69000/915532 | Loss: 1.110131734094466e-06\n",
      "Epoch 1/1 | Step 70000/915532 | Loss: 1.2516871947809705e-06\n",
      "Epoch 1/1 | Step 71000/915532 | Loss: 1.2144408856329392e-06\n",
      "Epoch 1/1 | Step 72000/915532 | Loss: 1.199540747620631e-06\n",
      "Epoch 1/1 | Step 73000/915532 | Loss: 9.611217137717176e-07\n",
      "Epoch 1/1 | Step 74000/915532 | Loss: 9.909252867146279e-07\n",
      "Epoch 1/1 | Step 75000/915532 | Loss: 9.685695658845361e-07\n",
      "Epoch 1/1 | Step 76000/915532 | Loss: 6.109451646807429e-07\n",
      "Epoch 1/1 | Step 77000/915532 | Loss: 5.960451403552725e-07\n",
      "Epoch 1/1 | Step 78000/915532 | Loss: 2.1159496554901125e-06\n",
      "Epoch 1/1 | Step 79000/915532 | Loss: 7.569684839836555e-06\n",
      "Epoch 1/1 | Step 80000/915532 | Loss: 1.3663552635989618e-05\n",
      "Epoch 1/1 | Step 81000/915532 | Loss: 4.14994156017201e-06\n",
      "Epoch 1/1 | Step 82000/915532 | Loss: 7.003538939898135e-07\n",
      "Epoch 1/1 | Step 83000/915532 | Loss: 9.811708878260106e-06\n",
      "Epoch 1/1 | Step 84000/915532 | Loss: 2.0414470327523304e-06\n",
      "Epoch 1/1 | Step 85000/915532 | Loss: 2.935513748525409e-06\n",
      "Epoch 1/1 | Step 86000/915532 | Loss: 4.0978167703542567e-07\n",
      "Epoch 1/1 | Step 87000/915532 | Loss: 1.966935542441206e-06\n",
      "Epoch 1/1 | Step 88000/915532 | Loss: 8.195628424800816e-07\n",
      "Epoch 1/1 | Step 89000/915532 | Loss: 1.5869698017922929e-06\n",
      "Epoch 1/1 | Step 90000/915532 | Loss: 2.6672626063373173e-06\n",
      "Epoch 1/1 | Step 91000/915532 | Loss: 6.474433575931471e-06\n",
      "Epoch 1/1 | Step 92000/915532 | Loss: 6.407495334315172e-07\n",
      "Epoch 1/1 | Step 93000/915532 | Loss: 1.318743215961149e-06\n",
      "Epoch 1/1 | Step 94000/915532 | Loss: 1.5050113688630518e-06\n",
      "Epoch 1/1 | Step 95000/915532 | Loss: 1.356001916974492e-06\n",
      "Epoch 1/1 | Step 96000/915532 | Loss: 6.109469836701464e-07\n",
      "Epoch 1/1 | Step 97000/915532 | Loss: 2.533195697651536e-07\n",
      "Epoch 1/1 | Step 98000/915532 | Loss: 5.669660367857432e-06\n",
      "Epoch 1/1 | Step 99000/915532 | Loss: 2.235161218777648e-06\n",
      "Epoch 1/1 | Step 100000/915532 | Loss: 5.960457087894611e-07\n",
      "Epoch 1/1 | Step 101000/915532 | Loss: 7.50508188502863e-05\n",
      "Epoch 1/1 | Step 102000/915532 | Loss: 4.2468266769901675e-07\n",
      "Epoch 1/1 | Step 103000/915532 | Loss: 3.454662873991765e-05\n",
      "Epoch 1/1 | Step 104000/915532 | Loss: 5.23021662957035e-06\n",
      "Epoch 1/1 | Step 105000/915532 | Loss: 5.595178663497791e-06\n",
      "Epoch 1/1 | Step 106000/915532 | Loss: 3.330378604005091e-06\n",
      "Epoch 1/1 | Step 107000/915532 | Loss: 2.9876389362470945e-06\n",
      "Epoch 1/1 | Step 108000/915532 | Loss: 3.650781934538827e-07\n",
      "Epoch 1/1 | Step 109000/915532 | Loss: 3.7997932622602093e-07\n",
      "Epoch 1/1 | Step 110000/915532 | Loss: 2.831170604622457e-06\n",
      "Epoch 1/1 | Step 111000/915532 | Loss: 4.917379214930406e-07\n",
      "Epoch 1/1 | Step 112000/915532 | Loss: 1.1175866632129328e-07\n",
      "Epoch 1/1 | Step 113000/915532 | Loss: 3.576276412786683e-07\n",
      "Epoch 1/1 | Step 114000/915532 | Loss: 5.587922373706533e-07\n",
      "Epoch 1/1 | Step 115000/915532 | Loss: 2.0861600091848231e-07\n",
      "Epoch 1/1 | Step 116000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 117000/915532 | Loss: 2.3841828067361348e-07\n",
      "Epoch 1/1 | Step 118000/915532 | Loss: 1.0125078915734775e-05\n",
      "Epoch 1/1 | Step 119000/915532 | Loss: 6.413218943635002e-05\n",
      "Epoch 1/1 | Step 120000/915532 | Loss: 2.1321633539628237e-05\n",
      "Epoch 1/1 | Step 121000/915532 | Loss: 7.152536909416085e-07\n",
      "Epoch 1/1 | Step 122000/915532 | Loss: 4.731036824523471e-06\n",
      "Epoch 1/1 | Step 123000/915532 | Loss: 4.3958385731457383e-07\n",
      "Epoch 1/1 | Step 124000/915532 | Loss: 5.5579098443558905e-06\n",
      "Epoch 1/1 | Step 125000/915532 | Loss: 6.988276709307684e-06\n",
      "Epoch 1/1 | Step 126000/915532 | Loss: 3.471018499112688e-05\n",
      "Epoch 1/1 | Step 127000/915532 | Loss: 2.2053504835639615e-06\n",
      "Epoch 1/1 | Step 128000/915532 | Loss: 5.364410071706516e-07\n",
      "Epoch 1/1 | Step 129000/915532 | Loss: 7.301549658222939e-07\n",
      "Epoch 1/1 | Step 130000/915532 | Loss: 1.3411042232291948e-07\n",
      "Epoch 1/1 | Step 131000/915532 | Loss: 2.8013766950607533e-06\n",
      "Epoch 1/1 | Step 132000/915532 | Loss: 7.286479103640886e-06\n",
      "Epoch 1/1 | Step 133000/915532 | Loss: 8.791678851594042e-07\n",
      "Epoch 1/1 | Step 134000/915532 | Loss: 2.7269015845377e-06\n",
      "Epoch 1/1 | Step 135000/915532 | Loss: 3.740145984920673e-06\n",
      "Epoch 1/1 | Step 136000/915532 | Loss: 6.482000571850222e-07\n",
      "Epoch 1/1 | Step 137000/915532 | Loss: 9.461638910579495e-06\n",
      "Epoch 1/1 | Step 138000/915532 | Loss: 3.501771743685822e-07\n",
      "Epoch 1/1 | Step 139000/915532 | Loss: 6.10947154200403e-07\n",
      "Epoch 1/1 | Step 140000/915532 | Loss: 1.6316673736582743e-06\n",
      "Epoch 1/1 | Step 141000/915532 | Loss: 1.4534984984493349e-05\n",
      "Epoch 1/1 | Step 142000/915532 | Loss: 1.4156013321553473e-06\n",
      "Epoch 1/1 | Step 143000/915532 | Loss: 6.332985549306613e-07\n",
      "Epoch 1/1 | Step 144000/915532 | Loss: 7.376065696007572e-07\n",
      "Epoch 1/1 | Step 145000/915532 | Loss: 3.4197760214738082e-06\n",
      "Epoch 1/1 | Step 146000/915532 | Loss: 3.0547363394362037e-07\n",
      "Epoch 1/1 | Step 147000/915532 | Loss: 2.764145847322652e-06\n",
      "Epoch 1/1 | Step 148000/915532 | Loss: 2.0762134226970375e-05\n",
      "Epoch 1/1 | Step 149000/915532 | Loss: 9.238710845238529e-07\n",
      "Epoch 1/1 | Step 150000/915532 | Loss: 5.066389690000506e-07\n",
      "Epoch 1/1 | Step 151000/915532 | Loss: 1.0244281838822644e-05\n",
      "Epoch 1/1 | Step 152000/915532 | Loss: 8.910488759283908e-06\n",
      "Epoch 1/1 | Step 153000/915532 | Loss: 9.536734069115482e-07\n",
      "Epoch 1/1 | Step 154000/915532 | Loss: 0.0005957643734291196\n",
      "Epoch 1/1 | Step 155000/915532 | Loss: 4.209482995065628e-06\n",
      "Epoch 1/1 | Step 156000/915532 | Loss: 1.5124596757232212e-06\n",
      "Epoch 1/1 | Step 157000/915532 | Loss: 1.2918406355311163e-05\n",
      "Epoch 1/1 | Step 158000/915532 | Loss: 3.322895736346254e-06\n",
      "Epoch 1/1 | Step 159000/915532 | Loss: 1.505014097347157e-06\n",
      "Epoch 1/1 | Step 160000/915532 | Loss: 3.65078278719011e-07\n",
      "Epoch 1/1 | Step 161000/915532 | Loss: 3.054736623653298e-07\n",
      "Epoch 1/1 | Step 162000/915532 | Loss: 4.917378646496218e-07\n",
      "Epoch 1/1 | Step 163000/915532 | Loss: 7.144897608668543e-06\n",
      "Epoch 1/1 | Step 164000/915532 | Loss: 7.003523023740854e-07\n",
      "Epoch 1/1 | Step 165000/915532 | Loss: 3.650781650321733e-07\n",
      "Epoch 1/1 | Step 166000/915532 | Loss: 2.1904506866121665e-06\n",
      "Epoch 1/1 | Step 167000/915532 | Loss: 2.9205771170381922e-06\n",
      "Epoch 1/1 | Step 168000/915532 | Loss: 1.7359667481287033e-06\n",
      "Epoch 1/1 | Step 169000/915532 | Loss: 1.892430418592994e-06\n",
      "Epoch 1/1 | Step 170000/915532 | Loss: 1.7806812593335053e-06\n",
      "Epoch 1/1 | Step 171000/915532 | Loss: 4.2468286665098276e-07\n",
      "Epoch 1/1 | Step 172000/915532 | Loss: 1.199540747620631e-06\n",
      "Epoch 1/1 | Step 173000/915532 | Loss: 2.011656192735245e-07\n",
      "Epoch 1/1 | Step 174000/915532 | Loss: 1.8626434439283912e-07\n",
      "Epoch 1/1 | Step 175000/915532 | Loss: 1.4944175745768007e-05\n",
      "Epoch 1/1 | Step 176000/915532 | Loss: 6.325387403194327e-06\n",
      "Epoch 1/1 | Step 177000/915532 | Loss: 3.3527601317473454e-07\n",
      "Epoch 1/1 | Step 178000/915532 | Loss: 4.842873408961168e-07\n",
      "Epoch 1/1 | Step 179000/915532 | Loss: 7.599572313665703e-07\n",
      "Epoch 1/1 | Step 180000/915532 | Loss: 8.940696005765858e-08\n",
      "Epoch 1/1 | Step 181000/915532 | Loss: 2.905724443280633e-07\n",
      "Epoch 1/1 | Step 182000/915532 | Loss: 1.1175869474300271e-07\n",
      "Epoch 1/1 | Step 183000/915532 | Loss: 4.991881041860324e-07\n",
      "Epoch 1/1 | Step 184000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 185000/915532 | Loss: 1.788138632718983e-07\n",
      "Epoch 1/1 | Step 186000/915532 | Loss: 1.9371493920061766e-07\n",
      "Epoch 1/1 | Step 187000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 188000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 189000/915532 | Loss: 3.12923674528065e-07\n",
      "Epoch 1/1 | Step 190000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 191000/915532 | Loss: 2.5853207716863835e-06\n",
      "Epoch 1/1 | Step 192000/915532 | Loss: 2.428874950055615e-06\n",
      "Epoch 1/1 | Step 193000/915532 | Loss: 7.063001703500049e-06\n",
      "Epoch 1/1 | Step 194000/915532 | Loss: 5.259988029138185e-06\n",
      "Epoch 1/1 | Step 195000/915532 | Loss: 6.78002152199042e-07\n",
      "Epoch 1/1 | Step 196000/915532 | Loss: 1.1562873623915948e-05\n",
      "Epoch 1/1 | Step 197000/915532 | Loss: 8.053583769651596e-06\n",
      "Epoch 1/1 | Step 198000/915532 | Loss: 5.885955829398881e-07\n",
      "Epoch 1/1 | Step 199000/915532 | Loss: 9.611214863980422e-07\n",
      "Epoch 1/1 | Step 200000/915532 | Loss: 1.2740440524794394e-06\n",
      "Epoch 1/1 | Step 201000/915532 | Loss: 2.011656192735245e-07\n",
      "Epoch 1/1 | Step 202000/915532 | Loss: 1.1175783356520697e-06\n",
      "Epoch 1/1 | Step 203000/915532 | Loss: 8.195619329853798e-07\n",
      "Epoch 1/1 | Step 204000/915532 | Loss: 3.941241175198229e-06\n",
      "Epoch 1/1 | Step 205000/915532 | Loss: 2.384183943604512e-07\n",
      "Epoch 1/1 | Step 206000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 207000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 208000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 209000/915532 | Loss: 9.1641646804419e-07\n",
      "Epoch 1/1 | Step 210000/915532 | Loss: 9.685753354915505e-08\n",
      "Epoch 1/1 | Step 211000/915532 | Loss: 4.470348002882929e-08\n",
      "Epoch 1/1 | Step 212000/915532 | Loss: 3.129237313714839e-07\n",
      "Epoch 1/1 | Step 213000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 214000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 215000/915532 | Loss: 1.6391264523463178e-07\n",
      "Epoch 1/1 | Step 216000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 217000/915532 | Loss: 1.4901144140821998e-07\n",
      "Epoch 1/1 | Step 218000/915532 | Loss: 4.581943358061835e-06\n",
      "Epoch 1/1 | Step 219000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 220000/915532 | Loss: 7.823081205060589e-07\n",
      "Epoch 1/1 | Step 221000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 222000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 223000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 224000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 225000/915532 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 226000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 227000/915532 | Loss: 3.3527555842738366e-07\n",
      "Epoch 1/1 | Step 228000/915532 | Loss: 1.9371481130292523e-07\n",
      "Epoch 1/1 | Step 229000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 230000/915532 | Loss: 2.155361471523065e-05\n",
      "Epoch 1/1 | Step 231000/915532 | Loss: 4.999194516130956e-06\n",
      "Epoch 1/1 | Step 232000/915532 | Loss: 2.3022005279926816e-06\n",
      "Epoch 1/1 | Step 233000/915532 | Loss: 1.2367914905553334e-06\n",
      "Epoch 1/1 | Step 234000/915532 | Loss: 9.76024239207618e-07\n",
      "Epoch 1/1 | Step 235000/915532 | Loss: 1.8924351934401784e-06\n",
      "Epoch 1/1 | Step 236000/915532 | Loss: 8.344612751898239e-07\n",
      "Epoch 1/1 | Step 237000/915532 | Loss: 6.183970526763005e-07\n",
      "Epoch 1/1 | Step 238000/915532 | Loss: 7.450579886381092e-08\n",
      "Epoch 1/1 | Step 239000/915532 | Loss: 3.5017683330806904e-07\n",
      "Epoch 1/1 | Step 240000/915532 | Loss: 5.513421115210804e-07\n",
      "Epoch 1/1 | Step 241000/915532 | Loss: 5.587930331785174e-07\n",
      "Epoch 1/1 | Step 242000/915532 | Loss: 2.3096771428754437e-07\n",
      "Epoch 1/1 | Step 243000/915532 | Loss: 1.1697327408910496e-06\n",
      "Epoch 1/1 | Step 244000/915532 | Loss: 2.7864846288139233e-06\n",
      "Epoch 1/1 | Step 245000/915532 | Loss: 1.6391271628890536e-07\n",
      "Epoch 1/1 | Step 246000/915532 | Loss: 3.501768901514879e-07\n",
      "Epoch 1/1 | Step 247000/915532 | Loss: 7.301538857973355e-07\n",
      "Epoch 1/1 | Step 248000/915532 | Loss: 9.685686563898344e-07\n",
      "Epoch 1/1 | Step 249000/915532 | Loss: 1.4901154088420299e-07\n",
      "Epoch 1/1 | Step 250000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 251000/915532 | Loss: 3.725282056166179e-07\n",
      "Epoch 1/1 | Step 252000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 253000/915532 | Loss: 4.0233084064311697e-07\n",
      "Epoch 1/1 | Step 254000/915532 | Loss: 2.168098035326693e-06\n",
      "Epoch 1/1 | Step 255000/915532 | Loss: 3.0547352025678265e-07\n",
      "Epoch 1/1 | Step 256000/915532 | Loss: 9.685753354915505e-08\n",
      "Epoch 1/1 | Step 257000/915532 | Loss: 3.1292415769712534e-07\n",
      "Epoch 1/1 | Step 258000/915532 | Loss: 4.842865450882528e-07\n",
      "Epoch 1/1 | Step 259000/915532 | Loss: 2.756713683993439e-07\n",
      "Epoch 1/1 | Step 260000/915532 | Loss: 2.8312194899626775e-07\n",
      "Epoch 1/1 | Step 261000/915532 | Loss: 1.7881390590446244e-07\n",
      "Epoch 1/1 | Step 262000/915532 | Loss: 9.089663990380359e-07\n",
      "Epoch 1/1 | Step 263000/915532 | Loss: 2.6822073095900123e-07\n",
      "Epoch 1/1 | Step 264000/915532 | Loss: 2.9802305334669654e-07\n",
      "Epoch 1/1 | Step 265000/915532 | Loss: 5.84163935855031e-05\n",
      "Epoch 1/1 | Step 266000/915532 | Loss: 9.811788913793862e-06\n",
      "Epoch 1/1 | Step 267000/915532 | Loss: 3.352753310537082e-07\n",
      "Epoch 1/1 | Step 268000/915532 | Loss: 1.9371493920061766e-07\n",
      "Epoch 1/1 | Step 269000/915532 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 270000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 271000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 272000/915532 | Loss: 8.195635814445268e-08\n",
      "Epoch 1/1 | Step 273000/915532 | Loss: 4.410590918269008e-06\n",
      "Epoch 1/1 | Step 274000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 275000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 276000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 277000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 278000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 279000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 280000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 281000/915532 | Loss: 4.097812507097842e-07\n",
      "Epoch 1/1 | Step 282000/915532 | Loss: 4.023307837996981e-07\n",
      "Epoch 1/1 | Step 283000/915532 | Loss: 8.940693163594915e-08\n",
      "Epoch 1/1 | Step 284000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 285000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 286000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 287000/915532 | Loss: 8.195634393359796e-08\n",
      "Epoch 1/1 | Step 288000/915532 | Loss: 1.5273581084329635e-06\n",
      "Epoch 1/1 | Step 289000/915532 | Loss: 1.616766439838102e-06\n",
      "Epoch 1/1 | Step 290000/915532 | Loss: 1.3336406254893518e-06\n",
      "Epoch 1/1 | Step 291000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 292000/915532 | Loss: 7.525050023104995e-07\n",
      "Epoch 1/1 | Step 293000/915532 | Loss: 1.5646212148112681e-07\n",
      "Epoch 1/1 | Step 294000/915532 | Loss: 1.639127447106148e-07\n",
      "Epoch 1/1 | Step 295000/915532 | Loss: 2.309677427092538e-07\n",
      "Epoch 1/1 | Step 296000/915532 | Loss: 5.498304744833149e-06\n",
      "Epoch 1/1 | Step 297000/915532 | Loss: 3.278254325778107e-07\n",
      "Epoch 1/1 | Step 298000/915532 | Loss: 0.005056146066635847\n",
      "Epoch 1/1 | Step 299000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 300000/915532 | Loss: 2.45868875481392e-07\n",
      "Epoch 1/1 | Step 301000/915532 | Loss: 1.4677513036076562e-06\n",
      "Epoch 1/1 | Step 302000/915532 | Loss: 0.0002539182605687529\n",
      "Epoch 1/1 | Step 303000/915532 | Loss: 2.754112210823223e-05\n",
      "Epoch 1/1 | Step 304000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 305000/915532 | Loss: 1.9669403172883904e-06\n",
      "Epoch 1/1 | Step 306000/915532 | Loss: 0.5438662171363831\n",
      "Epoch 1/1 | Step 307000/915532 | Loss: 3.553877149897744e-06\n",
      "Epoch 1/1 | Step 308000/915532 | Loss: 0.009367071092128754\n",
      "Epoch 1/1 | Step 309000/915532 | Loss: 5.610245352727361e-06\n",
      "Epoch 1/1 | Step 310000/915532 | Loss: 6.623495210078545e-06\n",
      "Epoch 1/1 | Step 311000/915532 | Loss: 0.0005284565268084407\n",
      "Epoch 1/1 | Step 312000/915532 | Loss: 3.94132393921609e-06\n",
      "Epoch 1/1 | Step 313000/915532 | Loss: 1.6689264157321304e-06\n",
      "Epoch 1/1 | Step 314000/915532 | Loss: 1.803028453650768e-06\n",
      "Epoch 1/1 | Step 315000/915532 | Loss: 3.352758994878968e-07\n",
      "Epoch 1/1 | Step 316000/915532 | Loss: 1.3411007557806442e-06\n",
      "Epoch 1/1 | Step 317000/915532 | Loss: 1.1465938769106288e-05\n",
      "Epoch 1/1 | Step 318000/915532 | Loss: 4.6193574121389247e-07\n",
      "Epoch 1/1 | Step 319000/915532 | Loss: 5.438920993583451e-07\n",
      "Epoch 1/1 | Step 320000/915532 | Loss: 2.0116506220801966e-06\n",
      "Epoch 1/1 | Step 321000/915532 | Loss: 2.5927786282409215e-06\n",
      "Epoch 1/1 | Step 322000/915532 | Loss: 0.0003355265944264829\n",
      "Epoch 1/1 | Step 323000/915532 | Loss: 2.8312063022895018e-06\n",
      "Epoch 1/1 | Step 324000/915532 | Loss: 5.5356040320475586e-06\n",
      "Epoch 1/1 | Step 325000/915532 | Loss: 9.051829692907631e-06\n",
      "Epoch 1/1 | Step 326000/915532 | Loss: 1.8998781570189749e-06\n",
      "Epoch 1/1 | Step 327000/915532 | Loss: 2.011643118748907e-06\n",
      "Epoch 1/1 | Step 328000/915532 | Loss: 1.6242171341218636e-06\n",
      "Epoch 1/1 | Step 329000/915532 | Loss: 1.5944183360261377e-06\n",
      "Epoch 1/1 | Step 330000/915532 | Loss: 2.928013373093563e-06\n",
      "Epoch 1/1 | Step 331000/915532 | Loss: 0.0015699926298111677\n",
      "Epoch 1/1 | Step 332000/915532 | Loss: 4.1723117760739115e-07\n",
      "Epoch 1/1 | Step 333000/915532 | Loss: 5.81144320221938e-07\n",
      "Epoch 1/1 | Step 334000/915532 | Loss: 9.462219736633415e-07\n",
      "Epoch 1/1 | Step 335000/915532 | Loss: 3.0174708172125975e-06\n",
      "Epoch 1/1 | Step 336000/915532 | Loss: 2.6225843612337485e-06\n",
      "Epoch 1/1 | Step 337000/915532 | Loss: 7.450570933542622e-07\n",
      "Epoch 1/1 | Step 338000/915532 | Loss: 8.046617381296528e-07\n",
      "Epoch 1/1 | Step 339000/915532 | Loss: 2.9876503049308667e-06\n",
      "Epoch 1/1 | Step 340000/915532 | Loss: 5.587918394667213e-07\n",
      "Epoch 1/1 | Step 341000/915532 | Loss: 2.7567119786908734e-07\n",
      "Epoch 1/1 | Step 342000/915532 | Loss: 1.341103512686459e-07\n",
      "Epoch 1/1 | Step 343000/915532 | Loss: 5.736923185395426e-07\n",
      "Epoch 1/1 | Step 344000/915532 | Loss: 7.152236321417149e-06\n",
      "Epoch 1/1 | Step 345000/915532 | Loss: 1.7136332530753862e-07\n",
      "Epoch 1/1 | Step 346000/915532 | Loss: 2.0055584172951058e-05\n",
      "Epoch 1/1 | Step 347000/915532 | Loss: 2.3543611860077363e-06\n",
      "Epoch 1/1 | Step 348000/915532 | Loss: 1.8775334638121421e-06\n",
      "Epoch 1/1 | Step 349000/915532 | Loss: 1.9594913283071946e-06\n",
      "Epoch 1/1 | Step 350000/915532 | Loss: 1.7136328267497447e-07\n",
      "Epoch 1/1 | Step 351000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 352000/915532 | Loss: 7.517192443629028e-06\n",
      "Epoch 1/1 | Step 353000/915532 | Loss: 2.682206172721635e-07\n",
      "Epoch 1/1 | Step 354000/915532 | Loss: 1.4901156930591242e-07\n",
      "Epoch 1/1 | Step 355000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 356000/915532 | Loss: 4.097808528058522e-07\n",
      "Epoch 1/1 | Step 357000/915532 | Loss: 2.6077020720549626e-07\n",
      "Epoch 1/1 | Step 358000/915532 | Loss: 1.5502790120081045e-05\n",
      "Epoch 1/1 | Step 359000/915532 | Loss: 8.940695295223122e-08\n",
      "Epoch 1/1 | Step 360000/915532 | Loss: 2.54386159213027e-05\n",
      "Epoch 1/1 | Step 361000/915532 | Loss: 2.2351734685344127e-07\n",
      "Epoch 1/1 | Step 362000/915532 | Loss: 2.2351738948600541e-07\n",
      "Epoch 1/1 | Step 363000/915532 | Loss: 2.3096791323951038e-07\n",
      "Epoch 1/1 | Step 364000/915532 | Loss: 3.1068184398463927e-06\n",
      "Epoch 1/1 | Step 365000/915532 | Loss: 1.2665981330428622e-07\n",
      "Epoch 1/1 | Step 366000/915532 | Loss: 1.3187420790927717e-06\n",
      "Epoch 1/1 | Step 367000/915532 | Loss: 9.462180514674401e-07\n",
      "Epoch 1/1 | Step 368000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 369000/915532 | Loss: 5.960463411724959e-08\n",
      "Epoch 1/1 | Step 370000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 371000/915532 | Loss: 3.725289587919178e-08\n",
      "Epoch 1/1 | Step 372000/915532 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 373000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 374000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 375000/915532 | Loss: 5.215404996761208e-08\n",
      "Epoch 1/1 | Step 376000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 377000/915532 | Loss: 3.725194801518228e-06\n",
      "Epoch 1/1 | Step 378000/915532 | Loss: 7.376038979600708e-07\n",
      "Epoch 1/1 | Step 379000/915532 | Loss: 1.5646209305941738e-07\n",
      "Epoch 1/1 | Step 380000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 381000/915532 | Loss: 3.725289943190546e-08\n",
      "Epoch 1/1 | Step 382000/915532 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 383000/915532 | Loss: 5.9604641222676946e-08\n",
      "Epoch 1/1 | Step 384000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 385000/915532 | Loss: 2.0861611460532004e-07\n",
      "Epoch 1/1 | Step 386000/915532 | Loss: 7.450579175838357e-08\n",
      "Epoch 1/1 | Step 387000/915532 | Loss: 9.685751223287298e-08\n",
      "Epoch 1/1 | Step 388000/915532 | Loss: 5.9604641222676946e-08\n",
      "Epoch 1/1 | Step 389000/915532 | Loss: 2.1606675204566272e-07\n",
      "Epoch 1/1 | Step 390000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 391000/915532 | Loss: 1.1175865211043856e-07\n",
      "Epoch 1/1 | Step 392000/915532 | Loss: 2.9802315282267955e-08\n",
      "Epoch 1/1 | Step 393000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 394000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 395000/915532 | Loss: 7.450576333667414e-08\n",
      "Epoch 1/1 | Step 396000/915532 | Loss: 1.4901160305669237e-08\n",
      "Epoch 1/1 | Step 397000/915532 | Loss: 1.5646216411369096e-07\n",
      "Epoch 1/1 | Step 398000/915532 | Loss: 2.3692768991168123e-06\n",
      "Epoch 1/1 | Step 399000/915532 | Loss: 3.0099859031906817e-06\n",
      "Epoch 1/1 | Step 400000/915532 | Loss: 5.304720161802834e-06\n",
      "Epoch 1/1 | Step 401000/915532 | Loss: 2.0861552911810577e-06\n",
      "Epoch 1/1 | Step 402000/915532 | Loss: 1.2963990911885048e-06\n",
      "Epoch 1/1 | Step 403000/915532 | Loss: 1.0654310926838662e-06\n",
      "Epoch 1/1 | Step 404000/915532 | Loss: 1.6798881915747188e-05\n",
      "Epoch 1/1 | Step 405000/915532 | Loss: 1.3783528629573993e-06\n",
      "Epoch 1/1 | Step 406000/915532 | Loss: 4.3213333356106887e-07\n",
      "Epoch 1/1 | Step 407000/915532 | Loss: 2.190456143580377e-06\n",
      "Epoch 1/1 | Step 408000/915532 | Loss: 1.4379509138962021e-06\n",
      "Epoch 1/1 | Step 409000/915532 | Loss: 1.2442445722626871e-06\n",
      "Epoch 1/1 | Step 410000/915532 | Loss: 5.215403575675737e-07\n",
      "Epoch 1/1 | Step 411000/915532 | Loss: 1.117583906307118e-06\n",
      "Epoch 1/1 | Step 412000/915532 | Loss: 1.192092469182171e-07\n",
      "Epoch 1/1 | Step 413000/915532 | Loss: 2.011656192735245e-07\n",
      "Epoch 1/1 | Step 414000/915532 | Loss: 2.980229965032777e-07\n",
      "Epoch 1/1 | Step 415000/915532 | Loss: 5.513423957381747e-07\n",
      "Epoch 1/1 | Step 416000/915532 | Loss: 2.6822067411558237e-07\n",
      "Epoch 1/1 | Step 417000/915532 | Loss: 1.415610029198433e-07\n",
      "Epoch 1/1 | Step 418000/915532 | Loss: 1.713632116207009e-07\n",
      "Epoch 1/1 | Step 419000/915532 | Loss: 1.1175868053214799e-07\n",
      "Epoch 1/1 | Step 420000/915532 | Loss: 2.980229965032777e-07\n",
      "Epoch 1/1 | Step 421000/915532 | Loss: 2.0116546295412263e-07\n",
      "Epoch 1/1 | Step 422000/915532 | Loss: 3.2781847494334215e-06\n",
      "Epoch 1/1 | Step 423000/915532 | Loss: 3.278252336258447e-07\n",
      "Epoch 1/1 | Step 424000/915532 | Loss: 1.1175868053214799e-07\n",
      "Epoch 1/1 | Step 425000/915532 | Loss: 0.0\n",
      "Epoch 1/1 | Step 426000/915532 | Loss: 7.078047019604128e-07\n",
      "Epoch 1/1 | Step 427000/915532 | Loss: 4.470344947549165e-07\n",
      "Epoch 1/1 | Step 428000/915532 | Loss: 2.6970601538778283e-06\n",
      "Epoch 1/1 | Step 429000/915532 | Loss: 8.031283869058825e-06\n",
      "Epoch 1/1 | Step 430000/915532 | Loss: 3.129242145405442e-07\n",
      "Epoch 1/1 | Step 431000/915532 | Loss: 5.349364073481411e-06\n",
      "Epoch 1/1 | Step 432000/915532 | Loss: 1.041526411427185e-05\n",
      "Epoch 1/1 | Step 433000/915532 | Loss: 1.4901148404078413e-07\n",
      "Epoch 1/1 | Step 434000/915532 | Loss: 4.365890617918922e-06\n",
      "Epoch 1/1 | Step 435000/915532 | Loss: 5.096106178825721e-06\n",
      "Epoch 1/1 | Step 436000/915532 | Loss: 1.6018623227864737e-06\n",
      "Epoch 1/1 | Step 437000/915532 | Loss: 2.078700617857976e-06\n",
      "Epoch 1/1 | Step 438000/915532 | Loss: 1.825377921704785e-06\n",
      "Epoch 1/1 | Step 439000/915532 | Loss: 2.011654345324132e-07\n",
      "Epoch 1/1 | Step 440000/915532 | Loss: 4.023304143174755e-07\n",
      "Epoch 1/1 | Step 441000/915532 | Loss: 7.748578241262294e-07\n",
      "Epoch 1/1 | Step 442000/915532 | Loss: 3.948795779251668e-07\n",
      "Epoch 1/1 | Step 443000/915532 | Loss: 1.1324825663905358e-06\n",
      "Epoch 1/1 | Step 444000/915532 | Loss: 2.384184938364342e-07\n",
      "Epoch 1/1 | Step 445000/915532 | Loss: 2.0712343484774465e-06\n",
      "Epoch 1/1 | Step 446000/915532 | Loss: 4.708650976681383e-06\n",
      "Epoch 1/1 | Step 447000/915532 | Loss: 5.960463766996327e-08\n",
      "Epoch 1/1 | Step 448000/915532 | Loss: 1.8626444386882213e-07\n",
      "Epoch 1/1 | Step 449000/915532 | Loss: 3.2782497783045983e-07\n",
      "Epoch 1/1 | Step 450000/915532 | Loss: 3.9735186874167994e-05\n",
      "Epoch 1/1 | Step 451000/915532 | Loss: 9.313158670920529e-07\n",
      "Epoch 1/1 | Step 452000/915532 | Loss: 3.203747667157586e-07\n",
      "Epoch 1/1 | Step 453000/915532 | Loss: 2.9802320611338473e-08\n",
      "Epoch 1/1 | Step 454000/915532 | Loss: 1.937147970920705e-07\n",
      "Epoch 1/1 | Step 455000/915532 | Loss: 7.003523592175043e-07\n",
      "Epoch 1/1 | Step 456000/915532 | Loss: 1.3411042232291948e-07\n",
      "Epoch 1/1 | Step 457000/915532 | Loss: 8.940694584680386e-08\n",
      "Epoch 1/1 | Step 458000/915532 | Loss: 1.1175867342672063e-07\n",
      "Epoch 1/1 | Step 459000/915532 | Loss: 6.481994887508336e-07\n",
      "Epoch 1/1 | Step 460000/915532 | Loss: 8.195636524988004e-08\n",
      "Epoch 1/1 | Step 461000/915532 | Loss: 1.7881384906104358e-07\n",
      "Epoch 1/1 | Step 462000/915532 | Loss: 4.172320871020929e-07\n",
      "Epoch 1/1 | Step 463000/915532 | Loss: 2.9802308176840597e-07\n",
      "Epoch 1/1 | Step 464000/915532 | Loss: 1.9371485393548937e-07\n",
      "Epoch 1/1 | Step 465000/915532 | Loss: 7.301530331460526e-07\n",
      "Epoch 1/1 | Step 466000/915532 | Loss: 6.779995374017744e-07\n",
      "Epoch 1/1 | Step 467000/915532 | Loss: 2.235170910580564e-07\n",
      "Epoch 1/1 | Step 468000/915532 | Loss: 5.736928869737312e-07\n",
      "Epoch 1/1 | Step 469000/915532 | Loss: 2.2351738238057806e-08\n",
      "Epoch 1/1 | Step 470000/915532 | Loss: 9.313206419392372e-07\n",
      "Epoch 1/1 | Step 471000/915532 | Loss: 2.6077000825353025e-07\n",
      "Epoch 1/1 | Step 472000/915532 | Loss: 5.513423388947558e-07\n",
      "Epoch 1/1 | Step 473000/915532 | Loss: 1.778953810571693e-05\n",
      "Epoch 1/1 | Step 474000/915532 | Loss: 1.4156097449813387e-07\n",
      "Epoch 1/1 | Step 475000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 476000/915532 | Loss: 4.172318881501269e-07\n",
      "Epoch 1/1 | Step 477000/915532 | Loss: 6.705521116145974e-08\n",
      "Epoch 1/1 | Step 478000/915532 | Loss: 1.5646213569198153e-07\n",
      "Epoch 1/1 | Step 479000/915532 | Loss: 1.7881382063933415e-07\n",
      "Epoch 1/1 | Step 480000/915532 | Loss: 1.1175868053214799e-07\n",
      "Epoch 1/1 | Step 481000/915532 | Loss: 8.940695295223122e-08\n",
      "Epoch 1/1 | Step 482000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 483000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 484000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 485000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 486000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 487000/915532 | Loss: 1.4901159417490817e-08\n",
      "Epoch 1/1 | Step 488000/915532 | Loss: 4.470348002882929e-08\n",
      "Epoch 1/1 | Step 489000/915532 | Loss: 1.1175868053214799e-07\n",
      "Epoch 1/1 | Step 490000/915532 | Loss: 2.5927520255208947e-06\n",
      "Epoch 1/1 | Step 491000/915532 | Loss: 1.937148823571988e-07\n",
      "Epoch 1/1 | Step 492000/915532 | Loss: 2.29474085244874e-06\n",
      "Epoch 1/1 | Step 493000/915532 | Loss: 8.761646313359961e-06\n",
      "Epoch 1/1 | Step 494000/915532 | Loss: 3.5017637856071815e-07\n",
      "Epoch 1/1 | Step 495000/915532 | Loss: 1.5795117178640794e-06\n",
      "Epoch 1/1 | Step 496000/915532 | Loss: 3.5762755601354e-07\n",
      "Epoch 1/1 | Step 497000/915532 | Loss: 1.1548349903023336e-06\n",
      "Epoch 1/1 | Step 498000/915532 | Loss: 1.0459769328008406e-05\n",
      "Epoch 1/1 | Step 499000/915532 | Loss: 1.1175869474300271e-07\n",
      "Epoch 1/1 | Step 500000/915532 | Loss: 8.940695295223122e-08\n",
      "Epoch 1/1 | Step 501000/915532 | Loss: 2.9802318834981634e-08\n",
      "Epoch 1/1 | Step 502000/915532 | Loss: 5.215405707303944e-08\n",
      "Epoch 1/1 | Step 503000/915532 | Loss: 2.7566559310798766e-06\n",
      "Epoch 1/1 | Step 504000/915532 | Loss: 1.5646213569198153e-07\n",
      "Epoch 1/1 | Step 505000/915532 | Loss: 5.960463056453591e-08\n",
      "Epoch 1/1 | Step 506000/915532 | Loss: 1.7881383485018887e-07\n",
      "Epoch 1/1 | Step 507000/915532 | Loss: 8.195603982130706e-07\n",
      "Epoch 1/1 | Step 508000/915532 | Loss: 5.215406062575312e-08\n",
      "Epoch 1/1 | Step 509000/915532 | Loss: 3.129242145405442e-07\n",
      "Epoch 1/1 | Step 510000/915532 | Loss: 3.4300395782338455e-05\n",
      "Epoch 1/1 | Step 511000/915532 | Loss: 3.620700954343192e-05\n",
      "Epoch 1/1 | Step 512000/915532 | Loss: 6.548791589011671e-06\n",
      "Epoch 1/1 | Step 513000/915532 | Loss: 7.919498784758616e-06\n",
      "Epoch 1/1 | Step 514000/915532 | Loss: 0.00019083758525084704\n",
      "Epoch 1/1 | Step 515000/915532 | Loss: 1.5422651813423727e-06\n",
      "Epoch 1/1 | Step 516000/915532 | Loss: 5.140897769706498e-07\n",
      "Epoch 1/1 | Step 517000/915532 | Loss: 7.376061716968252e-07\n",
      "Epoch 1/1 | Step 518000/915532 | Loss: 5.736944217460405e-07\n",
      "Epoch 1/1 | Step 519000/915532 | Loss: 2.533195697651536e-07\n",
      "Epoch 1/1 | Step 520000/915532 | Loss: 9.46222598940949e-07\n",
      "Epoch 1/1 | Step 521000/915532 | Loss: 3.7997943991285865e-07\n",
      "Epoch 1/1 | Step 522000/915532 | Loss: 4.738400093629025e-06\n",
      "Epoch 1/1 | Step 523000/915532 | Loss: 1.169737515738234e-06\n",
      "Epoch 1/1 | Step 524000/915532 | Loss: 2.317113967365003e-06\n",
      "Epoch 1/1 | Step 525000/915532 | Loss: 5.058900114818243e-06\n",
      "Epoch 1/1 | Step 526000/915532 | Loss: 2.607702356272057e-07\n",
      "Epoch 1/1 | Step 527000/915532 | Loss: 2.512064020265825e-05\n",
      "Epoch 1/1 | Step 528000/915532 | Loss: 3.6432988963497337e-06\n",
      "Epoch 1/1 | Step 529000/915532 | Loss: 2.391626139797154e-06\n",
      "Epoch 1/1 | Step 530000/915532 | Loss: 3.501770606817445e-07\n",
      "Epoch 1/1 | Step 531000/915532 | Loss: 2.153202785848407e-06\n",
      "Epoch 1/1 | Step 532000/915532 | Loss: 4.2468218452995643e-07\n",
      "Epoch 1/1 | Step 533000/915532 | Loss: 2.385966945439577e-05\n",
      "Epoch 1/1 | Step 534000/915532 | Loss: 1.6242217952822102e-06\n",
      "Epoch 1/1 | Step 535000/915532 | Loss: 5.945312750554876e-06\n",
      "Epoch 1/1 | Step 536000/915532 | Loss: 1.6391223880418693e-06\n",
      "Epoch 1/1 | Step 537000/915532 | Loss: 6.258204848563764e-06\n",
      "Epoch 1/1 | Step 538000/915532 | Loss: 4.023309827516641e-07\n",
      "Epoch 1/1 | Step 539000/915532 | Loss: 2.52524532697862e-05\n",
      "Epoch 1/1 | Step 540000/915532 | Loss: 1.3485433782989276e-06\n",
      "Epoch 1/1 | Step 541000/915532 | Loss: 2.4254677555290982e-05\n",
      "Epoch 1/1 | Step 542000/915532 | Loss: 1.2083679393981583e-05\n",
      "Epoch 1/1 | Step 543000/915532 | Loss: 3.3378141779394355e-06\n",
      "Epoch 1/1 | Step 544000/915532 | Loss: 1.0430811414607888e-07\n",
      "Epoch 1/1 | Step 545000/915532 | Loss: 3.300529215266579e-06\n",
      "Epoch 1/1 | Step 546000/915532 | Loss: 2.890793894039234e-06\n",
      "Epoch 1/1 | Step 547000/915532 | Loss: 3.6507813661046384e-07\n",
      "Epoch 1/1 | Step 548000/915532 | Loss: 1.8253776943311095e-06\n",
      "Epoch 1/1 | Step 549000/915532 | Loss: 1.490114556190747e-07\n",
      "Epoch 1/1 | Step 550000/915532 | Loss: 3.7997935464773036e-07\n",
      "Epoch 1/1 | Step 551000/915532 | Loss: 4.76836760299193e-07\n",
      "Epoch 1/1 | Step 552000/915532 | Loss: 5.811437517877494e-07\n",
      "Epoch 1/1 | Step 553000/915532 | Loss: 3.7252823403832735e-07\n",
      "Epoch 1/1 | Step 554000/915532 | Loss: 5.51342282051337e-07\n",
      "Epoch 1/1 | Step 555000/915532 | Loss: 1.7881382063933415e-07\n",
      "Epoch 1/1 | Step 556000/915532 | Loss: 1.4973952602304053e-05\n",
      "Epoch 1/1 | Step 557000/915532 | Loss: 4.2468275296414504e-07\n",
      "Epoch 1/1 | Step 558000/915532 | Loss: 6.258477469600621e-07\n",
      "Epoch 1/1 | Step 559000/915532 | Loss: 1.8626445807967684e-07\n",
      "Epoch 1/1 | Step 560000/915532 | Loss: 2.831215510923357e-07\n",
      "Epoch 1/1 | Step 561000/915532 | Loss: 5.438907919597114e-07\n",
      "Epoch 1/1 | Step 562000/915532 | Loss: 1.2665982751514093e-07\n",
      "Epoch 1/1 | Step 563000/915532 | Loss: 5.215394480728719e-07\n",
      "Epoch 1/1 | Step 564000/915532 | Loss: 1.9743849861697527e-06\n",
      "Epoch 1/1 | Step 565000/915532 | Loss: 4.470343810680788e-07\n",
      "Epoch 1/1 | Step 566000/915532 | Loss: 3.203746814506303e-07\n",
      "Epoch 1/1 | Step 567000/915532 | Loss: 1.4454046777245821e-06\n",
      "Epoch 1/1 | Step 568000/915532 | Loss: 8.866168172971811e-07\n",
      "Epoch 1/1 | Step 569000/915532 | Loss: 1.3411040811206476e-07\n",
      "Epoch 1/1 | Step 570000/915532 | Loss: 8.672240255691577e-06\n",
      "Epoch 1/1 | Step 571000/915532 | Loss: 2.1606675204566272e-07\n",
      "Epoch 1/1 | Step 572000/915532 | Loss: 1.192087779600115e-06\n",
      "Epoch 1/1 | Step 573000/915532 | Loss: 3.3825037917267764e-06\n",
      "Epoch 1/1 | Step 574000/915532 | Loss: 2.3022143977868836e-06\n",
      "Epoch 1/1 | Step 575000/915532 | Loss: 1.3411039390121005e-07\n",
      "Epoch 1/1 | Step 576000/915532 | Loss: 1.4156097449813387e-07\n",
      "Epoch 1/1 | Step 577000/915532 | Loss: 5.43891758297832e-07\n",
      "Epoch 1/1 | Step 578000/915532 | Loss: 9.685751223287298e-08\n",
      "Epoch 1/1 | Step 579000/915532 | Loss: 9.923742254613899e-06\n",
      "Epoch 1/1 | Step 580000/915532 | Loss: 2.4586898916822975e-07\n",
      "Epoch 1/1 | Step 581000/915532 | Loss: 7.450580152834618e-09\n",
      "Epoch 1/1 | Step 582000/915532 | Loss: 1.4901152667334827e-07\n",
      "Epoch 1/1 | Step 583000/915532 | Loss: 6.70552182668871e-08\n",
      "Epoch 1/1 | Step 584000/915532 | Loss: 2.2351740014414645e-08\n",
      "Epoch 1/1 | Step 585000/915532 | Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Backward pass and update\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader)} | Loss: {loss.item()}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}, Time Taken: {epoch_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(b_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "predictions_flat = predictions\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data = pd.DataFrame({\n",
    "    'Month': data.loc[test_idx, 'Month'].values, \n",
    "    'Place': data.loc[test_idx, 'place'].values,\n",
    "    'Crop' : data.loc[test_idx, 'Crop'].values,\n",
    "    'QueryType_code': predictions_flat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QueryType'] = data['QueryType'].astype('category')\n",
    "predicted_data['QueryType']=predicted_data['QueryType_code'].apply(lambda x: data['QueryType'].cat.categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions = predicted_data.groupby(['Month', 'Place', 'Crop'])['QueryType'].apply(lambda x: x.value_counts().index[:10]).reset_index()\n",
    "print(monthly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_predictions= monthly_predictions.explode('QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions = monthly_predictions.groupby(['Month', 'Place', 'Crop'])['QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_predictions[['Month', 'Place','Crop', 'QueryType']].to_csv('D:/Data/predicted_crop_wise_querytypes_in_India.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def precision(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Positives (FP)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    \n",
    "    # Calculate Precision\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(true_labels, predictions):\n",
    "    # Count the number of correct predictions\n",
    "    correct = np.sum(np.array(true_labels) == np.array(predictions))\n",
    "    # Calculate accuracy\n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels, predictions)\n",
    "mae = mae(true_labels, predictions)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "recall = recall(true_labels, predictions)\n",
    "\n",
    "accuracy= accuracy(true_labels, predictions)\n",
    "precision= precision(true_labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rsme, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rsme, mae]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/bert1_model_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop_querytype training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\2344155238.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "df[\"Crop_QueryType\"] = df[\"Crop\"] + \"_\" +df[\"QueryType\"]\n",
    "data=df[['Month','place','Crop_QueryType']]\n",
    "# Encode Crop_QueryType as categorical variable\n",
    "data['Crop_QueryType_code'] = data['Crop_QueryType'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\1077065516.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input texts by using DistrictName and Crop_QueryType\n",
    "data['text1'] = data.apply(lambda row: f\"{row['Month']} {row['place']} {row['Crop_QueryType']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text inputs\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs1 = tokenizer(data['text1'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract labels for training (Crop_QueryType codes)\n",
    "labels1 = torch.tensor(data['Crop_QueryType_code'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data without stratification\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1, train_idx1, test_idx1 = train_test_split(\n",
    "    inputs1['input_ids'], labels1, data.index, test_size=0.2, random_state=42)\n",
    "train_masks1, test_masks1 = train_test_split(inputs1['attention_mask'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\419209612.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\419209612.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_data1 = TensorDataset(train_inputs1, torch.tensor(train_masks1), train_labels1)\n",
    "train_sampler1 = RandomSampler(train_data1)\n",
    "train_dataloader1 = DataLoader(train_data1, sampler=train_sampler1, batch_size=batch_size)\n",
    "\n",
    "test_data1 = TensorDataset(test_inputs1, torch.tensor(test_masks1), test_labels1)\n",
    "test_sampler1 = SequentialSampler(test_data1)\n",
    "test_dataloader1 = DataLoader(test_data1, sampler=test_sampler1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8129, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=data['Crop_QueryType_code'].nunique())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and loss function\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Step 0/915532 | Loss: 8.996611595153809\n",
      "Epoch 1/1 | Step 10000/915532 | Loss: 1.120590090751648\n",
      "Epoch 1/1 | Step 20000/915532 | Loss: 0.4200342893600464\n",
      "Epoch 1/1 | Step 30000/915532 | Loss: 0.0005468620220199227\n",
      "Epoch 1/1 | Step 40000/915532 | Loss: 0.01645905151963234\n",
      "Epoch 1/1 | Step 50000/915532 | Loss: 0.4551782011985779\n",
      "Epoch 1/1 | Step 60000/915532 | Loss: 0.0010110278381034732\n",
      "Epoch 1/1 | Step 70000/915532 | Loss: 0.00020136857347097248\n",
      "Epoch 1/1 | Step 80000/915532 | Loss: 0.015328590758144855\n",
      "Epoch 1/1 | Step 90000/915532 | Loss: 0.487009733915329\n",
      "Epoch 1/1 | Step 100000/915532 | Loss: 0.0006027891649864614\n",
      "Epoch 1/1 | Step 110000/915532 | Loss: 0.00038166833110153675\n",
      "Epoch 1/1 | Step 120000/915532 | Loss: 0.00014021350943949074\n",
      "Epoch 1/1 | Step 130000/915532 | Loss: 0.00010214930080110207\n",
      "Epoch 1/1 | Step 140000/915532 | Loss: 0.00016501839854754508\n",
      "Epoch 1/1 | Step 150000/915532 | Loss: 0.7155891060829163\n",
      "Epoch 1/1 | Step 160000/915532 | Loss: 0.0009882700396701694\n",
      "Epoch 1/1 | Step 170000/915532 | Loss: 0.0002823015092872083\n",
      "Epoch 1/1 | Step 180000/915532 | Loss: 0.0002164921024814248\n",
      "Epoch 1/1 | Step 190000/915532 | Loss: 0.00015297398203983903\n",
      "Epoch 1/1 | Step 200000/915532 | Loss: 0.0037915001157671213\n",
      "Epoch 1/1 | Step 210000/915532 | Loss: 7.50449689803645e-05\n",
      "Epoch 1/1 | Step 220000/915532 | Loss: 0.0005563623271882534\n",
      "Epoch 1/1 | Step 230000/915532 | Loss: 0.00029090800671838224\n",
      "Epoch 1/1 | Step 240000/915532 | Loss: 0.0313727967441082\n",
      "Epoch 1/1 | Step 250000/915532 | Loss: 0.0005359902861528099\n",
      "Epoch 1/1 | Step 260000/915532 | Loss: 5.355601024348289e-05\n",
      "Epoch 1/1 | Step 270000/915532 | Loss: 0.0009398434776812792\n",
      "Epoch 1/1 | Step 280000/915532 | Loss: 0.00016495426825713366\n",
      "Epoch 1/1 | Step 290000/915532 | Loss: 0.0008582918089814484\n",
      "Epoch 1/1 | Step 300000/915532 | Loss: 0.00010515486792428419\n",
      "Epoch 1/1 | Step 310000/915532 | Loss: 4.713241651188582e-05\n",
      "Epoch 1/1 | Step 320000/915532 | Loss: 0.00010439135076012462\n",
      "Epoch 1/1 | Step 330000/915532 | Loss: 0.025037236511707306\n",
      "Epoch 1/1 | Step 340000/915532 | Loss: 3.795052907662466e-05\n",
      "Epoch 1/1 | Step 350000/915532 | Loss: 0.0004015358572360128\n",
      "Epoch 1/1 | Step 360000/915532 | Loss: 0.00015352756599895656\n",
      "Epoch 1/1 | Step 370000/915532 | Loss: 2.8593591196113266e-05\n",
      "Epoch 1/1 | Step 380000/915532 | Loss: 0.0007395547581836581\n",
      "Epoch 1/1 | Step 390000/915532 | Loss: 0.018035540357232094\n",
      "Epoch 1/1 | Step 400000/915532 | Loss: 0.000669140659738332\n",
      "Epoch 1/1 | Step 410000/915532 | Loss: 0.00111236865632236\n",
      "Epoch 1/1 | Step 420000/915532 | Loss: 0.00034949291148222983\n",
      "Epoch 1/1 | Step 430000/915532 | Loss: 0.00012143875937908888\n",
      "Epoch 1/1 | Step 440000/915532 | Loss: 1.3388402294367552e-05\n",
      "Epoch 1/1 | Step 450000/915532 | Loss: 0.6227198243141174\n",
      "Epoch 1/1 | Step 460000/915532 | Loss: 0.00010249565821141005\n",
      "Epoch 1/1 | Step 470000/915532 | Loss: 2.1002579160267487e-05\n",
      "Epoch 1/1 | Step 480000/915532 | Loss: 0.0007016444578766823\n",
      "Epoch 1/1 | Step 490000/915532 | Loss: 9.349557512905449e-05\n",
      "Epoch 1/1 | Step 500000/915532 | Loss: 0.0017982299905270338\n",
      "Epoch 1/1 | Step 510000/915532 | Loss: 0.7161174416542053\n",
      "Epoch 1/1 | Step 520000/915532 | Loss: 9.208534902427346e-05\n",
      "Epoch 1/1 | Step 530000/915532 | Loss: 4.5631004468305036e-05\n",
      "Epoch 1/1 | Step 540000/915532 | Loss: 0.0002319027262274176\n",
      "Epoch 1/1 | Step 550000/915532 | Loss: 0.00022257039381656796\n",
      "Epoch 1/1 | Step 560000/915532 | Loss: 4.393867129692808e-05\n",
      "Epoch 1/1 | Step 570000/915532 | Loss: 0.0006967040244489908\n",
      "Epoch 1/1 | Step 580000/915532 | Loss: 8.627837087260559e-05\n",
      "Epoch 1/1 | Step 590000/915532 | Loss: 2.2640462702838704e-05\n",
      "Epoch 1/1 | Step 600000/915532 | Loss: 0.00012542802141979337\n",
      "Epoch 1/1 | Step 610000/915532 | Loss: 5.479845276568085e-05\n",
      "Epoch 1/1 | Step 620000/915532 | Loss: 0.0038802127819508314\n",
      "Epoch 1/1 | Step 630000/915532 | Loss: 8.865386189427227e-05\n",
      "Epoch 1/1 | Step 640000/915532 | Loss: 8.212826651288196e-05\n",
      "Epoch 1/1 | Step 650000/915532 | Loss: 0.0005039412644691765\n",
      "Epoch 1/1 | Step 660000/915532 | Loss: 0.0003731547622010112\n",
      "Epoch 1/1 | Step 670000/915532 | Loss: 3.04860295727849e-05\n",
      "Epoch 1/1 | Step 680000/915532 | Loss: 2.2552188966074027e-05\n",
      "Epoch 1/1 | Step 690000/915532 | Loss: 1.5653195077902637e-05\n",
      "Epoch 1/1 | Step 700000/915532 | Loss: 0.000326466397382319\n",
      "Epoch 1/1 | Step 710000/915532 | Loss: 0.5689662098884583\n",
      "Epoch 1/1 | Step 720000/915532 | Loss: 2.5070015908568166e-05\n",
      "Epoch 1/1 | Step 730000/915532 | Loss: 6.09036760579329e-05\n",
      "Epoch 1/1 | Step 740000/915532 | Loss: 3.3161264582304284e-05\n",
      "Epoch 1/1 | Step 750000/915532 | Loss: 4.114704643143341e-05\n",
      "Epoch 1/1 | Step 760000/915532 | Loss: 3.2229167118202895e-05\n",
      "Epoch 1/1 | Step 770000/915532 | Loss: 0.0002211632381659001\n",
      "Epoch 1/1 | Step 780000/915532 | Loss: 0.00011079635442001745\n",
      "Epoch 1/1 | Step 790000/915532 | Loss: 3.2020878279581666e-05\n",
      "Epoch 1/1 | Step 800000/915532 | Loss: 7.25274148862809e-05\n",
      "Epoch 1/1 | Step 810000/915532 | Loss: 2.762444637482986e-05\n",
      "Epoch 1/1 | Step 820000/915532 | Loss: 2.0391518773976713e-05\n",
      "Epoch 1/1 | Step 830000/915532 | Loss: 0.00026551878545433283\n",
      "Epoch 1/1 | Step 840000/915532 | Loss: 0.0005575201357714832\n",
      "Epoch 1/1 | Step 850000/915532 | Loss: 0.0012335515348240733\n",
      "Epoch 1/1 | Step 860000/915532 | Loss: 0.00019512751896400005\n",
      "Epoch 1/1 | Step 870000/915532 | Loss: 7.451511919498444e-05\n",
      "Epoch 1/1 | Step 880000/915532 | Loss: 0.0006533266277983785\n",
      "Epoch 1/1 | Step 890000/915532 | Loss: 3.018081588379573e-05\n",
      "Epoch 1/1 | Step 900000/915532 | Loss: 9.643641533330083e-05\n",
      "Epoch 1/1 | Step 910000/915532 | Loss: 2.36473060795106e-05\n",
      "Epoch 1, Average Loss: 0.07708940988905852\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader1):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10000== 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{len(train_dataloader1)} | Loss: {loss.item()}\")\n",
    "\n",
    "    # End time for the epoch\n",
    "    # epoch_end_time = time.time()\n",
    "    \n",
    "    # Calculate average loss and time taken for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\") # Time Taken: {epoch_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_train_loss}\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make predictions for the next year\n",
    "model.eval()\n",
    "predictions1 = []\n",
    "true_labels1 = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader1:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask = b_input_ids.to(device), b_input_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions1.extend(preds)\n",
    "        true_labels1.extend(b_labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Month                              Place  Crop_QueryType_code\n",
      "0            9             UTTAR PRADESH_FATEHPUR                 5571\n",
      "1           12             ANDHRA PRADESH_NELLORE                 5554\n",
      "2            5                 RAJASTHAN_BHILWARA                 4410\n",
      "3           11          UTTAR PRADESH_KUSHI NAGAR                 7874\n",
      "4            9                  MAHARASHTRA_NASIK                 7553\n",
      "...        ...                                ...                  ...\n",
      "3662122      7                TAMILNADU_CUDDALORE                 5554\n",
      "3662123     12                     PUNJAB_PATIALA                 7893\n",
      "3662124     11                    BIHAR_DARBHANGA                 7889\n",
      "3662125      5  UTTAR PRADESH_GAUTAM BUDDHA NAGAR                 5917\n",
      "3662126      8              UTTAR PRADESH_ALIGARH                 1114\n",
      "\n",
      "[3662127 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# After prediction, use test_idx to reference the original data\n",
    "predictions_flat1 = predictions1\n",
    "\n",
    "# Use the indices from test_idx to retrieve the original Month and DistrictName from the original data\n",
    "predicted_data1 = pd.DataFrame({\n",
    "    'Month': data.loc[test_idx, 'Month'].values, \n",
    "    'Place': data.loc[test_idx, 'place'].values,\n",
    "    'Crop_QueryType_code': predictions_flat1\n",
    "})\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13480\\301140935.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Month                              Place  Crop_QueryType_code  \\\n",
      "0            9             UTTAR PRADESH_FATEHPUR                 5571   \n",
      "1           12             ANDHRA PRADESH_NELLORE                 5554   \n",
      "2            5                 RAJASTHAN_BHILWARA                 4410   \n",
      "3           11          UTTAR PRADESH_KUSHI NAGAR                 7874   \n",
      "4            9                  MAHARASHTRA_NASIK                 7553   \n",
      "...        ...                                ...                  ...   \n",
      "3662122      7                TAMILNADU_CUDDALORE                 5554   \n",
      "3662123     12                     PUNJAB_PATIALA                 7893   \n",
      "3662124     11                    BIHAR_DARBHANGA                 7889   \n",
      "3662125      5  UTTAR PRADESH_GAUTAM BUDDHA NAGAR                 5917   \n",
      "3662126      8              UTTAR PRADESH_ALIGARH                 1114   \n",
      "\n",
      "                               Crop_QueryType  \n",
      "0                          Paddy Dhan_Weather  \n",
      "1              Paddy Dhan_Nutrient Management  \n",
      "2                      Lemon_Plant Protection  \n",
      "3                     Wheat_Field Preparation  \n",
      "4                     Tomato_Plant Protection  \n",
      "...                                       ...  \n",
      "3662122        Paddy Dhan_Nutrient Management  \n",
      "3662123                         Wheat_Weather  \n",
      "3662124                       Wheat_Varieties  \n",
      "3662125                  Pig_Dairy Production  \n",
      "3662126  Black Gram urd bean_Plant Protection  \n",
      "\n",
      "[3662127 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data['Crop_QueryType'] = data['Crop_QueryType'].astype('category')\n",
    "predicted_data1['Crop_QueryType']=predicted_data1['Crop_QueryType_code'].apply(lambda x: data['Crop_QueryType'].cat.categories[x])\n",
    "print(predicted_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7756     12     WEST BENGAL_North DINAJPUR   \n",
      "7757     12            WEST BENGAL_PURULIA   \n",
      "7758     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7759     12     WEST BENGAL_South DINAJPUR   \n",
      "7760     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Index(['Chillies_Nutrient Management', 'Ash Go...  \n",
      "1     Index(['Groundnut pea nutmung phalli_Plant Pro...  \n",
      "2     Index(['Paddy Dhan_Plant Protection', 'Chillie...  \n",
      "3     Index(['Paddy Dhan_Plant Protection', 'Black G...  \n",
      "4     Index(['Chillies_Plant Protection', 'Chillies_...  \n",
      "...                                                 ...  \n",
      "7756  Index(['Potato_Plant Protection', 'Maize Makka...  \n",
      "7757  Index(['Potato_Plant Protection', 'Tomato_Plan...  \n",
      "7758  Index(['Paddy Dhan_Weather', 'Paddy Dhan_Plant...  \n",
      "7759  Index(['Potato_Plant Protection', 'Paddy Dhan_...  \n",
      "7760  Index(['Paddy Dhan_Weather', 'Potato_Plant Pro...  \n",
      "\n",
      "[7761 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_predictions1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: x.value_counts().index[:15]).reset_index()\n",
    "print(monthly_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7756     12     WEST BENGAL_North DINAJPUR   \n",
      "7757     12            WEST BENGAL_PURULIA   \n",
      "7758     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7759     12     WEST BENGAL_South DINAJPUR   \n",
      "7760     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Chillies_Nutrient Management, Ash Gourd Petha_...  \n",
      "1     Groundnut pea nutmung phalli_Plant Protection,...  \n",
      "2     Paddy Dhan_Plant Protection, Chillies_Plant Pr...  \n",
      "3     Paddy Dhan_Plant Protection, Black Gram urd be...  \n",
      "4     Chillies_Plant Protection, Chillies_Nutrient M...  \n",
      "...                                                 ...  \n",
      "7756  Potato_Plant Protection, Maize Makka_Plant Pro...  \n",
      "7757  Potato_Plant Protection, Tomato_Plant Protecti...  \n",
      "7758  Paddy Dhan_Weather, Paddy Dhan_Plant Protectio...  \n",
      "7759  Potato_Plant Protection, Paddy Dhan_Plant Prot...  \n",
      "7760  Paddy Dhan_Weather, Potato_Plant Protection, P...  \n",
      "\n",
      "[7761 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "monthly_predictions1= monthly_predictions1.explode('Crop_QueryType')\n",
    "# Concatenate the QueryType values for each unique combination of Month and Place\n",
    "monthly_predictions1 = monthly_predictions1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "print(monthly_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month                          Place  \\\n",
      "0         1        A AND N ISLANDS_NICOBAR   \n",
      "1         1        ANDHRA PRADESH_ANANTPUR   \n",
      "2         1        ANDHRA PRADESH_CHITTOOR   \n",
      "3         1   ANDHRA PRADESH_EAST GODAVARI   \n",
      "4         1          ANDHRA PRADESH_GUNTUR   \n",
      "...     ...                            ...   \n",
      "7756     12     WEST BENGAL_North DINAJPUR   \n",
      "7757     12            WEST BENGAL_PURULIA   \n",
      "7758     12  WEST BENGAL_SOUTH 24 PARGANAS   \n",
      "7759     12     WEST BENGAL_South DINAJPUR   \n",
      "7760     12     WEST BENGAL_WEST MEDINIPUR   \n",
      "\n",
      "                                         Crop_QueryType  \n",
      "0     Chillies_Nutrient Management, Ash Gourd Petha_...  \n",
      "1     Tomato_Plant Protection, BhindiOkraLadysfinger...  \n",
      "2     Chillies_Plant Protection, Black Gram urd bean...  \n",
      "3     Sugarcane Noble Cane_Weed Management, Paddy Dh...  \n",
      "4     Banana_Nutrient Management, Bengal Gram GramCh...  \n",
      "...                                                 ...  \n",
      "7756  Maize Makka_Plant Protection, Potato_Nutrient ...  \n",
      "7757  Tomato_Plant Protection, Paddy Dhan_Weather, P...  \n",
      "7758  Paddy Dhan_Weather, BhindiOkraLadysfinger_Fert...  \n",
      "7759  Chillies_Plant Protection, Potato_Plant Protec...  \n",
      "7760  Chillies_Plant Protection, Mustard_Weather, Pa...  \n",
      "\n",
      "[7761 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict most frequent Crop_QueryType for next year on a monthly basis\n",
    "monthly_data1 = predicted_data1.groupby(['Month', 'Place'])['Crop_QueryType'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "print(monthly_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV with only Month, Place, and Crop_QueryType columns\n",
    "monthly_data1.to_csv('D:/Data/Places_crop_querytypes_in_India.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true labels\n",
    "predictions1 = np.array(predictions1)\n",
    "true_labels1 = np.array(true_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(true_labels, predictions):\n",
    "    return np.sqrt(np.mean((np.array(true_labels) - np.array(predictions)) ** 2))\n",
    "\n",
    "def mae(true_labels, predictions):\n",
    "    return np.mean(np.abs(np.array(true_labels) - np.array(predictions)))\n",
    "\n",
    "def f1_score(true_labels, predictions):\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "def recall(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Negatives (FN)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fn = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 0))\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def precision(true_labels, predictions):\n",
    "    # Calculate True Positives (TP) and False Positives (FP)\n",
    "    tp = np.sum((np.array(true_labels) == 1) & (np.array(predictions) == 1))\n",
    "    fp = np.sum((np.array(true_labels) == 0) & (np.array(predictions) == 1))\n",
    "    \n",
    "    # Calculate Precision\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(true_labels, predictions):\n",
    "    # Count the number of correct predictions\n",
    "    correct = np.sum(np.array(true_labels) == np.array(predictions))\n",
    "    # Calculate accuracy\n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(true_labels1, predictions1)\n",
    "mae = mae(true_labels1, predictions1)\n",
    "\n",
    "# Calculate F1-Score and Recall\n",
    "f1= f1_score(true_labels1, predictions1)\n",
    "recall = recall(true_labels1, predictions1)\n",
    "\n",
    "accuracy = accuracy(true_labels1, predictions1)\n",
    "precision = precision(true_labels1, predictions1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "      Metric       Value\n",
      "0   Accuracy    0.997274\n",
      "1  Precision    0.000000\n",
      "2     Recall    0.000000\n",
      "3   F1-Score    0.000000\n",
      "4       RMSE  126.121665\n",
      "5        MAE    4.056349\n"
     ]
    }
   ],
   "source": [
    "# Create a bar plot for the metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE']\n",
    "values = [accuracy, precision, recall, f1, rmse, mae]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'Teal', 'orange'])\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics in a tabular format\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, rmse, mae]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_table)\n",
    "# Save the metrics to a CSV file\n",
    "metrics_table.to_csv('D:/Data/bert2 model_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.12166489186039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mean_squared_error(true_labels1, predictions1))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
